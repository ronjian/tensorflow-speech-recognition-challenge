{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dir = \"/home/maikfangogoair/tmp/save/\"\n",
    "pkl_file = \"spectrogram_v2.pkl\"\n",
    "with open (data_dir + pkl_file, 'rb') as fp:\n",
    "    all_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "input=tf.placeholder(tf.float32, (None, 99, 161), 'input')\n",
    "def spectrogram_ConvNet(input):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    \n",
    "    \n",
    "    reshape_input = tf.reshape(input, [-1, 99, 161, 1], 'reshape_input')\n",
    "    batch_input = tf.layers.batch_normalization(reshape_input)\n",
    "    \n",
    "    conv_1 = tf.layers.conv2d(batch_input, 64, (5,5), (1,1), 'same', name='conv_1')\n",
    "    relu_1 = tf.nn.relu(conv_1,'relu_1')\n",
    "    dropout_1 = tf.nn.dropout(relu_1, dropout_prob, name = 'dropout_1')\n",
    "    \n",
    "    max_pool = tf.nn.max_pool(dropout_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "    conv_2 = tf.layers.conv2d(max_pool, 64, (3,3), (1,1), 'same')\n",
    "    relu_2 = tf.nn.relu(conv_2,'relu_2')\n",
    "    dropout_2 = tf.nn.dropout(relu_2, dropout_prob, name = 'dropout_2')\n",
    "    \n",
    "    size=int(dropout_2.get_shape()[1]) * int(dropout_2.get_shape()[2]) * int(dropout_2.get_shape()[3])\n",
    "    flatten = tf.reshape(dropout_2, (-1, size), 'flatten')\n",
    "\n",
    "    logits = tf.layers.dense(flatten, 12,name = 'logits')\n",
    "    \n",
    "    return logits,dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"spectrogram_ConvNet\"\n",
    "if model_name == \"spectrogram_ConvNet\":\n",
    "    logits,dropout_prob = spectrogram_ConvNet(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training steps\n",
    "ground_truth_input = tf.placeholder(tf.int64, [None], name='groundtruth_input')\n",
    "learning_rate_input = tf.placeholder(tf.float32, [], name='learning_rate_input')\n",
    "cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(ground_truth_input, logits)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate_input).minimize(cross_entropy_mean)\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "confusion_matrix = tf.confusion_matrix(ground_truth_input, predicted_indices, num_classes=12)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, loss: 1.267259, accurancy: 0.583333.\n",
      "Step 2, loss: 1.164033, accurancy: 0.646667.\n",
      "Step 3, loss: 1.352372, accurancy: 0.590000.\n",
      "Step 4, loss: 1.190791, accurancy: 0.653333.\n",
      "Step 5, loss: 1.233237, accurancy: 0.606667.\n",
      "Step 6, loss: 1.246054, accurancy: 0.636667.\n",
      "Step 7, loss: 1.331899, accurancy: 0.606667.\n",
      "Step 8, loss: 1.184370, accurancy: 0.646667.\n",
      "Step 9, loss: 1.257778, accurancy: 0.573333.\n",
      "Step 10, loss: 1.360212, accurancy: 0.583333.\n",
      "Step 11, loss: 1.185382, accurancy: 0.616667.\n",
      "Step 12, loss: 1.336402, accurancy: 0.556667.\n",
      "Step 13, loss: 1.169189, accurancy: 0.610000.\n",
      "Step 14, loss: 1.266326, accurancy: 0.566667.\n",
      "Step 15, loss: 1.112651, accurancy: 0.653333.\n",
      "Step 16, loss: 1.102832, accurancy: 0.650000.\n",
      "Step 17, loss: 1.288378, accurancy: 0.590000.\n",
      "Step 18, loss: 1.323243, accurancy: 0.613333.\n",
      "Step 19, loss: 1.216810, accurancy: 0.623333.\n",
      "Step 20, loss: 1.291352, accurancy: 0.616667.\n",
      "Step 21, loss: 1.286364, accurancy: 0.613333.\n",
      "Step 22, loss: 1.181442, accurancy: 0.613333.\n",
      "Step 23, loss: 1.218441, accurancy: 0.586667.\n",
      "Step 24, loss: 1.197259, accurancy: 0.610000.\n",
      "Step 25, loss: 1.281002, accurancy: 0.606667.\n",
      "Step 26, loss: 1.202656, accurancy: 0.636667.\n",
      "Step 27, loss: 1.254107, accurancy: 0.610000.\n",
      "Step 28, loss: 1.236723, accurancy: 0.616667.\n",
      "Step 29, loss: 1.210623, accurancy: 0.600000.\n",
      "Step 30, loss: 1.376115, accurancy: 0.520000.\n",
      "Step 31, loss: 1.199844, accurancy: 0.636667.\n",
      "Step 32, loss: 1.101131, accurancy: 0.660000.\n",
      "Step 33, loss: 1.216444, accurancy: 0.650000.\n",
      "Step 34, loss: 1.223867, accurancy: 0.586667.\n",
      "Step 35, loss: 1.282438, accurancy: 0.580000.\n",
      "Step 36, loss: 1.155835, accurancy: 0.620000.\n",
      "Step 37, loss: 1.238764, accurancy: 0.620000.\n",
      "Step 38, loss: 1.322367, accurancy: 0.570000.\n",
      "Step 39, loss: 1.253260, accurancy: 0.606667.\n",
      "Step 40, loss: 1.370916, accurancy: 0.563333.\n",
      "Step 41, loss: 1.282919, accurancy: 0.593333.\n",
      "Step 42, loss: 1.385017, accurancy: 0.580000.\n",
      "Step 43, loss: 1.179337, accurancy: 0.626667.\n",
      "Step 44, loss: 1.346903, accurancy: 0.566667.\n",
      "Step 45, loss: 1.262537, accurancy: 0.640000.\n",
      "Step 46, loss: 1.313218, accurancy: 0.590000.\n",
      "Step 47, loss: 1.296055, accurancy: 0.573333.\n",
      "Step 48, loss: 1.341109, accurancy: 0.566667.\n",
      "Step 49, loss: 1.260961, accurancy: 0.583333.\n",
      "Step 50, loss: 1.219002, accurancy: 0.586667.\n",
      "Step 51, loss: 1.309741, accurancy: 0.603333.\n",
      "Step 52, loss: 1.212888, accurancy: 0.613333.\n",
      "Step 53, loss: 1.307472, accurancy: 0.563333.\n",
      "Step 54, loss: 1.286837, accurancy: 0.580000.\n",
      "Step 55, loss: 1.258032, accurancy: 0.576667.\n",
      "Step 56, loss: 1.348291, accurancy: 0.573333.\n",
      "Step 57, loss: 1.258848, accurancy: 0.623333.\n",
      "Step 58, loss: 1.156847, accurancy: 0.593333.\n",
      "Step 59, loss: 1.329406, accurancy: 0.616667.\n",
      "Step 60, loss: 1.241143, accurancy: 0.600000.\n",
      "Step 61, loss: 1.230295, accurancy: 0.620000.\n",
      "Step 62, loss: 1.163140, accurancy: 0.650000.\n",
      "Step 63, loss: 1.251185, accurancy: 0.596667.\n",
      "Step 64, loss: 1.250242, accurancy: 0.586667.\n",
      "Step 65, loss: 1.255291, accurancy: 0.603333.\n",
      "Step 66, loss: 1.185708, accurancy: 0.633333.\n",
      "Step 67, loss: 1.276518, accurancy: 0.586667.\n",
      "Step 68, loss: 1.120921, accurancy: 0.672727.\n",
      "Step 69, loss: 1.181545, accurancy: 0.643333.\n",
      "Step 70, loss: 1.166815, accurancy: 0.623333.\n",
      "Step 71, loss: 1.255256, accurancy: 0.593333.\n",
      "Step 72, loss: 1.232812, accurancy: 0.630000.\n",
      "Step 73, loss: 1.351306, accurancy: 0.563333.\n",
      "Step 74, loss: 1.239463, accurancy: 0.630000.\n",
      "Step 75, loss: 1.264788, accurancy: 0.573333.\n",
      "Step 76, loss: 1.186365, accurancy: 0.613333.\n",
      "Step 77, loss: 1.354015, accurancy: 0.603333.\n",
      "Step 78, loss: 1.226928, accurancy: 0.630000.\n",
      "Step 79, loss: 1.338581, accurancy: 0.550000.\n",
      "Step 80, loss: 1.201636, accurancy: 0.603333.\n",
      "Step 81, loss: 1.231137, accurancy: 0.606667.\n",
      "Step 82, loss: 1.144005, accurancy: 0.640000.\n",
      "Step 83, loss: 1.153579, accurancy: 0.640000.\n",
      "Step 84, loss: 1.280662, accurancy: 0.606667.\n",
      "Step 85, loss: 1.255555, accurancy: 0.626667.\n",
      "Step 86, loss: 1.303665, accurancy: 0.583333.\n",
      "Step 87, loss: 1.232837, accurancy: 0.636667.\n",
      "Step 88, loss: 1.255184, accurancy: 0.596667.\n",
      "Step 89, loss: 1.194994, accurancy: 0.653333.\n",
      "Step 90, loss: 1.096604, accurancy: 0.636667.\n",
      "Step 91, loss: 1.235914, accurancy: 0.593333.\n",
      "Step 92, loss: 1.238520, accurancy: 0.640000.\n",
      "Step 93, loss: 1.167082, accurancy: 0.626667.\n",
      "Step 94, loss: 1.322253, accurancy: 0.570000.\n",
      "Step 95, loss: 1.273380, accurancy: 0.573333.\n",
      "Step 96, loss: 1.170546, accurancy: 0.620000.\n",
      "Step 97, loss: 1.299882, accurancy: 0.586667.\n",
      "Step 98, loss: 1.293436, accurancy: 0.613333.\n",
      "Step 99, loss: 1.123588, accurancy: 0.640000.\n",
      "Step 100, loss: 1.191500, accurancy: 0.633333.\n",
      "Validation accurancy is 0.563333\n",
      "Step 101, loss: 1.336003, accurancy: 0.603333.\n",
      "Step 102, loss: 1.163630, accurancy: 0.623333.\n",
      "Step 103, loss: 1.198669, accurancy: 0.600000.\n",
      "Step 104, loss: 1.147206, accurancy: 0.620000.\n",
      "Step 105, loss: 1.304250, accurancy: 0.563333.\n",
      "Step 106, loss: 1.243342, accurancy: 0.580000.\n",
      "Step 107, loss: 1.271853, accurancy: 0.573333.\n",
      "Step 108, loss: 1.367247, accurancy: 0.590000.\n",
      "Step 109, loss: 1.281976, accurancy: 0.583333.\n",
      "Step 110, loss: 1.188216, accurancy: 0.610000.\n",
      "Step 111, loss: 1.272737, accurancy: 0.616667.\n",
      "Step 112, loss: 1.341379, accurancy: 0.583333.\n",
      "Step 113, loss: 1.293440, accurancy: 0.683333.\n",
      "Step 114, loss: 1.254916, accurancy: 0.576667.\n",
      "Step 115, loss: 1.271401, accurancy: 0.560000.\n",
      "Step 116, loss: 1.310231, accurancy: 0.583333.\n",
      "Step 117, loss: 1.245398, accurancy: 0.610000.\n",
      "Step 118, loss: 1.232723, accurancy: 0.566667.\n",
      "Step 119, loss: 1.261863, accurancy: 0.600000.\n",
      "Step 120, loss: 1.147056, accurancy: 0.653333.\n",
      "Step 121, loss: 1.253554, accurancy: 0.556667.\n",
      "Step 122, loss: 1.330414, accurancy: 0.593333.\n",
      "Step 123, loss: 1.300321, accurancy: 0.560000.\n",
      "Step 124, loss: 1.341110, accurancy: 0.546667.\n",
      "Step 125, loss: 1.253515, accurancy: 0.590000.\n",
      "Step 126, loss: 1.240404, accurancy: 0.610000.\n",
      "Step 127, loss: 1.189581, accurancy: 0.610000.\n",
      "Step 128, loss: 1.322262, accurancy: 0.593333.\n",
      "Step 129, loss: 1.123796, accurancy: 0.666667.\n",
      "Step 130, loss: 1.216669, accurancy: 0.613333.\n",
      "Step 131, loss: 1.174477, accurancy: 0.620000.\n",
      "Step 132, loss: 1.292977, accurancy: 0.566667.\n",
      "Step 133, loss: 1.235826, accurancy: 0.636667.\n",
      "Step 134, loss: 1.227789, accurancy: 0.640000.\n",
      "Step 135, loss: 1.163939, accurancy: 0.650000.\n",
      "Step 136, loss: 0.734557, accurancy: 0.833333.\n",
      "Step 137, loss: 1.036002, accurancy: 0.680000.\n",
      "Step 138, loss: 1.344065, accurancy: 0.570000.\n",
      "Step 139, loss: 1.168484, accurancy: 0.660000.\n",
      "Step 140, loss: 1.177395, accurancy: 0.613333.\n",
      "Step 141, loss: 1.162173, accurancy: 0.656667.\n",
      "Step 142, loss: 1.306235, accurancy: 0.573333.\n",
      "Step 143, loss: 1.166968, accurancy: 0.660000.\n",
      "Step 144, loss: 1.270154, accurancy: 0.610000.\n",
      "Step 145, loss: 1.352229, accurancy: 0.610000.\n",
      "Step 146, loss: 1.096387, accurancy: 0.643333.\n",
      "Step 147, loss: 1.364469, accurancy: 0.550000.\n",
      "Step 148, loss: 1.209517, accurancy: 0.576667.\n",
      "Step 149, loss: 1.244580, accurancy: 0.610000.\n",
      "Step 150, loss: 1.090388, accurancy: 0.643333.\n",
      "Step 151, loss: 1.128345, accurancy: 0.643333.\n",
      "Step 152, loss: 1.264394, accurancy: 0.610000.\n",
      "Step 153, loss: 1.352208, accurancy: 0.596667.\n",
      "Step 154, loss: 1.278599, accurancy: 0.616667.\n",
      "Step 155, loss: 1.236491, accurancy: 0.656667.\n",
      "Step 156, loss: 1.257112, accurancy: 0.630000.\n",
      "Step 157, loss: 1.118784, accurancy: 0.683333.\n",
      "Step 158, loss: 1.193795, accurancy: 0.606667.\n",
      "Step 159, loss: 1.184536, accurancy: 0.626667.\n",
      "Step 160, loss: 1.281146, accurancy: 0.610000.\n",
      "Step 161, loss: 1.191987, accurancy: 0.626667.\n",
      "Step 162, loss: 1.291793, accurancy: 0.580000.\n",
      "Step 163, loss: 1.187314, accurancy: 0.673333.\n",
      "Step 164, loss: 1.205510, accurancy: 0.646667.\n",
      "Step 165, loss: 1.373616, accurancy: 0.566667.\n",
      "Step 166, loss: 1.153123, accurancy: 0.633333.\n",
      "Step 167, loss: 1.055870, accurancy: 0.686667.\n",
      "Step 168, loss: 1.226259, accurancy: 0.623333.\n",
      "Step 169, loss: 1.229617, accurancy: 0.616667.\n",
      "Step 170, loss: 1.190655, accurancy: 0.633333.\n",
      "Step 171, loss: 1.182120, accurancy: 0.616667.\n",
      "Step 172, loss: 1.218109, accurancy: 0.593333.\n",
      "Step 173, loss: 1.331284, accurancy: 0.573333.\n",
      "Step 174, loss: 1.226077, accurancy: 0.606667.\n",
      "Step 175, loss: 1.361939, accurancy: 0.576667.\n",
      "Step 176, loss: 1.240381, accurancy: 0.606667.\n",
      "Step 177, loss: 1.343203, accurancy: 0.583333.\n",
      "Step 178, loss: 1.159099, accurancy: 0.603333.\n",
      "Step 179, loss: 1.294172, accurancy: 0.576667.\n",
      "Step 180, loss: 1.348350, accurancy: 0.573333.\n",
      "Step 181, loss: 1.258596, accurancy: 0.623333.\n",
      "Step 182, loss: 1.279771, accurancy: 0.573333.\n",
      "Step 183, loss: 1.305870, accurancy: 0.563333.\n",
      "Step 184, loss: 1.204566, accurancy: 0.616667.\n",
      "Step 185, loss: 1.210406, accurancy: 0.586667.\n",
      "Step 186, loss: 1.251383, accurancy: 0.596667.\n",
      "Step 187, loss: 1.208321, accurancy: 0.596667.\n",
      "Step 188, loss: 1.281801, accurancy: 0.566667.\n",
      "Step 189, loss: 1.306530, accurancy: 0.593333.\n",
      "Step 190, loss: 1.251861, accurancy: 0.596667.\n",
      "Step 191, loss: 1.295292, accurancy: 0.556667.\n",
      "Step 192, loss: 1.206051, accurancy: 0.630000.\n",
      "Step 193, loss: 1.182635, accurancy: 0.623333.\n",
      "Step 194, loss: 1.312326, accurancy: 0.590000.\n",
      "Step 195, loss: 1.190452, accurancy: 0.616667.\n",
      "Step 196, loss: 1.362090, accurancy: 0.583333.\n",
      "Step 197, loss: 1.200666, accurancy: 0.626667.\n",
      "Step 198, loss: 1.241586, accurancy: 0.583333.\n",
      "Step 199, loss: 1.256597, accurancy: 0.590000.\n",
      "Step 200, loss: 1.253132, accurancy: 0.580000.\n",
      "Validation accurancy is 0.646667\n",
      "Step 201, loss: 1.189226, accurancy: 0.603333.\n",
      "Step 202, loss: 1.239644, accurancy: 0.600000.\n",
      "Step 203, loss: 1.096368, accurancy: 0.620513.\n",
      "Step 204, loss: 1.189276, accurancy: 0.650000.\n",
      "Step 205, loss: 1.126492, accurancy: 0.653333.\n",
      "Step 206, loss: 1.183952, accurancy: 0.620000.\n",
      "Step 207, loss: 1.165980, accurancy: 0.656667.\n",
      "Step 208, loss: 1.266059, accurancy: 0.573333.\n",
      "Step 209, loss: 1.240966, accurancy: 0.620000.\n",
      "Step 210, loss: 1.292709, accurancy: 0.590000.\n",
      "Step 211, loss: 1.164543, accurancy: 0.650000.\n",
      "Step 212, loss: 1.310200, accurancy: 0.596667.\n",
      "Step 213, loss: 1.187330, accurancy: 0.646667.\n",
      "Step 214, loss: 1.291986, accurancy: 0.553333.\n",
      "Step 215, loss: 1.248661, accurancy: 0.580000.\n",
      "Step 216, loss: 1.242848, accurancy: 0.600000.\n",
      "Step 217, loss: 1.156052, accurancy: 0.646667.\n",
      "Step 218, loss: 1.136972, accurancy: 0.623333.\n",
      "Step 219, loss: 1.222423, accurancy: 0.603333.\n",
      "Step 220, loss: 1.235742, accurancy: 0.653333.\n",
      "Step 221, loss: 1.319953, accurancy: 0.593333.\n",
      "Step 222, loss: 1.268891, accurancy: 0.653333.\n",
      "Step 223, loss: 1.201896, accurancy: 0.616667.\n",
      "Step 224, loss: 1.176753, accurancy: 0.650000.\n",
      "Step 225, loss: 1.115861, accurancy: 0.646667.\n",
      "Step 226, loss: 1.189719, accurancy: 0.610000.\n",
      "Step 227, loss: 1.216310, accurancy: 0.623333.\n",
      "Step 228, loss: 1.206312, accurancy: 0.633333.\n",
      "Step 229, loss: 1.236756, accurancy: 0.593333.\n",
      "Step 230, loss: 1.193316, accurancy: 0.576667.\n",
      "Step 231, loss: 1.190642, accurancy: 0.646667.\n",
      "Step 232, loss: 1.286206, accurancy: 0.600000.\n",
      "Step 233, loss: 1.220198, accurancy: 0.640000.\n",
      "Step 234, loss: 1.136350, accurancy: 0.666667.\n",
      "Step 235, loss: 1.249921, accurancy: 0.616667.\n",
      "Step 236, loss: 1.312083, accurancy: 0.626667.\n",
      "Step 237, loss: 1.201442, accurancy: 0.633333.\n",
      "Step 238, loss: 1.202073, accurancy: 0.626667.\n",
      "Step 239, loss: 1.195755, accurancy: 0.606667.\n",
      "Step 240, loss: 1.309255, accurancy: 0.563333.\n",
      "Step 241, loss: 1.256452, accurancy: 0.593333.\n",
      "Step 242, loss: 1.240454, accurancy: 0.606667.\n",
      "Step 243, loss: 1.328043, accurancy: 0.593333.\n",
      "Step 244, loss: 1.275742, accurancy: 0.583333.\n",
      "Step 245, loss: 1.237094, accurancy: 0.623333.\n",
      "Step 246, loss: 1.220207, accurancy: 0.630000.\n",
      "Step 247, loss: 1.339573, accurancy: 0.570000.\n",
      "Step 248, loss: 1.259439, accurancy: 0.646667.\n",
      "Step 249, loss: 1.243119, accurancy: 0.586667.\n",
      "Step 250, loss: 1.276186, accurancy: 0.596667.\n",
      "Step 251, loss: 1.297944, accurancy: 0.596667.\n",
      "Step 252, loss: 1.244404, accurancy: 0.613333.\n",
      "Step 253, loss: 1.237414, accurancy: 0.550000.\n",
      "Step 254, loss: 1.193956, accurancy: 0.593333.\n",
      "Step 255, loss: 1.229196, accurancy: 0.646667.\n",
      "Step 256, loss: 1.277305, accurancy: 0.593333.\n",
      "Step 257, loss: 1.356904, accurancy: 0.580000.\n",
      "Step 258, loss: 1.244150, accurancy: 0.580000.\n",
      "Step 259, loss: 1.329790, accurancy: 0.596667.\n",
      "Step 260, loss: 1.204688, accurancy: 0.630000.\n",
      "Step 261, loss: 1.237480, accurancy: 0.620000.\n",
      "Step 262, loss: 1.216568, accurancy: 0.610000.\n",
      "Step 263, loss: 1.341243, accurancy: 0.566667.\n",
      "Step 264, loss: 1.194526, accurancy: 0.643333.\n",
      "Step 265, loss: 1.168227, accurancy: 0.606667.\n",
      "Step 266, loss: 1.304808, accurancy: 0.583333.\n",
      "Step 267, loss: 1.300668, accurancy: 0.573333.\n",
      "Step 268, loss: 1.228592, accurancy: 0.610000.\n",
      "Step 269, loss: 1.247124, accurancy: 0.593333.\n",
      "Step 270, loss: 1.237106, accurancy: 0.586667.\n",
      "Step 271, loss: 0.933118, accurancy: 0.716667.\n",
      "Step 272, loss: 1.089099, accurancy: 0.673333.\n",
      "Step 273, loss: 1.328920, accurancy: 0.583333.\n",
      "Step 274, loss: 1.177599, accurancy: 0.643333.\n",
      "Step 275, loss: 1.207498, accurancy: 0.640000.\n",
      "Step 276, loss: 1.232727, accurancy: 0.633333.\n",
      "Step 277, loss: 1.366737, accurancy: 0.603333.\n",
      "Step 278, loss: 1.225059, accurancy: 0.643333.\n",
      "Step 279, loss: 1.190794, accurancy: 0.626667.\n",
      "Step 280, loss: 1.383622, accurancy: 0.590000.\n",
      "Step 281, loss: 1.140274, accurancy: 0.630000.\n",
      "Step 282, loss: 1.321941, accurancy: 0.570000.\n",
      "Step 283, loss: 1.200963, accurancy: 0.583333.\n",
      "Step 284, loss: 1.265969, accurancy: 0.600000.\n",
      "Step 285, loss: 1.065246, accurancy: 0.666667.\n",
      "Step 286, loss: 1.107475, accurancy: 0.666667.\n",
      "Step 287, loss: 1.271738, accurancy: 0.596667.\n",
      "Step 288, loss: 1.276064, accurancy: 0.613333.\n",
      "Step 289, loss: 1.297385, accurancy: 0.603333.\n",
      "Step 290, loss: 1.189452, accurancy: 0.633333.\n",
      "Step 291, loss: 1.226970, accurancy: 0.616667.\n",
      "Step 292, loss: 1.172423, accurancy: 0.646667.\n",
      "Step 293, loss: 1.157589, accurancy: 0.616667.\n",
      "Step 294, loss: 1.178031, accurancy: 0.643333.\n",
      "Step 295, loss: 1.227841, accurancy: 0.640000.\n",
      "Step 296, loss: 1.143306, accurancy: 0.656667.\n",
      "Step 297, loss: 1.315568, accurancy: 0.573333.\n",
      "Step 298, loss: 1.262836, accurancy: 0.603333.\n",
      "Step 299, loss: 1.170951, accurancy: 0.656667.\n",
      "Step 300, loss: 1.333982, accurancy: 0.553333.\n",
      "Validation accurancy is 0.593333\n",
      "Step 301, loss: 1.132161, accurancy: 0.653333.\n",
      "Step 302, loss: 1.045709, accurancy: 0.683333.\n",
      "Step 303, loss: 1.201043, accurancy: 0.633333.\n",
      "Step 304, loss: 1.285897, accurancy: 0.593333.\n",
      "Step 305, loss: 1.169897, accurancy: 0.626667.\n",
      "Step 306, loss: 1.233793, accurancy: 0.603333.\n",
      "Step 307, loss: 1.236089, accurancy: 0.596667.\n",
      "Step 308, loss: 1.341872, accurancy: 0.563333.\n",
      "Step 309, loss: 1.284549, accurancy: 0.573333.\n",
      "Step 310, loss: 1.295339, accurancy: 0.623333.\n",
      "Step 311, loss: 1.311986, accurancy: 0.570000.\n",
      "Step 312, loss: 1.373676, accurancy: 0.563333.\n",
      "Step 313, loss: 1.155259, accurancy: 0.633333.\n",
      "Step 314, loss: 1.304332, accurancy: 0.580000.\n",
      "Step 315, loss: 1.317180, accurancy: 0.596667.\n",
      "Step 316, loss: 1.268433, accurancy: 0.613333.\n",
      "Step 317, loss: 1.293823, accurancy: 0.586667.\n",
      "Step 318, loss: 1.290892, accurancy: 0.560000.\n",
      "Step 319, loss: 1.254773, accurancy: 0.596667.\n",
      "Step 320, loss: 1.248488, accurancy: 0.593333.\n",
      "Step 321, loss: 1.266518, accurancy: 0.580000.\n",
      "Step 322, loss: 1.175593, accurancy: 0.636667.\n",
      "Step 323, loss: 1.333157, accurancy: 0.573333.\n",
      "Step 324, loss: 1.301184, accurancy: 0.576667.\n",
      "Step 325, loss: 1.233809, accurancy: 0.590000.\n",
      "Step 326, loss: 1.228804, accurancy: 0.610000.\n",
      "Step 327, loss: 1.272950, accurancy: 0.600000.\n",
      "Step 328, loss: 1.220992, accurancy: 0.596667.\n",
      "Step 329, loss: 1.291473, accurancy: 0.616667.\n",
      "Step 330, loss: 1.167300, accurancy: 0.613333.\n",
      "Step 331, loss: 1.235508, accurancy: 0.623333.\n",
      "Step 332, loss: 1.113961, accurancy: 0.646667.\n",
      "Step 333, loss: 1.247366, accurancy: 0.573333.\n",
      "Step 334, loss: 1.226877, accurancy: 0.636667.\n",
      "Step 335, loss: 1.261751, accurancy: 0.576667.\n",
      "Step 336, loss: 1.204795, accurancy: 0.623333.\n",
      "Step 337, loss: 1.195302, accurancy: 0.636667.\n",
      "Step 338, loss: 1.179002, accurancy: 0.626667.\n",
      "Step 339, loss: 1.136255, accurancy: 0.653333.\n",
      "Step 340, loss: 1.158024, accurancy: 0.646667.\n",
      "Step 341, loss: 1.259439, accurancy: 0.616667.\n",
      "Step 342, loss: 1.178420, accurancy: 0.643333.\n",
      "Step 343, loss: 1.195852, accurancy: 0.586667.\n",
      "Step 344, loss: 1.298087, accurancy: 0.650000.\n",
      "Step 345, loss: 1.230407, accurancy: 0.610000.\n",
      "Step 346, loss: 1.223886, accurancy: 0.626667.\n",
      "Step 347, loss: 1.264349, accurancy: 0.590000.\n",
      "Step 348, loss: 1.224149, accurancy: 0.613333.\n",
      "Step 349, loss: 1.190636, accurancy: 0.616667.\n",
      "Step 350, loss: 1.294979, accurancy: 0.570000.\n",
      "Step 351, loss: 1.223948, accurancy: 0.600000.\n",
      "Step 352, loss: 1.089051, accurancy: 0.660000.\n",
      "Step 353, loss: 1.130130, accurancy: 0.660000.\n",
      "Step 354, loss: 1.224250, accurancy: 0.623333.\n",
      "Step 355, loss: 1.233942, accurancy: 0.616667.\n",
      "Step 356, loss: 1.269555, accurancy: 0.593333.\n",
      "Step 357, loss: 1.185378, accurancy: 0.633333.\n",
      "Step 358, loss: 1.240300, accurancy: 0.630000.\n",
      "Step 359, loss: 1.207038, accurancy: 0.656667.\n",
      "Step 360, loss: 1.105383, accurancy: 0.650000.\n",
      "Step 361, loss: 1.213288, accurancy: 0.583333.\n",
      "Step 362, loss: 1.268394, accurancy: 0.596667.\n",
      "Step 363, loss: 1.227156, accurancy: 0.660000.\n",
      "Step 364, loss: 1.246258, accurancy: 0.590000.\n",
      "Step 365, loss: 1.258772, accurancy: 0.556667.\n",
      "Step 366, loss: 1.210270, accurancy: 0.623333.\n",
      "Step 367, loss: 1.202191, accurancy: 0.613333.\n",
      "Step 368, loss: 1.282176, accurancy: 0.600000.\n",
      "Step 369, loss: 1.109934, accurancy: 0.663333.\n",
      "Step 370, loss: 1.172416, accurancy: 0.600000.\n",
      "Step 371, loss: 1.254673, accurancy: 0.623333.\n",
      "Step 372, loss: 1.162753, accurancy: 0.613333.\n",
      "Step 373, loss: 1.174900, accurancy: 0.640000.\n",
      "Step 374, loss: 1.143774, accurancy: 0.646667.\n",
      "Step 375, loss: 1.347245, accurancy: 0.590000.\n",
      "Step 376, loss: 1.271436, accurancy: 0.560000.\n",
      "Step 377, loss: 1.259820, accurancy: 0.583333.\n",
      "Step 378, loss: 1.304285, accurancy: 0.606667.\n",
      "Step 379, loss: 1.278627, accurancy: 0.586667.\n",
      "Step 380, loss: 1.282564, accurancy: 0.590000.\n",
      "Step 381, loss: 1.165763, accurancy: 0.643333.\n",
      "Step 382, loss: 1.332572, accurancy: 0.556667.\n",
      "Step 383, loss: 1.218085, accurancy: 0.643333.\n",
      "Step 384, loss: 1.270400, accurancy: 0.613333.\n",
      "Step 385, loss: 1.296380, accurancy: 0.570000.\n",
      "Step 386, loss: 1.333857, accurancy: 0.570000.\n",
      "Step 387, loss: 1.194767, accurancy: 0.620000.\n",
      "Step 388, loss: 1.245426, accurancy: 0.613333.\n",
      "Step 389, loss: 1.279024, accurancy: 0.583333.\n",
      "Step 390, loss: 1.191071, accurancy: 0.633333.\n",
      "Step 391, loss: 1.280521, accurancy: 0.573333.\n",
      "Step 392, loss: 1.290165, accurancy: 0.603333.\n",
      "Step 393, loss: 1.233056, accurancy: 0.603333.\n",
      "Step 394, loss: 1.275696, accurancy: 0.590000.\n",
      "Step 395, loss: 1.220709, accurancy: 0.606667.\n",
      "Step 396, loss: 1.231444, accurancy: 0.586667.\n",
      "Step 397, loss: 1.255398, accurancy: 0.620000.\n",
      "Step 398, loss: 1.305497, accurancy: 0.590000.\n",
      "Step 399, loss: 1.188414, accurancy: 0.633333.\n",
      "Step 400, loss: 1.119934, accurancy: 0.670000.\n",
      "Validation accurancy is 0.646667\n",
      "Step 401, loss: 1.284780, accurancy: 0.563333.\n",
      "Step 402, loss: 1.182192, accurancy: 0.620000.\n",
      "Step 403, loss: 1.231130, accurancy: 0.616667.\n",
      "Step 404, loss: 1.171003, accurancy: 0.606667.\n",
      "Step 405, loss: 1.255819, accurancy: 0.566667.\n",
      "Step 406, loss: 1.042891, accurancy: 0.633333.\n",
      "Step 407, loss: 1.092567, accurancy: 0.666667.\n",
      "Step 408, loss: 1.337948, accurancy: 0.590000.\n",
      "Step 409, loss: 1.187085, accurancy: 0.620000.\n",
      "Step 410, loss: 1.164021, accurancy: 0.630000.\n",
      "Step 411, loss: 1.241018, accurancy: 0.606667.\n",
      "Step 412, loss: 1.274935, accurancy: 0.600000.\n",
      "Step 413, loss: 1.170233, accurancy: 0.650000.\n",
      "Step 414, loss: 1.184807, accurancy: 0.600000.\n",
      "Step 415, loss: 1.373818, accurancy: 0.563333.\n",
      "Step 416, loss: 1.140460, accurancy: 0.623333.\n",
      "Step 417, loss: 1.289511, accurancy: 0.586667.\n",
      "Step 418, loss: 1.193957, accurancy: 0.586667.\n",
      "Step 419, loss: 1.192082, accurancy: 0.606667.\n",
      "Step 420, loss: 1.021906, accurancy: 0.686667.\n",
      "Step 421, loss: 1.146621, accurancy: 0.636667.\n",
      "Step 422, loss: 1.287681, accurancy: 0.606667.\n",
      "Step 423, loss: 1.225044, accurancy: 0.623333.\n",
      "Step 424, loss: 1.295413, accurancy: 0.576667.\n",
      "Step 425, loss: 1.170441, accurancy: 0.650000.\n",
      "Step 426, loss: 1.284585, accurancy: 0.586667.\n",
      "Step 427, loss: 1.169854, accurancy: 0.663333.\n",
      "Step 428, loss: 1.135990, accurancy: 0.630000.\n",
      "Step 429, loss: 1.192531, accurancy: 0.623333.\n",
      "Step 430, loss: 1.267808, accurancy: 0.606667.\n",
      "Step 431, loss: 1.177717, accurancy: 0.670000.\n",
      "Step 432, loss: 1.357673, accurancy: 0.560000.\n",
      "Step 433, loss: 1.193212, accurancy: 0.600000.\n",
      "Step 434, loss: 1.214598, accurancy: 0.650000.\n",
      "Step 435, loss: 1.292604, accurancy: 0.623333.\n",
      "Step 436, loss: 1.181413, accurancy: 0.620000.\n",
      "Step 437, loss: 1.038908, accurancy: 0.676667.\n",
      "Step 438, loss: 1.188673, accurancy: 0.630000.\n",
      "Step 439, loss: 1.284989, accurancy: 0.603333.\n",
      "Step 440, loss: 1.145565, accurancy: 0.643333.\n",
      "Step 441, loss: 1.130265, accurancy: 0.646667.\n",
      "Step 442, loss: 1.280910, accurancy: 0.570000.\n",
      "Step 443, loss: 1.343741, accurancy: 0.583333.\n",
      "Step 444, loss: 1.327937, accurancy: 0.573333.\n",
      "Step 445, loss: 1.323784, accurancy: 0.576667.\n",
      "Step 446, loss: 1.230038, accurancy: 0.610000.\n",
      "Step 447, loss: 1.399264, accurancy: 0.563333.\n",
      "Step 448, loss: 1.167573, accurancy: 0.623333.\n",
      "Step 449, loss: 1.275432, accurancy: 0.593333.\n",
      "Step 450, loss: 1.251306, accurancy: 0.620000.\n",
      "Step 451, loss: 1.218298, accurancy: 0.626667.\n",
      "Step 452, loss: 1.297937, accurancy: 0.576667.\n",
      "Step 453, loss: 1.294909, accurancy: 0.576667.\n",
      "Step 454, loss: 1.292151, accurancy: 0.573333.\n",
      "Step 455, loss: 1.206038, accurancy: 0.613333.\n",
      "Step 456, loss: 1.230940, accurancy: 0.620000.\n",
      "Step 457, loss: 1.175672, accurancy: 0.603333.\n",
      "Step 458, loss: 1.201372, accurancy: 0.613333.\n",
      "Step 459, loss: 1.346291, accurancy: 0.563333.\n",
      "Step 460, loss: 1.284626, accurancy: 0.576667.\n",
      "Step 461, loss: 1.254748, accurancy: 0.580000.\n",
      "Step 462, loss: 1.267721, accurancy: 0.583333.\n",
      "Step 463, loss: 1.159459, accurancy: 0.596667.\n",
      "Step 464, loss: 1.334094, accurancy: 0.580000.\n",
      "Step 465, loss: 1.171743, accurancy: 0.653333.\n",
      "Step 466, loss: 1.232476, accurancy: 0.620000.\n",
      "Step 467, loss: 1.161141, accurancy: 0.616667.\n",
      "Step 468, loss: 1.184858, accurancy: 0.613333.\n",
      "Step 469, loss: 1.180096, accurancy: 0.613333.\n",
      "Step 470, loss: 1.220861, accurancy: 0.616667.\n",
      "Step 471, loss: 1.209130, accurancy: 0.623333.\n",
      "Step 472, loss: 1.229425, accurancy: 0.623333.\n",
      "Step 473, loss: 1.155743, accurancy: 0.615686.\n",
      "Step 474, loss: 1.187197, accurancy: 0.640000.\n",
      "Step 475, loss: 1.147880, accurancy: 0.623333.\n",
      "Step 476, loss: 1.253567, accurancy: 0.603333.\n",
      "Step 477, loss: 1.186282, accurancy: 0.633333.\n",
      "Step 478, loss: 1.227411, accurancy: 0.613333.\n",
      "Step 479, loss: 1.260141, accurancy: 0.626667.\n",
      "Step 480, loss: 1.238652, accurancy: 0.563333.\n",
      "Step 481, loss: 1.183310, accurancy: 0.636667.\n",
      "Step 482, loss: 1.320013, accurancy: 0.586667.\n",
      "Step 483, loss: 1.280044, accurancy: 0.606667.\n",
      "Step 484, loss: 1.165449, accurancy: 0.646667.\n",
      "Step 485, loss: 1.298643, accurancy: 0.600000.\n",
      "Step 486, loss: 1.184204, accurancy: 0.633333.\n",
      "Step 487, loss: 1.174948, accurancy: 0.620000.\n",
      "Step 488, loss: 1.176037, accurancy: 0.630000.\n",
      "Step 489, loss: 1.163120, accurancy: 0.616667.\n",
      "Step 490, loss: 1.229555, accurancy: 0.603333.\n",
      "Step 491, loss: 1.374277, accurancy: 0.583333.\n",
      "Step 492, loss: 1.221798, accurancy: 0.636667.\n",
      "Step 493, loss: 1.268184, accurancy: 0.586667.\n",
      "Step 494, loss: 1.259796, accurancy: 0.626667.\n",
      "Step 495, loss: 1.071526, accurancy: 0.666667.\n",
      "Step 496, loss: 1.168132, accurancy: 0.613333.\n",
      "Step 497, loss: 1.198489, accurancy: 0.643333.\n",
      "Step 498, loss: 1.291993, accurancy: 0.660000.\n",
      "Step 499, loss: 1.155648, accurancy: 0.633333.\n",
      "Step 500, loss: 1.304792, accurancy: 0.583333.\n",
      "Validation accurancy is 0.603333\n",
      "Step 501, loss: 1.236530, accurancy: 0.626667.\n",
      "Step 502, loss: 1.207200, accurancy: 0.613333.\n",
      "Step 503, loss: 1.268946, accurancy: 0.623333.\n",
      "Step 504, loss: 1.142374, accurancy: 0.676667.\n",
      "Step 505, loss: 1.173945, accurancy: 0.640000.\n",
      "Step 506, loss: 1.265875, accurancy: 0.643333.\n",
      "Step 507, loss: 1.213231, accurancy: 0.613333.\n",
      "Step 508, loss: 1.195856, accurancy: 0.603333.\n",
      "Step 509, loss: 1.169804, accurancy: 0.613333.\n",
      "Step 510, loss: 1.270853, accurancy: 0.556667.\n",
      "Step 511, loss: 1.286747, accurancy: 0.573333.\n",
      "Step 512, loss: 1.275424, accurancy: 0.570000.\n",
      "Step 513, loss: 1.345501, accurancy: 0.573333.\n",
      "Step 514, loss: 1.323065, accurancy: 0.560000.\n",
      "Step 515, loss: 1.270449, accurancy: 0.596667.\n",
      "Step 516, loss: 1.140727, accurancy: 0.633333.\n",
      "Step 517, loss: 1.301283, accurancy: 0.563333.\n",
      "Step 518, loss: 1.262487, accurancy: 0.643333.\n",
      "Step 519, loss: 1.263944, accurancy: 0.566667.\n",
      "Step 520, loss: 1.315361, accurancy: 0.543333.\n",
      "Step 521, loss: 1.307878, accurancy: 0.583333.\n",
      "Step 522, loss: 1.216836, accurancy: 0.576667.\n",
      "Step 523, loss: 1.189776, accurancy: 0.623333.\n",
      "Step 524, loss: 1.277859, accurancy: 0.596667.\n",
      "Step 525, loss: 1.233146, accurancy: 0.603333.\n",
      "Step 526, loss: 1.288276, accurancy: 0.543333.\n",
      "Step 527, loss: 1.300628, accurancy: 0.593333.\n",
      "Step 528, loss: 1.236863, accurancy: 0.596667.\n",
      "Step 529, loss: 1.253120, accurancy: 0.606667.\n",
      "Step 530, loss: 1.209984, accurancy: 0.630000.\n",
      "Step 531, loss: 1.119175, accurancy: 0.643333.\n",
      "Step 532, loss: 1.300881, accurancy: 0.593333.\n",
      "Step 533, loss: 1.305429, accurancy: 0.590000.\n",
      "Step 534, loss: 1.145384, accurancy: 0.630000.\n",
      "Step 535, loss: 1.109957, accurancy: 0.660000.\n",
      "Step 536, loss: 1.266450, accurancy: 0.600000.\n",
      "Step 537, loss: 1.187577, accurancy: 0.630000.\n",
      "Step 538, loss: 1.275279, accurancy: 0.590000.\n",
      "Step 539, loss: 1.122811, accurancy: 0.643333.\n",
      "Step 540, loss: 1.198187, accurancy: 0.630000.\n",
      "Step 541, loss: 1.098716, accurancy: 0.666667.\n",
      "Step 542, loss: 1.107693, accurancy: 0.650000.\n",
      "Step 543, loss: 1.330130, accurancy: 0.576667.\n",
      "Step 544, loss: 1.163669, accurancy: 0.643333.\n",
      "Step 545, loss: 1.135676, accurancy: 0.640000.\n",
      "Step 546, loss: 1.259064, accurancy: 0.580000.\n",
      "Step 547, loss: 1.227249, accurancy: 0.616667.\n",
      "Step 548, loss: 1.274485, accurancy: 0.613333.\n",
      "Step 549, loss: 1.129688, accurancy: 0.626667.\n",
      "Step 550, loss: 1.324218, accurancy: 0.580000.\n",
      "Step 551, loss: 1.181217, accurancy: 0.646667.\n",
      "Step 552, loss: 1.264415, accurancy: 0.566667.\n",
      "Step 553, loss: 1.177192, accurancy: 0.610000.\n",
      "Step 554, loss: 1.188756, accurancy: 0.603333.\n",
      "Step 555, loss: 1.052207, accurancy: 0.670000.\n",
      "Step 556, loss: 1.152184, accurancy: 0.636667.\n",
      "Step 557, loss: 1.237479, accurancy: 0.603333.\n",
      "Step 558, loss: 1.238141, accurancy: 0.633333.\n",
      "Step 559, loss: 1.249249, accurancy: 0.626667.\n",
      "Step 560, loss: 1.217376, accurancy: 0.640000.\n",
      "Step 561, loss: 1.274516, accurancy: 0.603333.\n",
      "Step 562, loss: 1.166494, accurancy: 0.686667.\n",
      "Step 563, loss: 1.089524, accurancy: 0.656667.\n",
      "Step 564, loss: 1.224291, accurancy: 0.586667.\n",
      "Step 565, loss: 1.192000, accurancy: 0.646667.\n",
      "Step 566, loss: 1.119701, accurancy: 0.666667.\n",
      "Step 567, loss: 1.289868, accurancy: 0.610000.\n",
      "Step 568, loss: 1.240610, accurancy: 0.576667.\n",
      "Step 569, loss: 1.112132, accurancy: 0.683333.\n",
      "Step 570, loss: 1.289375, accurancy: 0.583333.\n",
      "Step 571, loss: 1.190198, accurancy: 0.623333.\n",
      "Step 572, loss: 1.067838, accurancy: 0.716667.\n",
      "Step 573, loss: 1.184730, accurancy: 0.650000.\n",
      "Step 574, loss: 1.266629, accurancy: 0.610000.\n",
      "Step 575, loss: 1.213259, accurancy: 0.620000.\n",
      "Step 576, loss: 1.139969, accurancy: 0.640000.\n",
      "Step 577, loss: 1.154946, accurancy: 0.626667.\n",
      "Step 578, loss: 1.351083, accurancy: 0.566667.\n",
      "Step 579, loss: 1.284215, accurancy: 0.600000.\n",
      "Step 580, loss: 1.267883, accurancy: 0.606667.\n",
      "Step 581, loss: 1.297100, accurancy: 0.613333.\n",
      "Step 582, loss: 1.395597, accurancy: 0.566667.\n",
      "Step 583, loss: 1.173292, accurancy: 0.633333.\n",
      "Step 584, loss: 1.256954, accurancy: 0.620000.\n",
      "Step 585, loss: 1.254013, accurancy: 0.616667.\n",
      "Step 586, loss: 1.308037, accurancy: 0.640000.\n",
      "Step 587, loss: 1.231285, accurancy: 0.610000.\n",
      "Step 588, loss: 1.279834, accurancy: 0.566667.\n",
      "Step 589, loss: 1.263247, accurancy: 0.626667.\n",
      "Step 590, loss: 1.224531, accurancy: 0.606667.\n",
      "Step 591, loss: 1.210982, accurancy: 0.573333.\n",
      "Step 592, loss: 1.251886, accurancy: 0.616667.\n",
      "Step 593, loss: 1.186204, accurancy: 0.606667.\n",
      "Step 594, loss: 1.240157, accurancy: 0.590000.\n",
      "Step 595, loss: 1.364293, accurancy: 0.546667.\n",
      "Step 596, loss: 1.213031, accurancy: 0.626667.\n",
      "Step 597, loss: 1.312994, accurancy: 0.573333.\n",
      "Step 598, loss: 1.178613, accurancy: 0.656667.\n",
      "Step 599, loss: 1.227482, accurancy: 0.613333.\n",
      "Step 600, loss: 1.201244, accurancy: 0.590000.\n",
      "Validation accurancy is 0.560000\n",
      "Step 601, loss: 1.249573, accurancy: 0.613333.\n",
      "Step 602, loss: 1.142012, accurancy: 0.623333.\n",
      "Step 603, loss: 1.165683, accurancy: 0.663333.\n",
      "Step 604, loss: 1.212526, accurancy: 0.636667.\n",
      "Step 605, loss: 1.242646, accurancy: 0.603333.\n",
      "Step 606, loss: 1.260353, accurancy: 0.620000.\n",
      "Step 607, loss: 1.173965, accurancy: 0.620000.\n",
      "Step 608, loss: 1.143062, accurancy: 0.645614.\n",
      "Step 609, loss: 1.199504, accurancy: 0.593333.\n",
      "Step 610, loss: 1.129982, accurancy: 0.630000.\n",
      "Step 611, loss: 1.274965, accurancy: 0.593333.\n",
      "Step 612, loss: 1.156800, accurancy: 0.633333.\n",
      "Step 613, loss: 1.236327, accurancy: 0.580000.\n",
      "Step 614, loss: 1.183778, accurancy: 0.626667.\n",
      "Step 615, loss: 1.252707, accurancy: 0.626667.\n",
      "Step 616, loss: 1.203564, accurancy: 0.626667.\n",
      "Step 617, loss: 1.247455, accurancy: 0.606667.\n",
      "Step 618, loss: 1.325852, accurancy: 0.586667.\n",
      "Step 619, loss: 1.157268, accurancy: 0.606667.\n",
      "Step 620, loss: 1.299357, accurancy: 0.623333.\n",
      "Step 621, loss: 1.183551, accurancy: 0.603333.\n",
      "Step 622, loss: 1.158069, accurancy: 0.630000.\n",
      "Step 623, loss: 1.063123, accurancy: 0.673333.\n",
      "Step 624, loss: 1.134995, accurancy: 0.636667.\n",
      "Step 625, loss: 1.218029, accurancy: 0.613333.\n",
      "Step 626, loss: 1.323247, accurancy: 0.590000.\n",
      "Step 627, loss: 1.191300, accurancy: 0.623333.\n",
      "Step 628, loss: 1.254173, accurancy: 0.616667.\n",
      "Step 629, loss: 1.220327, accurancy: 0.623333.\n",
      "Step 630, loss: 1.087631, accurancy: 0.663333.\n",
      "Step 631, loss: 1.129660, accurancy: 0.620000.\n",
      "Step 632, loss: 1.178956, accurancy: 0.620000.\n",
      "Step 633, loss: 1.242321, accurancy: 0.646667.\n",
      "Step 634, loss: 1.161018, accurancy: 0.626667.\n",
      "Step 635, loss: 1.296271, accurancy: 0.556667.\n",
      "Step 636, loss: 1.182421, accurancy: 0.623333.\n",
      "Step 637, loss: 1.236859, accurancy: 0.606667.\n",
      "Step 638, loss: 1.287732, accurancy: 0.596667.\n",
      "Step 639, loss: 1.179629, accurancy: 0.666667.\n",
      "Step 640, loss: 1.070893, accurancy: 0.650000.\n",
      "Step 641, loss: 1.177522, accurancy: 0.663333.\n",
      "Step 642, loss: 1.237738, accurancy: 0.606667.\n",
      "Step 643, loss: 1.192247, accurancy: 0.646667.\n",
      "Step 644, loss: 1.146169, accurancy: 0.590000.\n",
      "Step 645, loss: 1.218046, accurancy: 0.563333.\n",
      "Step 646, loss: 1.360593, accurancy: 0.563333.\n",
      "Step 647, loss: 1.219364, accurancy: 0.583333.\n",
      "Step 648, loss: 1.362001, accurancy: 0.573333.\n",
      "Step 649, loss: 1.304812, accurancy: 0.593333.\n",
      "Step 650, loss: 1.290590, accurancy: 0.606667.\n",
      "Step 651, loss: 1.126907, accurancy: 0.636667.\n",
      "Step 652, loss: 1.272350, accurancy: 0.600000.\n",
      "Step 653, loss: 1.224088, accurancy: 0.633333.\n",
      "Step 654, loss: 1.223823, accurancy: 0.613333.\n",
      "Step 655, loss: 1.296502, accurancy: 0.583333.\n",
      "Step 656, loss: 1.307232, accurancy: 0.560000.\n",
      "Step 657, loss: 1.197490, accurancy: 0.590000.\n",
      "Step 658, loss: 1.175678, accurancy: 0.646667.\n",
      "Step 659, loss: 1.289323, accurancy: 0.623333.\n",
      "Step 660, loss: 1.212499, accurancy: 0.600000.\n",
      "Step 661, loss: 1.218406, accurancy: 0.593333.\n",
      "Step 662, loss: 1.251255, accurancy: 0.670000.\n",
      "Step 663, loss: 1.205574, accurancy: 0.610000.\n",
      "Step 664, loss: 1.359581, accurancy: 0.530000.\n",
      "Step 665, loss: 1.230204, accurancy: 0.623333.\n",
      "Step 666, loss: 1.189929, accurancy: 0.600000.\n",
      "Step 667, loss: 1.327412, accurancy: 0.596667.\n",
      "Step 668, loss: 1.223623, accurancy: 0.600000.\n",
      "Step 669, loss: 1.188260, accurancy: 0.640000.\n",
      "Step 670, loss: 1.133917, accurancy: 0.693333.\n",
      "Step 671, loss: 1.264966, accurancy: 0.606667.\n",
      "Step 672, loss: 1.181718, accurancy: 0.593333.\n",
      "Step 673, loss: 1.266451, accurancy: 0.606667.\n",
      "Step 674, loss: 1.089046, accurancy: 0.643333.\n",
      "Step 675, loss: 1.226290, accurancy: 0.623333.\n",
      "Step 676, loss: 1.079726, accurancy: 0.693333.\n",
      "Step 677, loss: 1.112165, accurancy: 0.653333.\n",
      "Step 678, loss: 1.210524, accurancy: 0.636667.\n",
      "Step 679, loss: 1.271723, accurancy: 0.593333.\n",
      "Step 680, loss: 1.153401, accurancy: 0.666667.\n",
      "Step 681, loss: 1.288278, accurancy: 0.586667.\n",
      "Step 682, loss: 1.244512, accurancy: 0.616667.\n",
      "Step 683, loss: 1.305629, accurancy: 0.590000.\n",
      "Step 684, loss: 1.122496, accurancy: 0.616667.\n",
      "Step 685, loss: 1.314239, accurancy: 0.573333.\n",
      "Step 686, loss: 1.146806, accurancy: 0.633333.\n",
      "Step 687, loss: 1.310421, accurancy: 0.586667.\n",
      "Step 688, loss: 1.137376, accurancy: 0.630000.\n",
      "Step 689, loss: 1.162318, accurancy: 0.613333.\n",
      "Step 690, loss: 1.104511, accurancy: 0.653333.\n",
      "Step 691, loss: 1.114603, accurancy: 0.640000.\n",
      "Step 692, loss: 1.256901, accurancy: 0.583333.\n",
      "Step 693, loss: 1.205187, accurancy: 0.660000.\n",
      "Step 694, loss: 1.267419, accurancy: 0.620000.\n",
      "Step 695, loss: 1.162994, accurancy: 0.640000.\n",
      "Step 696, loss: 1.257114, accurancy: 0.606667.\n",
      "Step 697, loss: 1.162428, accurancy: 0.663333.\n",
      "Step 698, loss: 1.121501, accurancy: 0.630000.\n",
      "Step 699, loss: 1.224917, accurancy: 0.576667.\n",
      "Step 700, loss: 1.230007, accurancy: 0.586667.\n",
      "Validation accurancy is 0.553333\n",
      "Step 701, loss: 1.166304, accurancy: 0.650000.\n",
      "Step 702, loss: 1.313425, accurancy: 0.603333.\n",
      "Step 703, loss: 1.235257, accurancy: 0.606667.\n",
      "Step 704, loss: 1.135883, accurancy: 0.656667.\n",
      "Step 705, loss: 1.244442, accurancy: 0.610000.\n",
      "Step 706, loss: 1.209990, accurancy: 0.643333.\n",
      "Step 707, loss: 1.121243, accurancy: 0.643333.\n",
      "Step 708, loss: 1.185251, accurancy: 0.623333.\n",
      "Step 709, loss: 1.331029, accurancy: 0.583333.\n",
      "Step 710, loss: 1.100230, accurancy: 0.640000.\n",
      "Step 711, loss: 1.198217, accurancy: 0.603333.\n",
      "Step 712, loss: 1.173004, accurancy: 0.623333.\n",
      "Step 713, loss: 1.293885, accurancy: 0.570000.\n",
      "Step 714, loss: 1.256413, accurancy: 0.603333.\n",
      "Step 715, loss: 1.222557, accurancy: 0.613333.\n",
      "Step 716, loss: 1.319564, accurancy: 0.603333.\n",
      "Step 717, loss: 1.375275, accurancy: 0.570000.\n",
      "Step 718, loss: 1.187395, accurancy: 0.650000.\n",
      "Step 719, loss: 1.250016, accurancy: 0.620000.\n",
      "Step 720, loss: 1.261133, accurancy: 0.563333.\n",
      "Step 721, loss: 1.281275, accurancy: 0.626667.\n",
      "Step 722, loss: 1.293333, accurancy: 0.583333.\n",
      "Step 723, loss: 1.217347, accurancy: 0.590000.\n",
      "Step 724, loss: 1.286314, accurancy: 0.583333.\n",
      "Step 725, loss: 1.238916, accurancy: 0.606667.\n",
      "Step 726, loss: 1.206094, accurancy: 0.603333.\n",
      "Step 727, loss: 1.263545, accurancy: 0.566667.\n",
      "Step 728, loss: 1.140692, accurancy: 0.633333.\n",
      "Step 729, loss: 1.262294, accurancy: 0.600000.\n",
      "Step 730, loss: 1.270013, accurancy: 0.596667.\n",
      "Step 731, loss: 1.257548, accurancy: 0.613333.\n",
      "Step 732, loss: 1.325882, accurancy: 0.566667.\n",
      "Step 733, loss: 1.221961, accurancy: 0.596667.\n",
      "Step 734, loss: 1.287446, accurancy: 0.580000.\n",
      "Step 735, loss: 1.143150, accurancy: 0.643333.\n",
      "Step 736, loss: 1.230377, accurancy: 0.610000.\n",
      "Step 737, loss: 1.116165, accurancy: 0.663333.\n",
      "Step 738, loss: 1.160791, accurancy: 0.616667.\n",
      "Step 739, loss: 1.221266, accurancy: 0.600000.\n",
      "Step 740, loss: 1.265547, accurancy: 0.586667.\n",
      "Step 741, loss: 1.165766, accurancy: 0.656667.\n",
      "Step 742, loss: 1.194145, accurancy: 0.626667.\n",
      "Step 743, loss: 1.138690, accurancy: 0.643333.\n",
      "Step 744, loss: 1.021404, accurancy: 0.600000.\n",
      "Step 745, loss: 1.049735, accurancy: 0.670000.\n",
      "Step 746, loss: 1.277157, accurancy: 0.600000.\n",
      "Step 747, loss: 1.228379, accurancy: 0.603333.\n",
      "Step 748, loss: 1.168918, accurancy: 0.620000.\n",
      "Step 749, loss: 1.209975, accurancy: 0.646667.\n",
      "Step 750, loss: 1.296586, accurancy: 0.600000.\n",
      "Step 751, loss: 1.202366, accurancy: 0.620000.\n",
      "Step 752, loss: 1.262324, accurancy: 0.586667.\n",
      "Step 753, loss: 1.362241, accurancy: 0.630000.\n",
      "Step 754, loss: 1.137888, accurancy: 0.626667.\n",
      "Step 755, loss: 1.330386, accurancy: 0.586667.\n",
      "Step 756, loss: 1.098882, accurancy: 0.616667.\n",
      "Step 757, loss: 1.230272, accurancy: 0.613333.\n",
      "Step 758, loss: 1.100708, accurancy: 0.673333.\n",
      "Step 759, loss: 1.101369, accurancy: 0.676667.\n",
      "Step 760, loss: 1.227135, accurancy: 0.590000.\n",
      "Step 761, loss: 1.351900, accurancy: 0.590000.\n",
      "Step 762, loss: 1.222635, accurancy: 0.646667.\n",
      "Step 763, loss: 1.229701, accurancy: 0.613333.\n",
      "Step 764, loss: 1.185648, accurancy: 0.656667.\n",
      "Step 765, loss: 1.092597, accurancy: 0.680000.\n",
      "Step 766, loss: 1.155378, accurancy: 0.606667.\n",
      "Step 767, loss: 1.157342, accurancy: 0.620000.\n",
      "Step 768, loss: 1.268313, accurancy: 0.603333.\n",
      "Step 769, loss: 1.152456, accurancy: 0.630000.\n",
      "Step 770, loss: 1.313047, accurancy: 0.580000.\n",
      "Step 771, loss: 1.201567, accurancy: 0.630000.\n",
      "Step 772, loss: 1.156957, accurancy: 0.630000.\n",
      "Step 773, loss: 1.283682, accurancy: 0.603333.\n",
      "Step 774, loss: 1.189860, accurancy: 0.643333.\n",
      "Step 775, loss: 1.088177, accurancy: 0.660000.\n",
      "Step 776, loss: 1.217880, accurancy: 0.633333.\n",
      "Step 777, loss: 1.276312, accurancy: 0.590000.\n",
      "Step 778, loss: 1.183565, accurancy: 0.636667.\n",
      "Step 779, loss: 1.188608, accurancy: 0.610000.\n",
      "Step 780, loss: 1.201958, accurancy: 0.610000.\n",
      "Step 781, loss: 1.313661, accurancy: 0.566667.\n",
      "Step 782, loss: 1.214096, accurancy: 0.610000.\n",
      "Step 783, loss: 1.373255, accurancy: 0.580000.\n",
      "Step 784, loss: 1.243044, accurancy: 0.606667.\n",
      "Step 785, loss: 1.308101, accurancy: 0.586667.\n",
      "Step 786, loss: 1.112595, accurancy: 0.636667.\n",
      "Step 787, loss: 1.293501, accurancy: 0.613333.\n",
      "Step 788, loss: 1.258303, accurancy: 0.643333.\n",
      "Step 789, loss: 1.211225, accurancy: 0.620000.\n",
      "Step 790, loss: 1.327960, accurancy: 0.556667.\n",
      "Step 791, loss: 1.255279, accurancy: 0.586667.\n",
      "Step 792, loss: 1.160288, accurancy: 0.610000.\n",
      "Step 793, loss: 1.213427, accurancy: 0.590000.\n",
      "Step 794, loss: 1.298204, accurancy: 0.570000.\n",
      "Step 795, loss: 1.185271, accurancy: 0.620000.\n",
      "Step 796, loss: 1.217357, accurancy: 0.613333.\n",
      "Step 797, loss: 1.278457, accurancy: 0.593333.\n",
      "Step 798, loss: 1.212054, accurancy: 0.636667.\n",
      "Step 799, loss: 1.348215, accurancy: 0.566667.\n",
      "Step 800, loss: 1.250589, accurancy: 0.593333.\n",
      "Validation accurancy is 0.636667\n",
      "Step 801, loss: 1.145332, accurancy: 0.616667.\n",
      "Step 802, loss: 1.283664, accurancy: 0.603333.\n",
      "Step 803, loss: 1.196065, accurancy: 0.613333.\n",
      "Step 804, loss: 1.280260, accurancy: 0.620000.\n",
      "Step 805, loss: 1.187446, accurancy: 0.646667.\n",
      "Step 806, loss: 1.179093, accurancy: 0.610000.\n",
      "Step 807, loss: 1.262120, accurancy: 0.600000.\n",
      "Step 808, loss: 1.218535, accurancy: 0.620000.\n",
      "Step 809, loss: 1.127875, accurancy: 0.610000.\n",
      "Step 810, loss: 1.195431, accurancy: 0.613333.\n",
      "Step 811, loss: 1.055093, accurancy: 0.694444.\n",
      "Step 812, loss: 1.194970, accurancy: 0.630000.\n",
      "Step 813, loss: 1.205504, accurancy: 0.603333.\n",
      "Step 814, loss: 1.228446, accurancy: 0.630000.\n",
      "Step 815, loss: 1.212343, accurancy: 0.606667.\n",
      "Step 816, loss: 1.234641, accurancy: 0.616667.\n",
      "Step 817, loss: 1.202741, accurancy: 0.623333.\n",
      "Step 818, loss: 1.293513, accurancy: 0.573333.\n",
      "Step 819, loss: 1.187348, accurancy: 0.656667.\n",
      "Step 820, loss: 1.241490, accurancy: 0.606667.\n",
      "Step 821, loss: 1.227558, accurancy: 0.610000.\n",
      "Step 822, loss: 1.268814, accurancy: 0.600000.\n",
      "Step 823, loss: 1.232818, accurancy: 0.590000.\n",
      "Step 824, loss: 1.207472, accurancy: 0.616667.\n",
      "Step 825, loss: 1.073439, accurancy: 0.636667.\n",
      "Step 826, loss: 1.116311, accurancy: 0.640000.\n",
      "Step 827, loss: 1.190625, accurancy: 0.603333.\n",
      "Step 828, loss: 1.215846, accurancy: 0.616667.\n",
      "Step 829, loss: 1.297155, accurancy: 0.606667.\n",
      "Step 830, loss: 1.223774, accurancy: 0.646667.\n",
      "Step 831, loss: 1.190638, accurancy: 0.626667.\n",
      "Step 832, loss: 1.150735, accurancy: 0.673333.\n",
      "Step 833, loss: 1.087458, accurancy: 0.670000.\n",
      "Step 834, loss: 1.133289, accurancy: 0.623333.\n",
      "Step 835, loss: 1.221719, accurancy: 0.613333.\n",
      "Step 836, loss: 1.168388, accurancy: 0.660000.\n",
      "Step 837, loss: 1.318604, accurancy: 0.590000.\n",
      "Step 838, loss: 1.197011, accurancy: 0.626667.\n",
      "Step 839, loss: 1.172864, accurancy: 0.623333.\n",
      "Step 840, loss: 1.231250, accurancy: 0.590000.\n",
      "Step 841, loss: 1.179223, accurancy: 0.643333.\n",
      "Step 842, loss: 1.082884, accurancy: 0.650000.\n",
      "Step 843, loss: 1.173810, accurancy: 0.600000.\n",
      "Step 844, loss: 1.231351, accurancy: 0.646667.\n",
      "Step 845, loss: 1.129603, accurancy: 0.666667.\n",
      "Step 846, loss: 1.220927, accurancy: 0.616667.\n",
      "Step 847, loss: 1.122531, accurancy: 0.603333.\n",
      "Step 848, loss: 1.292526, accurancy: 0.603333.\n",
      "Step 849, loss: 1.238447, accurancy: 0.563333.\n",
      "Step 850, loss: 1.238641, accurancy: 0.586667.\n",
      "Step 851, loss: 1.355448, accurancy: 0.586667.\n",
      "Step 852, loss: 1.317493, accurancy: 0.570000.\n",
      "Step 853, loss: 1.220396, accurancy: 0.610000.\n",
      "Step 854, loss: 1.231926, accurancy: 0.620000.\n",
      "Step 855, loss: 1.247198, accurancy: 0.576667.\n",
      "Step 856, loss: 1.262732, accurancy: 0.623333.\n",
      "Step 857, loss: 1.206213, accurancy: 0.626667.\n",
      "Step 858, loss: 1.264559, accurancy: 0.606667.\n",
      "Step 859, loss: 1.276237, accurancy: 0.600000.\n",
      "Step 860, loss: 1.229478, accurancy: 0.580000.\n",
      "Step 861, loss: 1.094198, accurancy: 0.643333.\n",
      "Step 862, loss: 1.257944, accurancy: 0.606667.\n",
      "Step 863, loss: 1.216041, accurancy: 0.606667.\n",
      "Step 864, loss: 1.242655, accurancy: 0.566667.\n",
      "Step 865, loss: 1.327005, accurancy: 0.583333.\n",
      "Step 866, loss: 1.218677, accurancy: 0.603333.\n",
      "Step 867, loss: 1.332939, accurancy: 0.573333.\n",
      "Step 868, loss: 1.209983, accurancy: 0.583333.\n",
      "Step 869, loss: 1.198843, accurancy: 0.636667.\n",
      "Step 870, loss: 1.155650, accurancy: 0.650000.\n",
      "Step 871, loss: 1.276431, accurancy: 0.580000.\n",
      "Step 872, loss: 1.153212, accurancy: 0.620000.\n",
      "Step 873, loss: 1.166675, accurancy: 0.610000.\n",
      "Step 874, loss: 1.198980, accurancy: 0.600000.\n",
      "Step 875, loss: 1.231247, accurancy: 0.606667.\n",
      "Step 876, loss: 1.191078, accurancy: 0.646667.\n",
      "Step 877, loss: 1.244645, accurancy: 0.580000.\n",
      "Step 878, loss: 1.154602, accurancy: 0.623333.\n",
      "Step 879, loss: 0.860067, accurancy: 0.622222.\n",
      "Step 880, loss: 1.061562, accurancy: 0.650000.\n",
      "Step 881, loss: 1.283919, accurancy: 0.616667.\n",
      "Step 882, loss: 1.131622, accurancy: 0.673333.\n",
      "Step 883, loss: 1.210138, accurancy: 0.623333.\n",
      "Step 884, loss: 1.139277, accurancy: 0.646667.\n",
      "Step 885, loss: 1.295086, accurancy: 0.606667.\n",
      "Step 886, loss: 1.143681, accurancy: 0.640000.\n",
      "Step 887, loss: 1.205203, accurancy: 0.620000.\n",
      "Step 888, loss: 1.367782, accurancy: 0.600000.\n",
      "Step 889, loss: 1.154889, accurancy: 0.630000.\n",
      "Step 890, loss: 1.298908, accurancy: 0.560000.\n",
      "Step 891, loss: 1.148168, accurancy: 0.603333.\n",
      "Step 892, loss: 1.240988, accurancy: 0.613333.\n",
      "Step 893, loss: 1.034865, accurancy: 0.686667.\n",
      "Step 894, loss: 1.093545, accurancy: 0.656667.\n",
      "Step 895, loss: 1.300423, accurancy: 0.586667.\n",
      "Step 896, loss: 1.247319, accurancy: 0.600000.\n",
      "Step 897, loss: 1.246819, accurancy: 0.630000.\n",
      "Step 898, loss: 1.183661, accurancy: 0.626667.\n",
      "Step 899, loss: 1.184945, accurancy: 0.643333.\n",
      "Step 900, loss: 1.180658, accurancy: 0.670000.\n",
      "Validation accurancy is 0.666667\n",
      "Step 901, loss: 1.110673, accurancy: 0.663333.\n",
      "Step 902, loss: 1.219847, accurancy: 0.590000.\n",
      "Step 903, loss: 1.208113, accurancy: 0.630000.\n",
      "Step 904, loss: 1.169529, accurancy: 0.670000.\n",
      "Step 905, loss: 1.313421, accurancy: 0.573333.\n",
      "Step 906, loss: 1.222809, accurancy: 0.613333.\n",
      "Step 907, loss: 1.105911, accurancy: 0.640000.\n",
      "Step 908, loss: 1.283368, accurancy: 0.600000.\n",
      "Step 909, loss: 1.124572, accurancy: 0.663333.\n",
      "Step 910, loss: 1.061526, accurancy: 0.696667.\n",
      "Step 911, loss: 1.177430, accurancy: 0.623333.\n",
      "Step 912, loss: 1.266772, accurancy: 0.603333.\n",
      "Step 913, loss: 1.184170, accurancy: 0.623333.\n",
      "Step 914, loss: 1.144808, accurancy: 0.643333.\n",
      "Step 915, loss: 1.213204, accurancy: 0.586667.\n",
      "Step 916, loss: 1.326269, accurancy: 0.543333.\n",
      "Step 917, loss: 1.282052, accurancy: 0.556667.\n",
      "Step 918, loss: 1.330671, accurancy: 0.573333.\n",
      "Step 919, loss: 1.208306, accurancy: 0.596667.\n",
      "Step 920, loss: 1.363881, accurancy: 0.576667.\n",
      "Step 921, loss: 1.137099, accurancy: 0.636667.\n",
      "Step 922, loss: 1.290345, accurancy: 0.596667.\n",
      "Step 923, loss: 1.241951, accurancy: 0.643333.\n",
      "Step 924, loss: 1.220385, accurancy: 0.643333.\n",
      "Step 925, loss: 1.247261, accurancy: 0.600000.\n",
      "Step 926, loss: 1.271093, accurancy: 0.556667.\n",
      "Step 927, loss: 1.167302, accurancy: 0.613333.\n",
      "Step 928, loss: 1.210211, accurancy: 0.613333.\n",
      "Step 929, loss: 1.265820, accurancy: 0.586667.\n",
      "Step 930, loss: 1.155365, accurancy: 0.626667.\n",
      "Step 931, loss: 1.214317, accurancy: 0.600000.\n",
      "Step 932, loss: 1.270177, accurancy: 0.600000.\n",
      "Step 933, loss: 1.216219, accurancy: 0.590000.\n",
      "Step 934, loss: 1.265379, accurancy: 0.570000.\n",
      "Step 935, loss: 1.205584, accurancy: 0.613333.\n",
      "Step 936, loss: 1.218537, accurancy: 0.586667.\n",
      "Step 937, loss: 1.248886, accurancy: 0.610000.\n",
      "Step 938, loss: 1.195098, accurancy: 0.603333.\n",
      "Step 939, loss: 1.258725, accurancy: 0.620000.\n",
      "Step 940, loss: 1.137884, accurancy: 0.650000.\n",
      "Step 941, loss: 1.153835, accurancy: 0.656667.\n",
      "Step 942, loss: 1.177276, accurancy: 0.610000.\n",
      "Step 943, loss: 1.259655, accurancy: 0.606667.\n",
      "Step 944, loss: 1.178797, accurancy: 0.630000.\n",
      "Step 945, loss: 1.214937, accurancy: 0.626667.\n",
      "Step 946, loss: 1.121143, accurancy: 0.647619.\n",
      "Step 947, loss: 1.156989, accurancy: 0.610000.\n",
      "Step 948, loss: 1.166078, accurancy: 0.653333.\n",
      "Step 949, loss: 1.194380, accurancy: 0.636667.\n",
      "Step 950, loss: 1.169125, accurancy: 0.623333.\n",
      "Step 951, loss: 1.200017, accurancy: 0.593333.\n",
      "Step 952, loss: 1.331339, accurancy: 0.593333.\n",
      "Step 953, loss: 1.220418, accurancy: 0.606667.\n",
      "Step 954, loss: 1.189490, accurancy: 0.646667.\n",
      "Step 955, loss: 1.240386, accurancy: 0.616667.\n",
      "Step 956, loss: 1.250724, accurancy: 0.626667.\n",
      "Step 957, loss: 1.209080, accurancy: 0.613333.\n",
      "Step 958, loss: 1.264001, accurancy: 0.566667.\n",
      "Step 959, loss: 1.228357, accurancy: 0.603333.\n",
      "Step 960, loss: 1.080253, accurancy: 0.696667.\n",
      "Step 961, loss: 1.096360, accurancy: 0.653333.\n",
      "Step 962, loss: 1.213709, accurancy: 0.606667.\n",
      "Step 963, loss: 1.181488, accurancy: 0.646667.\n",
      "Step 964, loss: 1.253145, accurancy: 0.613333.\n",
      "Step 965, loss: 1.204422, accurancy: 0.630000.\n",
      "Step 966, loss: 1.268353, accurancy: 0.600000.\n",
      "Step 967, loss: 1.171704, accurancy: 0.650000.\n",
      "Step 968, loss: 1.099236, accurancy: 0.656667.\n",
      "Step 969, loss: 1.172518, accurancy: 0.630000.\n",
      "Step 970, loss: 1.200633, accurancy: 0.633333.\n",
      "Step 971, loss: 1.226434, accurancy: 0.620000.\n",
      "Step 972, loss: 1.250015, accurancy: 0.623333.\n",
      "Step 973, loss: 1.160641, accurancy: 0.606667.\n",
      "Step 974, loss: 1.176395, accurancy: 0.676667.\n",
      "Step 975, loss: 1.260996, accurancy: 0.626667.\n",
      "Step 976, loss: 1.240069, accurancy: 0.633333.\n",
      "Step 977, loss: 1.138090, accurancy: 0.640000.\n",
      "Step 978, loss: 1.139767, accurancy: 0.643333.\n",
      "Step 979, loss: 1.231724, accurancy: 0.636667.\n",
      "Step 980, loss: 1.156415, accurancy: 0.630000.\n",
      "Step 981, loss: 1.212120, accurancy: 0.603333.\n",
      "Step 982, loss: 1.092884, accurancy: 0.666667.\n",
      "Step 983, loss: 1.383922, accurancy: 0.543333.\n",
      "Step 984, loss: 1.231102, accurancy: 0.596667.\n",
      "Step 985, loss: 1.180120, accurancy: 0.620000.\n",
      "Step 986, loss: 1.314453, accurancy: 0.610000.\n",
      "Step 987, loss: 1.368590, accurancy: 0.566667.\n",
      "Step 988, loss: 1.258694, accurancy: 0.613333.\n",
      "Step 989, loss: 1.150156, accurancy: 0.680000.\n",
      "Step 990, loss: 1.260473, accurancy: 0.596667.\n",
      "Step 991, loss: 1.208551, accurancy: 0.670000.\n",
      "Step 992, loss: 1.233017, accurancy: 0.586667.\n",
      "Step 993, loss: 1.307713, accurancy: 0.603333.\n",
      "Step 994, loss: 1.286780, accurancy: 0.593333.\n",
      "Step 995, loss: 1.181475, accurancy: 0.603333.\n",
      "Step 996, loss: 1.189469, accurancy: 0.610000.\n",
      "Step 997, loss: 1.184917, accurancy: 0.610000.\n",
      "Step 998, loss: 1.181157, accurancy: 0.623333.\n",
      "Step 999, loss: 1.209959, accurancy: 0.586667.\n",
      "Step 1000, loss: 1.292923, accurancy: 0.603333.\n",
      "Validation accurancy is 0.613333\n",
      "Step 1001, loss: 1.214623, accurancy: 0.596667.\n",
      "Step 1002, loss: 1.297253, accurancy: 0.600000.\n",
      "Step 1003, loss: 1.176683, accurancy: 0.620000.\n",
      "Step 1004, loss: 1.269650, accurancy: 0.593333.\n",
      "Step 1005, loss: 1.181239, accurancy: 0.633333.\n",
      "Step 1006, loss: 1.277585, accurancy: 0.603333.\n",
      "Step 1007, loss: 1.083567, accurancy: 0.680000.\n",
      "Step 1008, loss: 1.117221, accurancy: 0.636667.\n",
      "Step 1009, loss: 1.251761, accurancy: 0.590000.\n",
      "Step 1010, loss: 1.229877, accurancy: 0.626667.\n",
      "Step 1011, loss: 1.183939, accurancy: 0.640000.\n",
      "Step 1012, loss: 1.185424, accurancy: 0.606667.\n",
      "Step 1013, loss: 1.131173, accurancy: 0.613333.\n",
      "Step 1014, loss: 0.938744, accurancy: 0.680000.\n",
      "Step 1015, loss: 1.060688, accurancy: 0.663333.\n",
      "Step 1016, loss: 1.279399, accurancy: 0.636667.\n",
      "Step 1017, loss: 1.183718, accurancy: 0.640000.\n",
      "Step 1018, loss: 1.144116, accurancy: 0.640000.\n",
      "Step 1019, loss: 1.227212, accurancy: 0.623333.\n",
      "Step 1020, loss: 1.217838, accurancy: 0.646667.\n",
      "Step 1021, loss: 1.204345, accurancy: 0.616667.\n",
      "Step 1022, loss: 1.152490, accurancy: 0.616667.\n",
      "Step 1023, loss: 1.372630, accurancy: 0.566667.\n",
      "Step 1024, loss: 1.085832, accurancy: 0.660000.\n",
      "Step 1025, loss: 1.375533, accurancy: 0.576667.\n",
      "Step 1026, loss: 1.181333, accurancy: 0.630000.\n",
      "Step 1027, loss: 1.190950, accurancy: 0.620000.\n",
      "Step 1028, loss: 1.081986, accurancy: 0.626667.\n",
      "Step 1029, loss: 1.069335, accurancy: 0.660000.\n",
      "Step 1030, loss: 1.336331, accurancy: 0.533333.\n",
      "Step 1031, loss: 1.234474, accurancy: 0.636667.\n",
      "Step 1032, loss: 1.291025, accurancy: 0.596667.\n",
      "Step 1033, loss: 1.134040, accurancy: 0.660000.\n",
      "Step 1034, loss: 1.242085, accurancy: 0.600000.\n",
      "Step 1035, loss: 1.177517, accurancy: 0.670000.\n",
      "Step 1036, loss: 1.166526, accurancy: 0.623333.\n",
      "Step 1037, loss: 1.158555, accurancy: 0.653333.\n",
      "Step 1038, loss: 1.245495, accurancy: 0.600000.\n",
      "Step 1039, loss: 1.096739, accurancy: 0.686667.\n",
      "Step 1040, loss: 1.252278, accurancy: 0.596667.\n",
      "Step 1041, loss: 1.253963, accurancy: 0.566667.\n",
      "Step 1042, loss: 1.143006, accurancy: 0.666667.\n",
      "Step 1043, loss: 1.313605, accurancy: 0.593333.\n",
      "Step 1044, loss: 1.111410, accurancy: 0.683333.\n",
      "Step 1045, loss: 1.028506, accurancy: 0.683333.\n",
      "Step 1046, loss: 1.151884, accurancy: 0.640000.\n",
      "Step 1047, loss: 1.269825, accurancy: 0.620000.\n",
      "Step 1048, loss: 1.110949, accurancy: 0.660000.\n",
      "Step 1049, loss: 1.127156, accurancy: 0.626667.\n",
      "Step 1050, loss: 1.269789, accurancy: 0.563333.\n",
      "Step 1051, loss: 1.347827, accurancy: 0.570000.\n",
      "Step 1052, loss: 1.246214, accurancy: 0.573333.\n",
      "Step 1053, loss: 1.342240, accurancy: 0.590000.\n",
      "Step 1054, loss: 1.170441, accurancy: 0.616667.\n",
      "Step 1055, loss: 1.394261, accurancy: 0.540000.\n",
      "Step 1056, loss: 1.138586, accurancy: 0.646667.\n",
      "Step 1057, loss: 1.213118, accurancy: 0.583333.\n",
      "Step 1058, loss: 1.262964, accurancy: 0.620000.\n",
      "Step 1059, loss: 1.209335, accurancy: 0.626667.\n",
      "Step 1060, loss: 1.210279, accurancy: 0.606667.\n",
      "Step 1061, loss: 1.312604, accurancy: 0.550000.\n",
      "Step 1062, loss: 1.229339, accurancy: 0.636667.\n",
      "Step 1063, loss: 1.228406, accurancy: 0.586667.\n",
      "Step 1064, loss: 1.245696, accurancy: 0.580000.\n",
      "Step 1065, loss: 1.140691, accurancy: 0.626667.\n",
      "Step 1066, loss: 1.190378, accurancy: 0.610000.\n",
      "Step 1067, loss: 1.345295, accurancy: 0.556667.\n",
      "Step 1068, loss: 1.237654, accurancy: 0.586667.\n",
      "Step 1069, loss: 1.253234, accurancy: 0.616667.\n",
      "Step 1070, loss: 1.279035, accurancy: 0.580000.\n",
      "Step 1071, loss: 1.145766, accurancy: 0.630000.\n",
      "Step 1072, loss: 1.295598, accurancy: 0.590000.\n",
      "Step 1073, loss: 1.139533, accurancy: 0.663333.\n",
      "Step 1074, loss: 1.215581, accurancy: 0.626667.\n",
      "Step 1075, loss: 1.134478, accurancy: 0.676667.\n",
      "Step 1076, loss: 1.193892, accurancy: 0.633333.\n",
      "Step 1077, loss: 1.219190, accurancy: 0.640000.\n",
      "Step 1078, loss: 1.249326, accurancy: 0.613333.\n",
      "Step 1079, loss: 1.182514, accurancy: 0.613333.\n",
      "Step 1080, loss: 1.194960, accurancy: 0.623333.\n",
      "Step 1081, loss: 1.132611, accurancy: 0.629167.\n",
      "Step 1082, loss: 1.158164, accurancy: 0.623333.\n",
      "Step 1083, loss: 1.162918, accurancy: 0.653333.\n",
      "Step 1084, loss: 1.239284, accurancy: 0.600000.\n",
      "Step 1085, loss: 1.199183, accurancy: 0.593333.\n",
      "Step 1086, loss: 1.184972, accurancy: 0.586667.\n",
      "Step 1087, loss: 1.236627, accurancy: 0.670000.\n",
      "Step 1088, loss: 1.230556, accurancy: 0.613333.\n",
      "Step 1089, loss: 1.197421, accurancy: 0.613333.\n",
      "Step 1090, loss: 1.228720, accurancy: 0.610000.\n",
      "Step 1091, loss: 1.259763, accurancy: 0.630000.\n",
      "Step 1092, loss: 1.229548, accurancy: 0.593333.\n",
      "Step 1093, loss: 1.226398, accurancy: 0.583333.\n",
      "Step 1094, loss: 1.150707, accurancy: 0.616667.\n",
      "Step 1095, loss: 1.105756, accurancy: 0.613333.\n",
      "Step 1096, loss: 1.102461, accurancy: 0.650000.\n",
      "Step 1097, loss: 1.176959, accurancy: 0.620000.\n",
      "Step 1098, loss: 1.182584, accurancy: 0.650000.\n",
      "Step 1099, loss: 1.267517, accurancy: 0.586667.\n",
      "Step 1100, loss: 1.172959, accurancy: 0.643333.\n",
      "Validation accurancy is 0.633333\n",
      "Step 1101, loss: 1.231315, accurancy: 0.636667.\n",
      "Step 1102, loss: 1.186945, accurancy: 0.653333.\n",
      "Step 1103, loss: 1.079598, accurancy: 0.663333.\n",
      "Step 1104, loss: 1.191174, accurancy: 0.603333.\n",
      "Step 1105, loss: 1.204968, accurancy: 0.616667.\n",
      "Step 1106, loss: 1.233732, accurancy: 0.666667.\n",
      "Step 1107, loss: 1.224612, accurancy: 0.586667.\n",
      "Step 1108, loss: 1.246396, accurancy: 0.590000.\n",
      "Step 1109, loss: 1.192545, accurancy: 0.650000.\n",
      "Step 1110, loss: 1.194317, accurancy: 0.630000.\n",
      "Step 1111, loss: 1.251744, accurancy: 0.620000.\n",
      "Step 1112, loss: 1.077202, accurancy: 0.670000.\n",
      "Step 1113, loss: 1.168329, accurancy: 0.626667.\n",
      "Step 1114, loss: 1.241152, accurancy: 0.650000.\n",
      "Step 1115, loss: 1.154048, accurancy: 0.600000.\n",
      "Step 1116, loss: 1.189369, accurancy: 0.620000.\n",
      "Step 1117, loss: 1.127923, accurancy: 0.653333.\n",
      "Step 1118, loss: 1.283276, accurancy: 0.536667.\n",
      "Step 1119, loss: 1.229273, accurancy: 0.583333.\n",
      "Step 1120, loss: 1.233409, accurancy: 0.620000.\n",
      "Step 1121, loss: 1.307585, accurancy: 0.593333.\n",
      "Step 1122, loss: 1.295760, accurancy: 0.573333.\n",
      "Step 1123, loss: 1.236738, accurancy: 0.603333.\n",
      "Step 1124, loss: 1.189584, accurancy: 0.623333.\n",
      "Step 1125, loss: 1.267605, accurancy: 0.610000.\n",
      "Step 1126, loss: 1.181101, accurancy: 0.673333.\n",
      "Step 1127, loss: 1.200742, accurancy: 0.593333.\n",
      "Step 1128, loss: 1.374187, accurancy: 0.526667.\n",
      "Step 1129, loss: 1.273297, accurancy: 0.610000.\n",
      "Step 1130, loss: 1.163726, accurancy: 0.600000.\n",
      "Step 1131, loss: 1.153818, accurancy: 0.640000.\n",
      "Step 1132, loss: 1.234444, accurancy: 0.600000.\n",
      "Step 1133, loss: 1.165685, accurancy: 0.630000.\n",
      "Step 1134, loss: 1.208109, accurancy: 0.610000.\n",
      "Step 1135, loss: 1.281091, accurancy: 0.616667.\n",
      "Step 1136, loss: 1.187194, accurancy: 0.640000.\n",
      "Step 1137, loss: 1.328192, accurancy: 0.573333.\n",
      "Step 1138, loss: 1.202044, accurancy: 0.636667.\n",
      "Step 1139, loss: 1.155196, accurancy: 0.583333.\n",
      "Step 1140, loss: 1.217589, accurancy: 0.633333.\n",
      "Step 1141, loss: 1.215960, accurancy: 0.610000.\n",
      "Step 1142, loss: 1.111248, accurancy: 0.683333.\n",
      "Step 1143, loss: 1.086084, accurancy: 0.660000.\n",
      "Step 1144, loss: 1.240415, accurancy: 0.583333.\n",
      "Step 1145, loss: 1.208479, accurancy: 0.620000.\n",
      "Step 1146, loss: 1.229250, accurancy: 0.640000.\n",
      "Step 1147, loss: 1.137614, accurancy: 0.630000.\n",
      "Step 1148, loss: 1.189506, accurancy: 0.630000.\n",
      "Step 1149, loss: 1.074162, accurancy: 0.647619.\n",
      "Step 1150, loss: 1.090189, accurancy: 0.660000.\n",
      "Step 1151, loss: 1.286120, accurancy: 0.596667.\n",
      "Step 1152, loss: 1.164681, accurancy: 0.620000.\n",
      "Step 1153, loss: 1.187366, accurancy: 0.613333.\n",
      "Step 1154, loss: 1.258529, accurancy: 0.586667.\n",
      "Step 1155, loss: 1.225935, accurancy: 0.636667.\n",
      "Step 1156, loss: 1.223297, accurancy: 0.620000.\n",
      "Step 1157, loss: 1.160774, accurancy: 0.636667.\n",
      "Step 1158, loss: 1.350896, accurancy: 0.583333.\n",
      "Step 1159, loss: 1.119234, accurancy: 0.643333.\n",
      "Step 1160, loss: 1.230651, accurancy: 0.613333.\n",
      "Step 1161, loss: 1.175616, accurancy: 0.610000.\n",
      "Step 1162, loss: 1.136183, accurancy: 0.653333.\n",
      "Step 1163, loss: 1.033378, accurancy: 0.663333.\n",
      "Step 1164, loss: 1.104975, accurancy: 0.623333.\n",
      "Step 1165, loss: 1.205575, accurancy: 0.583333.\n",
      "Step 1166, loss: 1.234929, accurancy: 0.650000.\n",
      "Step 1167, loss: 1.275934, accurancy: 0.586667.\n",
      "Step 1168, loss: 1.123623, accurancy: 0.660000.\n",
      "Step 1169, loss: 1.308020, accurancy: 0.600000.\n",
      "Step 1170, loss: 1.126513, accurancy: 0.640000.\n",
      "Step 1171, loss: 1.099622, accurancy: 0.643333.\n",
      "Step 1172, loss: 1.145971, accurancy: 0.623333.\n",
      "Step 1173, loss: 1.206154, accurancy: 0.620000.\n",
      "Step 1174, loss: 1.141777, accurancy: 0.646667.\n",
      "Step 1175, loss: 1.267524, accurancy: 0.603333.\n",
      "Step 1176, loss: 1.219742, accurancy: 0.596667.\n",
      "Step 1177, loss: 1.163590, accurancy: 0.656667.\n",
      "Step 1178, loss: 1.286557, accurancy: 0.606667.\n",
      "Step 1179, loss: 1.164765, accurancy: 0.640000.\n",
      "Step 1180, loss: 1.064421, accurancy: 0.696667.\n",
      "Step 1181, loss: 1.179448, accurancy: 0.636667.\n",
      "Step 1182, loss: 1.262287, accurancy: 0.623333.\n",
      "Step 1183, loss: 1.152888, accurancy: 0.626667.\n",
      "Step 1184, loss: 1.138464, accurancy: 0.653333.\n",
      "Step 1185, loss: 1.174309, accurancy: 0.620000.\n",
      "Step 1186, loss: 1.266554, accurancy: 0.583333.\n",
      "Step 1187, loss: 1.263451, accurancy: 0.613333.\n",
      "Step 1188, loss: 1.199727, accurancy: 0.620000.\n",
      "Step 1189, loss: 1.324236, accurancy: 0.573333.\n",
      "Step 1190, loss: 1.302758, accurancy: 0.593333.\n",
      "Step 1191, loss: 1.124168, accurancy: 0.636667.\n",
      "Step 1192, loss: 1.312722, accurancy: 0.570000.\n",
      "Step 1193, loss: 1.283631, accurancy: 0.603333.\n",
      "Step 1194, loss: 1.250207, accurancy: 0.633333.\n",
      "Step 1195, loss: 1.198778, accurancy: 0.610000.\n",
      "Step 1196, loss: 1.261441, accurancy: 0.593333.\n",
      "Step 1197, loss: 1.225706, accurancy: 0.623333.\n",
      "Step 1198, loss: 1.213194, accurancy: 0.586667.\n",
      "Step 1199, loss: 1.199352, accurancy: 0.593333.\n",
      "Step 1200, loss: 1.195372, accurancy: 0.630000.\n",
      "Validation accurancy is 0.620000\n",
      "Step 1201, loss: 1.190108, accurancy: 0.613333.\n",
      "Step 1202, loss: 1.232620, accurancy: 0.596667.\n",
      "Step 1203, loss: 1.333970, accurancy: 0.556667.\n",
      "Step 1204, loss: 1.218601, accurancy: 0.626667.\n",
      "Step 1205, loss: 1.276877, accurancy: 0.596667.\n",
      "Step 1206, loss: 1.167562, accurancy: 0.640000.\n",
      "Step 1207, loss: 1.245454, accurancy: 0.613333.\n",
      "Step 1208, loss: 1.224375, accurancy: 0.593333.\n",
      "Step 1209, loss: 1.219544, accurancy: 0.630000.\n",
      "Step 1210, loss: 1.138789, accurancy: 0.663333.\n",
      "Step 1211, loss: 1.152973, accurancy: 0.633333.\n",
      "Step 1212, loss: 1.204099, accurancy: 0.613333.\n",
      "Step 1213, loss: 1.242801, accurancy: 0.593333.\n",
      "Step 1214, loss: 1.217386, accurancy: 0.620000.\n",
      "Step 1215, loss: 1.222308, accurancy: 0.606667.\n",
      "Step 1216, loss: 1.087559, accurancy: 0.640741.\n",
      "Step 1217, loss: 1.131968, accurancy: 0.613333.\n",
      "Step 1218, loss: 1.106697, accurancy: 0.660000.\n",
      "Step 1219, loss: 1.244573, accurancy: 0.633333.\n",
      "Step 1220, loss: 1.152307, accurancy: 0.620000.\n",
      "Step 1221, loss: 1.187748, accurancy: 0.620000.\n",
      "Step 1222, loss: 1.198984, accurancy: 0.653333.\n",
      "Step 1223, loss: 1.256922, accurancy: 0.590000.\n",
      "Step 1224, loss: 1.137413, accurancy: 0.666667.\n",
      "Step 1225, loss: 1.245651, accurancy: 0.613333.\n",
      "Step 1226, loss: 1.245742, accurancy: 0.620000.\n",
      "Step 1227, loss: 1.128524, accurancy: 0.640000.\n",
      "Step 1228, loss: 1.276908, accurancy: 0.603333.\n",
      "Step 1229, loss: 1.200852, accurancy: 0.616667.\n",
      "Step 1230, loss: 1.104209, accurancy: 0.640000.\n",
      "Step 1231, loss: 1.055793, accurancy: 0.653333.\n",
      "Step 1232, loss: 1.123033, accurancy: 0.656667.\n",
      "Step 1233, loss: 1.149882, accurancy: 0.616667.\n",
      "Step 1234, loss: 1.270180, accurancy: 0.603333.\n",
      "Step 1235, loss: 1.198225, accurancy: 0.660000.\n",
      "Step 1236, loss: 1.210963, accurancy: 0.593333.\n",
      "Step 1237, loss: 1.215362, accurancy: 0.660000.\n",
      "Step 1238, loss: 1.059081, accurancy: 0.663333.\n",
      "Step 1239, loss: 1.152754, accurancy: 0.626667.\n",
      "Step 1240, loss: 1.171645, accurancy: 0.633333.\n",
      "Step 1241, loss: 1.224255, accurancy: 0.630000.\n",
      "Step 1242, loss: 1.126120, accurancy: 0.666667.\n",
      "Step 1243, loss: 1.248160, accurancy: 0.580000.\n",
      "Step 1244, loss: 1.201466, accurancy: 0.626667.\n",
      "Step 1245, loss: 1.202306, accurancy: 0.623333.\n",
      "Step 1246, loss: 1.215029, accurancy: 0.620000.\n",
      "Step 1247, loss: 1.111790, accurancy: 0.643333.\n",
      "Step 1248, loss: 1.060961, accurancy: 0.653333.\n",
      "Step 1249, loss: 1.208044, accurancy: 0.640000.\n",
      "Step 1250, loss: 1.225361, accurancy: 0.610000.\n",
      "Step 1251, loss: 1.170985, accurancy: 0.630000.\n",
      "Step 1252, loss: 1.134798, accurancy: 0.620000.\n",
      "Step 1253, loss: 1.251424, accurancy: 0.576667.\n",
      "Step 1254, loss: 1.268324, accurancy: 0.580000.\n",
      "Step 1255, loss: 1.216027, accurancy: 0.610000.\n",
      "Step 1256, loss: 1.262043, accurancy: 0.623333.\n",
      "Step 1257, loss: 1.271833, accurancy: 0.600000.\n",
      "Step 1258, loss: 1.217990, accurancy: 0.623333.\n",
      "Step 1259, loss: 1.220059, accurancy: 0.626667.\n",
      "Step 1260, loss: 1.247934, accurancy: 0.603333.\n",
      "Step 1261, loss: 1.256640, accurancy: 0.636667.\n",
      "Step 1262, loss: 1.172773, accurancy: 0.633333.\n",
      "Step 1263, loss: 1.268316, accurancy: 0.606667.\n",
      "Step 1264, loss: 1.276709, accurancy: 0.576667.\n",
      "Step 1265, loss: 1.208636, accurancy: 0.600000.\n",
      "Step 1266, loss: 1.103515, accurancy: 0.646667.\n",
      "Step 1267, loss: 1.295959, accurancy: 0.580000.\n",
      "Step 1268, loss: 1.227743, accurancy: 0.633333.\n",
      "Step 1269, loss: 1.208980, accurancy: 0.606667.\n",
      "Step 1270, loss: 1.291753, accurancy: 0.613333.\n",
      "Step 1271, loss: 1.125912, accurancy: 0.626667.\n",
      "Step 1272, loss: 1.346210, accurancy: 0.586667.\n",
      "Step 1273, loss: 1.201474, accurancy: 0.640000.\n",
      "Step 1274, loss: 1.154615, accurancy: 0.603333.\n",
      "Step 1275, loss: 1.200412, accurancy: 0.653333.\n",
      "Step 1276, loss: 1.254094, accurancy: 0.613333.\n",
      "Step 1277, loss: 1.145734, accurancy: 0.643333.\n",
      "Step 1278, loss: 1.171984, accurancy: 0.650000.\n",
      "Step 1279, loss: 1.339321, accurancy: 0.566667.\n",
      "Step 1280, loss: 1.177717, accurancy: 0.623333.\n",
      "Step 1281, loss: 1.215130, accurancy: 0.616667.\n",
      "Step 1282, loss: 1.145675, accurancy: 0.610000.\n",
      "Step 1283, loss: 1.244212, accurancy: 0.583333.\n",
      "Step 1284, loss: 1.016201, accurancy: 0.725926.\n",
      "Step 1285, loss: 1.120910, accurancy: 0.666667.\n",
      "Step 1286, loss: 1.215288, accurancy: 0.630000.\n",
      "Step 1287, loss: 1.173547, accurancy: 0.626667.\n",
      "Step 1288, loss: 1.096605, accurancy: 0.663333.\n",
      "Step 1289, loss: 1.235680, accurancy: 0.586667.\n",
      "Step 1290, loss: 1.202481, accurancy: 0.653333.\n",
      "Step 1291, loss: 1.267394, accurancy: 0.600000.\n",
      "Step 1292, loss: 1.094247, accurancy: 0.640000.\n",
      "Step 1293, loss: 1.347218, accurancy: 0.583333.\n",
      "Step 1294, loss: 1.142399, accurancy: 0.666667.\n",
      "Step 1295, loss: 1.252250, accurancy: 0.626667.\n",
      "Step 1296, loss: 1.123570, accurancy: 0.626667.\n",
      "Step 1297, loss: 1.115235, accurancy: 0.646667.\n",
      "Step 1298, loss: 1.075799, accurancy: 0.696667.\n",
      "Step 1299, loss: 1.090200, accurancy: 0.643333.\n",
      "Step 1300, loss: 1.221610, accurancy: 0.620000.\n",
      "Validation accurancy is 0.626667\n",
      "Step 1301, loss: 1.213452, accurancy: 0.626667.\n",
      "Step 1302, loss: 1.248457, accurancy: 0.580000.\n",
      "Step 1303, loss: 1.209134, accurancy: 0.616667.\n",
      "Step 1304, loss: 1.282609, accurancy: 0.563333.\n",
      "Step 1305, loss: 1.098379, accurancy: 0.680000.\n",
      "Step 1306, loss: 1.087976, accurancy: 0.660000.\n",
      "Step 1307, loss: 1.200441, accurancy: 0.596667.\n",
      "Step 1308, loss: 1.245531, accurancy: 0.616667.\n",
      "Step 1309, loss: 1.137868, accurancy: 0.646667.\n",
      "Step 1310, loss: 1.249391, accurancy: 0.616667.\n",
      "Step 1311, loss: 1.218274, accurancy: 0.620000.\n",
      "Step 1312, loss: 1.083757, accurancy: 0.683333.\n",
      "Step 1313, loss: 1.329137, accurancy: 0.573333.\n",
      "Step 1314, loss: 1.087971, accurancy: 0.680000.\n",
      "Step 1315, loss: 1.048950, accurancy: 0.666667.\n",
      "Step 1316, loss: 1.164281, accurancy: 0.630000.\n",
      "Step 1317, loss: 1.243704, accurancy: 0.633333.\n",
      "Step 1318, loss: 1.221715, accurancy: 0.590000.\n",
      "Step 1319, loss: 1.166970, accurancy: 0.650000.\n",
      "Step 1320, loss: 1.175552, accurancy: 0.566667.\n",
      "Step 1321, loss: 1.306197, accurancy: 0.590000.\n",
      "Step 1322, loss: 1.248365, accurancy: 0.596667.\n",
      "Step 1323, loss: 1.231302, accurancy: 0.583333.\n",
      "Step 1324, loss: 1.288491, accurancy: 0.603333.\n",
      "Step 1325, loss: 1.353735, accurancy: 0.583333.\n",
      "Step 1326, loss: 1.147330, accurancy: 0.640000.\n",
      "Step 1327, loss: 1.264756, accurancy: 0.610000.\n",
      "Step 1328, loss: 1.262096, accurancy: 0.600000.\n",
      "Step 1329, loss: 1.234938, accurancy: 0.643333.\n",
      "Step 1330, loss: 1.223444, accurancy: 0.573333.\n",
      "Step 1331, loss: 1.187562, accurancy: 0.596667.\n",
      "Step 1332, loss: 1.313148, accurancy: 0.576667.\n",
      "Step 1333, loss: 1.248387, accurancy: 0.560000.\n",
      "Step 1334, loss: 1.218894, accurancy: 0.610000.\n",
      "Step 1335, loss: 1.269493, accurancy: 0.590000.\n",
      "Step 1336, loss: 1.104088, accurancy: 0.640000.\n",
      "Step 1337, loss: 1.205776, accurancy: 0.593333.\n",
      "Step 1338, loss: 1.286464, accurancy: 0.573333.\n",
      "Step 1339, loss: 1.213076, accurancy: 0.630000.\n",
      "Step 1340, loss: 1.284616, accurancy: 0.610000.\n",
      "Step 1341, loss: 1.183734, accurancy: 0.603333.\n",
      "Step 1342, loss: 1.226810, accurancy: 0.593333.\n",
      "Step 1343, loss: 1.197873, accurancy: 0.640000.\n",
      "Step 1344, loss: 1.251917, accurancy: 0.606667.\n",
      "Step 1345, loss: 1.144681, accurancy: 0.673333.\n",
      "Step 1346, loss: 1.134442, accurancy: 0.660000.\n",
      "Step 1347, loss: 1.181635, accurancy: 0.626667.\n",
      "Step 1348, loss: 1.201443, accurancy: 0.620000.\n",
      "Step 1349, loss: 1.227107, accurancy: 0.646667.\n",
      "Step 1350, loss: 1.213788, accurancy: 0.610000.\n",
      "Step 1351, loss: 1.113908, accurancy: 0.656667.\n",
      "Step 1352, loss: 1.188639, accurancy: 0.593333.\n",
      "Step 1353, loss: 1.084843, accurancy: 0.653333.\n",
      "Step 1354, loss: 1.277919, accurancy: 0.616667.\n",
      "Step 1355, loss: 1.133825, accurancy: 0.636667.\n",
      "Step 1356, loss: 1.159496, accurancy: 0.626667.\n",
      "Step 1357, loss: 1.161107, accurancy: 0.653333.\n",
      "Step 1358, loss: 1.259981, accurancy: 0.586667.\n",
      "Step 1359, loss: 1.228242, accurancy: 0.623333.\n",
      "Step 1360, loss: 1.192476, accurancy: 0.620000.\n",
      "Step 1361, loss: 1.258335, accurancy: 0.646667.\n",
      "Step 1362, loss: 1.136584, accurancy: 0.620000.\n",
      "Step 1363, loss: 1.315400, accurancy: 0.583333.\n",
      "Step 1364, loss: 1.155678, accurancy: 0.616667.\n",
      "Step 1365, loss: 1.163752, accurancy: 0.626667.\n",
      "Step 1366, loss: 1.034222, accurancy: 0.683333.\n",
      "Step 1367, loss: 1.054698, accurancy: 0.653333.\n",
      "Step 1368, loss: 1.238054, accurancy: 0.586667.\n",
      "Step 1369, loss: 1.284943, accurancy: 0.606667.\n",
      "Step 1370, loss: 1.211975, accurancy: 0.626667.\n",
      "Step 1371, loss: 1.174308, accurancy: 0.660000.\n",
      "Step 1372, loss: 1.134215, accurancy: 0.656667.\n",
      "Step 1373, loss: 1.093411, accurancy: 0.686667.\n",
      "Step 1374, loss: 1.108086, accurancy: 0.620000.\n",
      "Step 1375, loss: 1.229659, accurancy: 0.590000.\n",
      "Step 1376, loss: 1.227513, accurancy: 0.630000.\n",
      "Step 1377, loss: 1.166921, accurancy: 0.630000.\n",
      "Step 1378, loss: 1.272457, accurancy: 0.590000.\n",
      "Step 1379, loss: 1.192358, accurancy: 0.660000.\n",
      "Step 1380, loss: 1.136792, accurancy: 0.643333.\n",
      "Step 1381, loss: 1.305845, accurancy: 0.576667.\n",
      "Step 1382, loss: 1.181444, accurancy: 0.636667.\n",
      "Step 1383, loss: 1.034861, accurancy: 0.686667.\n",
      "Step 1384, loss: 1.164589, accurancy: 0.653333.\n",
      "Step 1385, loss: 1.183992, accurancy: 0.626667.\n",
      "Step 1386, loss: 1.198317, accurancy: 0.613333.\n",
      "Step 1387, loss: 1.092367, accurancy: 0.656667.\n",
      "Step 1388, loss: 1.162371, accurancy: 0.620000.\n",
      "Step 1389, loss: 1.270610, accurancy: 0.580000.\n",
      "Step 1390, loss: 1.216075, accurancy: 0.616667.\n",
      "Step 1391, loss: 1.324002, accurancy: 0.596667.\n",
      "Step 1392, loss: 1.258173, accurancy: 0.583333.\n",
      "Step 1393, loss: 1.267575, accurancy: 0.606667.\n",
      "Step 1394, loss: 1.062553, accurancy: 0.683333.\n",
      "Step 1395, loss: 1.295442, accurancy: 0.556667.\n",
      "Step 1396, loss: 1.198085, accurancy: 0.656667.\n",
      "Step 1397, loss: 1.233471, accurancy: 0.613333.\n",
      "Step 1398, loss: 1.222041, accurancy: 0.606667.\n",
      "Step 1399, loss: 1.249811, accurancy: 0.590000.\n",
      "Step 1400, loss: 1.184178, accurancy: 0.616667.\n",
      "Validation accurancy is 0.603333\n",
      "Step 1401, loss: 1.123244, accurancy: 0.640000.\n",
      "Step 1402, loss: 1.265657, accurancy: 0.576667.\n",
      "Step 1403, loss: 1.175040, accurancy: 0.656667.\n",
      "Step 1404, loss: 1.235969, accurancy: 0.583333.\n",
      "Step 1405, loss: 1.207007, accurancy: 0.620000.\n",
      "Step 1406, loss: 1.218189, accurancy: 0.580000.\n",
      "Step 1407, loss: 1.308232, accurancy: 0.610000.\n",
      "Step 1408, loss: 1.161653, accurancy: 0.656667.\n",
      "Step 1409, loss: 1.102666, accurancy: 0.636667.\n",
      "Step 1410, loss: 1.235799, accurancy: 0.613333.\n",
      "Step 1411, loss: 1.212271, accurancy: 0.636667.\n",
      "Step 1412, loss: 1.208185, accurancy: 0.613333.\n",
      "Step 1413, loss: 1.116161, accurancy: 0.683333.\n",
      "Step 1414, loss: 1.215586, accurancy: 0.583333.\n",
      "Step 1415, loss: 1.139492, accurancy: 0.653333.\n",
      "Step 1416, loss: 1.182101, accurancy: 0.610000.\n",
      "Step 1417, loss: 1.102391, accurancy: 0.633333.\n",
      "Step 1418, loss: 1.194998, accurancy: 0.620000.\n",
      "Step 1419, loss: 1.032052, accurancy: 0.678788.\n",
      "Step 1420, loss: 1.109459, accurancy: 0.660000.\n",
      "Step 1421, loss: 1.176556, accurancy: 0.640000.\n",
      "Step 1422, loss: 1.250221, accurancy: 0.596667.\n",
      "Step 1423, loss: 1.141335, accurancy: 0.666667.\n",
      "Step 1424, loss: 1.248994, accurancy: 0.613333.\n",
      "Step 1425, loss: 1.195835, accurancy: 0.626667.\n",
      "Step 1426, loss: 1.233877, accurancy: 0.600000.\n",
      "Step 1427, loss: 1.115064, accurancy: 0.680000.\n",
      "Step 1428, loss: 1.266676, accurancy: 0.630000.\n",
      "Step 1429, loss: 1.181270, accurancy: 0.636667.\n",
      "Step 1430, loss: 1.304272, accurancy: 0.573333.\n",
      "Step 1431, loss: 1.150668, accurancy: 0.616667.\n",
      "Step 1432, loss: 1.174595, accurancy: 0.653333.\n",
      "Step 1433, loss: 1.022596, accurancy: 0.686667.\n",
      "Step 1434, loss: 1.134075, accurancy: 0.650000.\n",
      "Step 1435, loss: 1.186565, accurancy: 0.603333.\n",
      "Step 1436, loss: 1.216827, accurancy: 0.650000.\n",
      "Step 1437, loss: 1.274029, accurancy: 0.600000.\n",
      "Step 1438, loss: 1.136347, accurancy: 0.670000.\n",
      "Step 1439, loss: 1.213138, accurancy: 0.636667.\n",
      "Step 1440, loss: 1.145187, accurancy: 0.636667.\n",
      "Step 1441, loss: 1.077856, accurancy: 0.633333.\n",
      "Step 1442, loss: 1.160246, accurancy: 0.603333.\n",
      "Step 1443, loss: 1.220691, accurancy: 0.620000.\n",
      "Step 1444, loss: 1.130680, accurancy: 0.666667.\n",
      "Step 1445, loss: 1.282778, accurancy: 0.620000.\n",
      "Step 1446, loss: 1.224710, accurancy: 0.603333.\n",
      "Step 1447, loss: 1.148162, accurancy: 0.643333.\n",
      "Step 1448, loss: 1.206761, accurancy: 0.616667.\n",
      "Step 1449, loss: 1.198513, accurancy: 0.640000.\n",
      "Step 1450, loss: 1.034972, accurancy: 0.686667.\n",
      "Step 1451, loss: 1.107310, accurancy: 0.660000.\n",
      "Step 1452, loss: 1.261388, accurancy: 0.640000.\n",
      "Step 1453, loss: 1.097585, accurancy: 0.646667.\n",
      "Step 1454, loss: 1.183688, accurancy: 0.633333.\n",
      "Step 1455, loss: 1.068483, accurancy: 0.680000.\n",
      "Step 1456, loss: 1.344005, accurancy: 0.553333.\n",
      "Step 1457, loss: 1.231707, accurancy: 0.590000.\n",
      "Step 1458, loss: 1.221392, accurancy: 0.620000.\n",
      "Step 1459, loss: 1.270924, accurancy: 0.636667.\n",
      "Step 1460, loss: 1.330044, accurancy: 0.603333.\n",
      "Step 1461, loss: 1.182695, accurancy: 0.613333.\n",
      "Step 1462, loss: 1.203910, accurancy: 0.653333.\n",
      "Step 1463, loss: 1.251810, accurancy: 0.580000.\n",
      "Step 1464, loss: 1.229649, accurancy: 0.670000.\n",
      "Step 1465, loss: 1.192456, accurancy: 0.603333.\n",
      "Step 1466, loss: 1.205358, accurancy: 0.623333.\n",
      "Step 1467, loss: 1.257754, accurancy: 0.623333.\n",
      "Step 1468, loss: 1.232592, accurancy: 0.563333.\n",
      "Step 1469, loss: 1.129382, accurancy: 0.610000.\n",
      "Step 1470, loss: 1.235836, accurancy: 0.603333.\n",
      "Step 1471, loss: 1.155501, accurancy: 0.673333.\n",
      "Step 1472, loss: 1.201634, accurancy: 0.593333.\n",
      "Step 1473, loss: 1.244971, accurancy: 0.593333.\n",
      "Step 1474, loss: 1.197099, accurancy: 0.620000.\n",
      "Step 1475, loss: 1.254523, accurancy: 0.613333.\n",
      "Step 1476, loss: 1.183003, accurancy: 0.640000.\n",
      "Step 1477, loss: 1.289001, accurancy: 0.576667.\n",
      "Step 1478, loss: 1.133234, accurancy: 0.650000.\n",
      "Step 1479, loss: 1.203709, accurancy: 0.613333.\n",
      "Step 1480, loss: 1.090986, accurancy: 0.663333.\n",
      "Step 1481, loss: 1.200701, accurancy: 0.616667.\n",
      "Step 1482, loss: 1.197030, accurancy: 0.633333.\n",
      "Step 1483, loss: 1.215605, accurancy: 0.610000.\n",
      "Step 1484, loss: 1.140719, accurancy: 0.636667.\n",
      "Step 1485, loss: 1.230794, accurancy: 0.596667.\n",
      "Step 1486, loss: 1.110535, accurancy: 0.670000.\n",
      "Step 1487, loss: 0.801479, accurancy: 0.800000.\n",
      "Step 1488, loss: 1.059817, accurancy: 0.676667.\n",
      "Step 1489, loss: 1.336317, accurancy: 0.580000.\n",
      "Step 1490, loss: 1.142820, accurancy: 0.643333.\n",
      "Step 1491, loss: 1.206943, accurancy: 0.626667.\n",
      "Step 1492, loss: 1.123694, accurancy: 0.643333.\n",
      "Step 1493, loss: 1.289844, accurancy: 0.606667.\n",
      "Step 1494, loss: 1.134865, accurancy: 0.660000.\n",
      "Step 1495, loss: 1.217406, accurancy: 0.606667.\n",
      "Step 1496, loss: 1.334655, accurancy: 0.583333.\n",
      "Step 1497, loss: 1.088874, accurancy: 0.643333.\n",
      "Step 1498, loss: 1.337441, accurancy: 0.563333.\n",
      "Step 1499, loss: 1.132486, accurancy: 0.620000.\n",
      "Step 1500, loss: 1.223262, accurancy: 0.620000.\n",
      "Validation accurancy is 0.586667\n",
      "Step 1501, loss: 1.041298, accurancy: 0.663333.\n",
      "Step 1502, loss: 1.159074, accurancy: 0.633333.\n",
      "Step 1503, loss: 1.184347, accurancy: 0.626667.\n",
      "Step 1504, loss: 1.281873, accurancy: 0.623333.\n",
      "Step 1505, loss: 1.203241, accurancy: 0.670000.\n",
      "Step 1506, loss: 1.204408, accurancy: 0.620000.\n",
      "Step 1507, loss: 1.178478, accurancy: 0.650000.\n",
      "Step 1508, loss: 1.110725, accurancy: 0.643333.\n",
      "Step 1509, loss: 1.117247, accurancy: 0.636667.\n",
      "Step 1510, loss: 1.176234, accurancy: 0.613333.\n",
      "Step 1511, loss: 1.240112, accurancy: 0.630000.\n",
      "Step 1512, loss: 1.181890, accurancy: 0.643333.\n",
      "Step 1513, loss: 1.260987, accurancy: 0.586667.\n",
      "Step 1514, loss: 1.188983, accurancy: 0.653333.\n",
      "Step 1515, loss: 1.131360, accurancy: 0.643333.\n",
      "Step 1516, loss: 1.304106, accurancy: 0.616667.\n",
      "Step 1517, loss: 1.123062, accurancy: 0.660000.\n",
      "Step 1518, loss: 1.033587, accurancy: 0.663333.\n",
      "Step 1519, loss: 1.188109, accurancy: 0.650000.\n",
      "Step 1520, loss: 1.217193, accurancy: 0.593333.\n",
      "Step 1521, loss: 1.110170, accurancy: 0.656667.\n",
      "Step 1522, loss: 1.126860, accurancy: 0.663333.\n",
      "Step 1523, loss: 1.195217, accurancy: 0.606667.\n",
      "Step 1524, loss: 1.351840, accurancy: 0.556667.\n",
      "Step 1525, loss: 1.230994, accurancy: 0.616667.\n",
      "Step 1526, loss: 1.328603, accurancy: 0.550000.\n",
      "Step 1527, loss: 1.213942, accurancy: 0.600000.\n",
      "Step 1528, loss: 1.308834, accurancy: 0.606667.\n",
      "Step 1529, loss: 1.124165, accurancy: 0.676667.\n",
      "Step 1530, loss: 1.231702, accurancy: 0.603333.\n",
      "Step 1531, loss: 1.288767, accurancy: 0.623333.\n",
      "Step 1532, loss: 1.184372, accurancy: 0.646667.\n",
      "Step 1533, loss: 1.261398, accurancy: 0.583333.\n",
      "Step 1534, loss: 1.206312, accurancy: 0.610000.\n",
      "Step 1535, loss: 1.187795, accurancy: 0.633333.\n",
      "Step 1536, loss: 1.157117, accurancy: 0.613333.\n",
      "Step 1537, loss: 1.180572, accurancy: 0.623333.\n",
      "Step 1538, loss: 1.160890, accurancy: 0.626667.\n",
      "Step 1539, loss: 1.268932, accurancy: 0.596667.\n",
      "Step 1540, loss: 1.238015, accurancy: 0.606667.\n",
      "Step 1541, loss: 1.175266, accurancy: 0.603333.\n",
      "Step 1542, loss: 1.311679, accurancy: 0.580000.\n",
      "Step 1543, loss: 1.218258, accurancy: 0.630000.\n",
      "Step 1544, loss: 1.126525, accurancy: 0.626667.\n",
      "Step 1545, loss: 1.292225, accurancy: 0.640000.\n",
      "Step 1546, loss: 1.154431, accurancy: 0.643333.\n",
      "Step 1547, loss: 1.245067, accurancy: 0.623333.\n",
      "Step 1548, loss: 1.136760, accurancy: 0.636667.\n",
      "Step 1549, loss: 1.130733, accurancy: 0.616667.\n",
      "Step 1550, loss: 1.202639, accurancy: 0.606667.\n",
      "Step 1551, loss: 1.166640, accurancy: 0.616667.\n",
      "Step 1552, loss: 1.181545, accurancy: 0.633333.\n",
      "Step 1553, loss: 1.181249, accurancy: 0.623333.\n",
      "Step 1554, loss: 1.063431, accurancy: 0.676923.\n",
      "Step 1555, loss: 1.121000, accurancy: 0.646667.\n",
      "Step 1556, loss: 1.103019, accurancy: 0.660000.\n",
      "Step 1557, loss: 1.200430, accurancy: 0.633333.\n",
      "Step 1558, loss: 1.135641, accurancy: 0.656667.\n",
      "Step 1559, loss: 1.244389, accurancy: 0.606667.\n",
      "Step 1560, loss: 1.189872, accurancy: 0.653333.\n",
      "Step 1561, loss: 1.217722, accurancy: 0.600000.\n",
      "Step 1562, loss: 1.134768, accurancy: 0.630000.\n",
      "Step 1563, loss: 1.265224, accurancy: 0.633333.\n",
      "Step 1564, loss: 1.208570, accurancy: 0.643333.\n",
      "Step 1565, loss: 1.214956, accurancy: 0.590000.\n",
      "Step 1566, loss: 1.137486, accurancy: 0.623333.\n",
      "Step 1567, loss: 1.272458, accurancy: 0.596667.\n",
      "Step 1568, loss: 1.042550, accurancy: 0.700000.\n",
      "Step 1569, loss: 1.112472, accurancy: 0.626667.\n",
      "Step 1570, loss: 1.195360, accurancy: 0.603333.\n",
      "Step 1571, loss: 1.203432, accurancy: 0.593333.\n",
      "Step 1572, loss: 1.241596, accurancy: 0.593333.\n",
      "Step 1573, loss: 1.162654, accurancy: 0.633333.\n",
      "Step 1574, loss: 1.205757, accurancy: 0.616667.\n",
      "Step 1575, loss: 1.139565, accurancy: 0.676667.\n",
      "Step 1576, loss: 1.035882, accurancy: 0.666667.\n",
      "Step 1577, loss: 1.181272, accurancy: 0.600000.\n",
      "Step 1578, loss: 1.225861, accurancy: 0.620000.\n",
      "Step 1579, loss: 1.155494, accurancy: 0.673333.\n",
      "Step 1580, loss: 1.217299, accurancy: 0.593333.\n",
      "Step 1581, loss: 1.232235, accurancy: 0.596667.\n",
      "Step 1582, loss: 1.149128, accurancy: 0.650000.\n",
      "Step 1583, loss: 1.206571, accurancy: 0.603333.\n",
      "Step 1584, loss: 1.221169, accurancy: 0.620000.\n",
      "Step 1585, loss: 1.021566, accurancy: 0.686667.\n",
      "Step 1586, loss: 1.110455, accurancy: 0.683333.\n",
      "Step 1587, loss: 1.204723, accurancy: 0.650000.\n",
      "Step 1588, loss: 1.171481, accurancy: 0.626667.\n",
      "Step 1589, loss: 1.220535, accurancy: 0.643333.\n",
      "Step 1590, loss: 1.108289, accurancy: 0.623333.\n",
      "Step 1591, loss: 1.269335, accurancy: 0.586667.\n",
      "Step 1592, loss: 1.192608, accurancy: 0.596667.\n",
      "Step 1593, loss: 1.240690, accurancy: 0.596667.\n",
      "Step 1594, loss: 1.328256, accurancy: 0.613333.\n",
      "Step 1595, loss: 1.286618, accurancy: 0.616667.\n",
      "Step 1596, loss: 1.190968, accurancy: 0.630000.\n",
      "Step 1597, loss: 1.178741, accurancy: 0.630000.\n",
      "Step 1598, loss: 1.264094, accurancy: 0.600000.\n",
      "Step 1599, loss: 1.230276, accurancy: 0.650000.\n",
      "Step 1600, loss: 1.212982, accurancy: 0.573333.\n",
      "Validation accurancy is 0.590000\n",
      "Step 1601, loss: 1.256687, accurancy: 0.600000.\n",
      "Step 1602, loss: 1.267278, accurancy: 0.606667.\n",
      "Step 1603, loss: 1.189487, accurancy: 0.603333.\n",
      "Step 1604, loss: 1.152226, accurancy: 0.656667.\n",
      "Step 1605, loss: 1.229444, accurancy: 0.606667.\n",
      "Step 1606, loss: 1.199361, accurancy: 0.613333.\n",
      "Step 1607, loss: 1.210274, accurancy: 0.603333.\n",
      "Step 1608, loss: 1.291782, accurancy: 0.593333.\n",
      "Step 1609, loss: 1.201657, accurancy: 0.610000.\n",
      "Step 1610, loss: 1.252476, accurancy: 0.603333.\n",
      "Step 1611, loss: 1.161454, accurancy: 0.636667.\n",
      "Step 1612, loss: 1.218583, accurancy: 0.633333.\n",
      "Step 1613, loss: 1.172672, accurancy: 0.663333.\n",
      "Step 1614, loss: 1.285163, accurancy: 0.590000.\n",
      "Step 1615, loss: 1.085487, accurancy: 0.686667.\n",
      "Step 1616, loss: 1.123626, accurancy: 0.636667.\n",
      "Step 1617, loss: 1.190146, accurancy: 0.606667.\n",
      "Step 1618, loss: 1.230181, accurancy: 0.590000.\n",
      "Step 1619, loss: 1.166997, accurancy: 0.643333.\n",
      "Step 1620, loss: 1.152350, accurancy: 0.640000.\n",
      "Step 1621, loss: 1.116718, accurancy: 0.643333.\n",
      "Step 1622, loss: 0.850422, accurancy: 0.750000.\n",
      "Step 1623, loss: 1.046424, accurancy: 0.650000.\n",
      "Step 1624, loss: 1.273843, accurancy: 0.626667.\n",
      "Step 1625, loss: 1.106885, accurancy: 0.660000.\n",
      "Step 1626, loss: 1.128510, accurancy: 0.650000.\n",
      "Step 1627, loss: 1.184352, accurancy: 0.650000.\n",
      "Step 1628, loss: 1.271633, accurancy: 0.613333.\n",
      "Step 1629, loss: 1.171874, accurancy: 0.650000.\n",
      "Step 1630, loss: 1.124822, accurancy: 0.633333.\n",
      "Step 1631, loss: 1.295021, accurancy: 0.606667.\n",
      "Step 1632, loss: 1.064558, accurancy: 0.696667.\n",
      "Step 1633, loss: 1.332516, accurancy: 0.586667.\n",
      "Step 1634, loss: 1.122184, accurancy: 0.600000.\n",
      "Step 1635, loss: 1.212905, accurancy: 0.616667.\n",
      "Step 1636, loss: 0.991373, accurancy: 0.696667.\n",
      "Step 1637, loss: 1.080514, accurancy: 0.643333.\n",
      "Step 1638, loss: 1.275003, accurancy: 0.586667.\n",
      "Step 1639, loss: 1.260793, accurancy: 0.613333.\n",
      "Step 1640, loss: 1.287170, accurancy: 0.590000.\n",
      "Step 1641, loss: 1.151459, accurancy: 0.660000.\n",
      "Step 1642, loss: 1.126341, accurancy: 0.660000.\n",
      "Step 1643, loss: 1.144518, accurancy: 0.666667.\n",
      "Step 1644, loss: 1.113591, accurancy: 0.646667.\n",
      "Step 1645, loss: 1.196434, accurancy: 0.616667.\n",
      "Step 1646, loss: 1.196392, accurancy: 0.620000.\n",
      "Step 1647, loss: 1.160422, accurancy: 0.633333.\n",
      "Step 1648, loss: 1.296409, accurancy: 0.580000.\n",
      "Step 1649, loss: 1.205366, accurancy: 0.623333.\n",
      "Step 1650, loss: 1.154800, accurancy: 0.653333.\n",
      "Step 1651, loss: 1.277571, accurancy: 0.583333.\n",
      "Step 1652, loss: 1.106343, accurancy: 0.633333.\n",
      "Step 1653, loss: 1.036468, accurancy: 0.676667.\n",
      "Step 1654, loss: 1.173005, accurancy: 0.640000.\n",
      "Step 1655, loss: 1.234154, accurancy: 0.620000.\n",
      "Step 1656, loss: 1.114976, accurancy: 0.656667.\n",
      "Step 1657, loss: 1.193687, accurancy: 0.613333.\n",
      "Step 1658, loss: 1.187483, accurancy: 0.580000.\n",
      "Step 1659, loss: 1.278475, accurancy: 0.586667.\n",
      "Step 1660, loss: 1.339601, accurancy: 0.560000.\n",
      "Step 1661, loss: 1.236491, accurancy: 0.630000.\n",
      "Step 1662, loss: 1.236706, accurancy: 0.590000.\n",
      "Step 1663, loss: 1.370760, accurancy: 0.590000.\n",
      "Step 1664, loss: 1.115971, accurancy: 0.640000.\n",
      "Step 1665, loss: 1.283923, accurancy: 0.600000.\n",
      "Step 1666, loss: 1.202715, accurancy: 0.636667.\n",
      "Step 1667, loss: 1.211592, accurancy: 0.646667.\n",
      "Step 1668, loss: 1.222347, accurancy: 0.593333.\n",
      "Step 1669, loss: 1.255500, accurancy: 0.590000.\n",
      "Step 1670, loss: 1.250354, accurancy: 0.600000.\n",
      "Step 1671, loss: 1.174571, accurancy: 0.610000.\n",
      "Step 1672, loss: 1.222966, accurancy: 0.613333.\n",
      "Step 1673, loss: 1.137087, accurancy: 0.646667.\n",
      "Step 1674, loss: 1.209185, accurancy: 0.600000.\n",
      "Step 1675, loss: 1.280290, accurancy: 0.583333.\n",
      "Step 1676, loss: 1.236933, accurancy: 0.630000.\n",
      "Step 1677, loss: 1.259986, accurancy: 0.596667.\n",
      "Step 1678, loss: 1.318435, accurancy: 0.566667.\n",
      "Step 1679, loss: 1.171187, accurancy: 0.610000.\n",
      "Step 1680, loss: 1.246723, accurancy: 0.603333.\n",
      "Step 1681, loss: 1.101412, accurancy: 0.663333.\n",
      "Step 1682, loss: 1.153182, accurancy: 0.646667.\n",
      "Step 1683, loss: 1.089578, accurancy: 0.646667.\n",
      "Step 1684, loss: 1.183637, accurancy: 0.586667.\n",
      "Step 1685, loss: 1.156974, accurancy: 0.626667.\n",
      "Step 1686, loss: 1.251920, accurancy: 0.586667.\n",
      "Step 1687, loss: 1.152109, accurancy: 0.656667.\n",
      "Step 1688, loss: 1.154207, accurancy: 0.633333.\n",
      "Step 1689, loss: 1.122233, accurancy: 0.622222.\n",
      "Step 1690, loss: 1.126306, accurancy: 0.650000.\n",
      "Step 1691, loss: 1.157768, accurancy: 0.626667.\n",
      "Step 1692, loss: 1.176243, accurancy: 0.660000.\n",
      "Step 1693, loss: 1.184442, accurancy: 0.626667.\n",
      "Step 1694, loss: 1.162208, accurancy: 0.606667.\n",
      "Step 1695, loss: 1.195871, accurancy: 0.683333.\n",
      "Step 1696, loss: 1.235434, accurancy: 0.603333.\n",
      "Step 1697, loss: 1.185914, accurancy: 0.596667.\n",
      "Step 1698, loss: 1.212412, accurancy: 0.610000.\n",
      "Step 1699, loss: 1.207797, accurancy: 0.626667.\n",
      "Step 1700, loss: 1.153718, accurancy: 0.616667.\n",
      "Validation accurancy is 0.613333\n",
      "Step 1701, loss: 1.276128, accurancy: 0.610000.\n",
      "Step 1702, loss: 1.226961, accurancy: 0.596667.\n",
      "Step 1703, loss: 0.997549, accurancy: 0.720000.\n",
      "Step 1704, loss: 1.116452, accurancy: 0.633333.\n",
      "Step 1705, loss: 1.179448, accurancy: 0.640000.\n",
      "Step 1706, loss: 1.202581, accurancy: 0.633333.\n",
      "Step 1707, loss: 1.277626, accurancy: 0.623333.\n",
      "Step 1708, loss: 1.164353, accurancy: 0.643333.\n",
      "Step 1709, loss: 1.189349, accurancy: 0.623333.\n",
      "Step 1710, loss: 1.104540, accurancy: 0.680000.\n",
      "Step 1711, loss: 1.065685, accurancy: 0.636667.\n",
      "Step 1712, loss: 1.136010, accurancy: 0.620000.\n",
      "Step 1713, loss: 1.217640, accurancy: 0.590000.\n",
      "Step 1714, loss: 1.195634, accurancy: 0.653333.\n",
      "Step 1715, loss: 1.209337, accurancy: 0.620000.\n",
      "Step 1716, loss: 1.194074, accurancy: 0.596667.\n",
      "Step 1717, loss: 1.227771, accurancy: 0.650000.\n",
      "Step 1718, loss: 1.182502, accurancy: 0.640000.\n",
      "Step 1719, loss: 1.200372, accurancy: 0.630000.\n",
      "Step 1720, loss: 1.063172, accurancy: 0.663333.\n",
      "Step 1721, loss: 1.136741, accurancy: 0.670000.\n",
      "Step 1722, loss: 1.194217, accurancy: 0.633333.\n",
      "Step 1723, loss: 1.117241, accurancy: 0.646667.\n",
      "Step 1724, loss: 1.238178, accurancy: 0.643333.\n",
      "Step 1725, loss: 1.126474, accurancy: 0.643333.\n",
      "Step 1726, loss: 1.326326, accurancy: 0.570000.\n",
      "Step 1727, loss: 1.198744, accurancy: 0.603333.\n",
      "Step 1728, loss: 1.189207, accurancy: 0.613333.\n",
      "Step 1729, loss: 1.252352, accurancy: 0.633333.\n",
      "Step 1730, loss: 1.266708, accurancy: 0.586667.\n",
      "Step 1731, loss: 1.278629, accurancy: 0.623333.\n",
      "Step 1732, loss: 1.150784, accurancy: 0.656667.\n",
      "Step 1733, loss: 1.317514, accurancy: 0.570000.\n",
      "Step 1734, loss: 1.222486, accurancy: 0.640000.\n",
      "Step 1735, loss: 1.215335, accurancy: 0.603333.\n",
      "Step 1736, loss: 1.270125, accurancy: 0.560000.\n",
      "Step 1737, loss: 1.264069, accurancy: 0.586667.\n",
      "Step 1738, loss: 1.140986, accurancy: 0.630000.\n",
      "Step 1739, loss: 1.188279, accurancy: 0.616667.\n",
      "Step 1740, loss: 1.229738, accurancy: 0.583333.\n",
      "Step 1741, loss: 1.154839, accurancy: 0.626667.\n",
      "Step 1742, loss: 1.235640, accurancy: 0.583333.\n",
      "Step 1743, loss: 1.253411, accurancy: 0.616667.\n",
      "Step 1744, loss: 1.189922, accurancy: 0.606667.\n",
      "Step 1745, loss: 1.329805, accurancy: 0.546667.\n",
      "Step 1746, loss: 1.162215, accurancy: 0.646667.\n",
      "Step 1747, loss: 1.204965, accurancy: 0.630000.\n",
      "Step 1748, loss: 1.214042, accurancy: 0.626667.\n",
      "Step 1749, loss: 1.270493, accurancy: 0.583333.\n",
      "Step 1750, loss: 1.128125, accurancy: 0.640000.\n",
      "Step 1751, loss: 1.085890, accurancy: 0.650000.\n",
      "Step 1752, loss: 1.216080, accurancy: 0.610000.\n",
      "Step 1753, loss: 1.173323, accurancy: 0.643333.\n",
      "Step 1754, loss: 1.218346, accurancy: 0.610000.\n",
      "Step 1755, loss: 1.165734, accurancy: 0.626667.\n",
      "Step 1756, loss: 1.161205, accurancy: 0.633333.\n",
      "Step 1757, loss: 0.986655, accurancy: 0.688889.\n",
      "Step 1758, loss: 1.054519, accurancy: 0.666667.\n",
      "Step 1759, loss: 1.267784, accurancy: 0.600000.\n",
      "Step 1760, loss: 1.122890, accurancy: 0.650000.\n",
      "Step 1761, loss: 1.133017, accurancy: 0.680000.\n",
      "Step 1762, loss: 1.160615, accurancy: 0.656667.\n",
      "Step 1763, loss: 1.276275, accurancy: 0.616667.\n",
      "Step 1764, loss: 1.176084, accurancy: 0.620000.\n",
      "Step 1765, loss: 1.110777, accurancy: 0.660000.\n",
      "Step 1766, loss: 1.384479, accurancy: 0.593333.\n",
      "Step 1767, loss: 1.069408, accurancy: 0.646667.\n",
      "Step 1768, loss: 1.274794, accurancy: 0.580000.\n",
      "Step 1769, loss: 1.204964, accurancy: 0.600000.\n",
      "Step 1770, loss: 1.142544, accurancy: 0.643333.\n",
      "Step 1771, loss: 1.031191, accurancy: 0.686667.\n",
      "Step 1772, loss: 1.090029, accurancy: 0.650000.\n",
      "Step 1773, loss: 1.277892, accurancy: 0.570000.\n",
      "Step 1774, loss: 1.181336, accurancy: 0.660000.\n",
      "Step 1775, loss: 1.241372, accurancy: 0.603333.\n",
      "Step 1776, loss: 1.135076, accurancy: 0.646667.\n",
      "Step 1777, loss: 1.227371, accurancy: 0.633333.\n",
      "Step 1778, loss: 1.130355, accurancy: 0.670000.\n",
      "Step 1779, loss: 1.127465, accurancy: 0.640000.\n",
      "Step 1780, loss: 1.098021, accurancy: 0.650000.\n",
      "Step 1781, loss: 1.222885, accurancy: 0.630000.\n",
      "Step 1782, loss: 1.119802, accurancy: 0.666667.\n",
      "Step 1783, loss: 1.281957, accurancy: 0.600000.\n",
      "Step 1784, loss: 1.170002, accurancy: 0.620000.\n",
      "Step 1785, loss: 1.128629, accurancy: 0.646667.\n",
      "Step 1786, loss: 1.238017, accurancy: 0.613333.\n",
      "Step 1787, loss: 1.196385, accurancy: 0.613333.\n",
      "Step 1788, loss: 1.085852, accurancy: 0.673333.\n",
      "Step 1789, loss: 1.150001, accurancy: 0.636667.\n",
      "Step 1790, loss: 1.263066, accurancy: 0.613333.\n",
      "Step 1791, loss: 1.136082, accurancy: 0.620000.\n",
      "Step 1792, loss: 1.118072, accurancy: 0.626667.\n",
      "Step 1793, loss: 1.184699, accurancy: 0.606667.\n",
      "Step 1794, loss: 1.262603, accurancy: 0.593333.\n",
      "Step 1795, loss: 1.295357, accurancy: 0.593333.\n",
      "Step 1796, loss: 1.266501, accurancy: 0.623333.\n",
      "Step 1797, loss: 1.187844, accurancy: 0.610000.\n",
      "Step 1798, loss: 1.346750, accurancy: 0.576667.\n",
      "Step 1799, loss: 1.104214, accurancy: 0.620000.\n",
      "Step 1800, loss: 1.246577, accurancy: 0.606667.\n",
      "Validation accurancy is 0.610000\n",
      "Step 1801, loss: 1.219043, accurancy: 0.606667.\n",
      "Step 1802, loss: 1.229098, accurancy: 0.613333.\n",
      "Step 1803, loss: 1.204023, accurancy: 0.596667.\n",
      "Step 1804, loss: 1.252897, accurancy: 0.546667.\n",
      "Step 1805, loss: 1.241653, accurancy: 0.643333.\n",
      "Step 1806, loss: 1.155913, accurancy: 0.630000.\n",
      "Step 1807, loss: 1.197934, accurancy: 0.616667.\n",
      "Step 1808, loss: 1.191462, accurancy: 0.616667.\n",
      "Step 1809, loss: 1.168529, accurancy: 0.633333.\n",
      "Step 1810, loss: 1.262198, accurancy: 0.580000.\n",
      "Step 1811, loss: 1.214999, accurancy: 0.576667.\n",
      "Step 1812, loss: 1.166578, accurancy: 0.620000.\n",
      "Step 1813, loss: 1.296820, accurancy: 0.556667.\n",
      "Step 1814, loss: 1.132088, accurancy: 0.660000.\n",
      "Step 1815, loss: 1.265318, accurancy: 0.566667.\n",
      "Step 1816, loss: 1.125781, accurancy: 0.653333.\n",
      "Step 1817, loss: 1.216172, accurancy: 0.626667.\n",
      "Step 1818, loss: 1.118771, accurancy: 0.660000.\n",
      "Step 1819, loss: 1.184626, accurancy: 0.620000.\n",
      "Step 1820, loss: 1.160190, accurancy: 0.610000.\n",
      "Step 1821, loss: 1.231842, accurancy: 0.606667.\n",
      "Step 1822, loss: 1.188936, accurancy: 0.636667.\n",
      "Step 1823, loss: 1.187819, accurancy: 0.613333.\n",
      "Step 1824, loss: 1.111737, accurancy: 0.658824.\n",
      "Step 1825, loss: 1.145942, accurancy: 0.633333.\n",
      "Step 1826, loss: 1.131310, accurancy: 0.650000.\n",
      "Step 1827, loss: 1.224012, accurancy: 0.640000.\n",
      "Step 1828, loss: 1.136684, accurancy: 0.636667.\n",
      "Step 1829, loss: 1.200327, accurancy: 0.616667.\n",
      "Step 1830, loss: 1.238820, accurancy: 0.606667.\n",
      "Step 1831, loss: 1.195055, accurancy: 0.590000.\n",
      "Step 1832, loss: 1.143126, accurancy: 0.663333.\n",
      "Step 1833, loss: 1.214649, accurancy: 0.626667.\n",
      "Step 1834, loss: 1.222354, accurancy: 0.630000.\n",
      "Step 1835, loss: 1.163658, accurancy: 0.636667.\n",
      "Step 1836, loss: 1.249525, accurancy: 0.603333.\n",
      "Step 1837, loss: 1.148271, accurancy: 0.640000.\n",
      "Step 1838, loss: 1.124554, accurancy: 0.630000.\n",
      "Step 1839, loss: 1.074871, accurancy: 0.650000.\n",
      "Step 1840, loss: 1.151104, accurancy: 0.616667.\n",
      "Step 1841, loss: 1.170114, accurancy: 0.600000.\n",
      "Step 1842, loss: 1.290916, accurancy: 0.600000.\n",
      "Step 1843, loss: 1.192198, accurancy: 0.656667.\n",
      "Step 1844, loss: 1.183587, accurancy: 0.636667.\n",
      "Step 1845, loss: 1.167687, accurancy: 0.666667.\n",
      "Step 1846, loss: 1.052239, accurancy: 0.666667.\n",
      "Step 1847, loss: 1.123007, accurancy: 0.623333.\n",
      "Step 1848, loss: 1.110593, accurancy: 0.620000.\n",
      "Step 1849, loss: 1.150697, accurancy: 0.676667.\n",
      "Step 1850, loss: 1.108129, accurancy: 0.620000.\n",
      "Step 1851, loss: 1.227363, accurancy: 0.610000.\n",
      "Step 1852, loss: 1.166038, accurancy: 0.676667.\n",
      "Step 1853, loss: 1.165539, accurancy: 0.630000.\n",
      "Step 1854, loss: 1.214863, accurancy: 0.646667.\n",
      "Step 1855, loss: 1.077343, accurancy: 0.663333.\n",
      "Step 1856, loss: 1.147382, accurancy: 0.633333.\n",
      "Step 1857, loss: 1.159801, accurancy: 0.670000.\n",
      "Step 1858, loss: 1.167703, accurancy: 0.646667.\n",
      "Step 1859, loss: 1.172344, accurancy: 0.640000.\n",
      "Step 1860, loss: 1.118712, accurancy: 0.640000.\n",
      "Step 1861, loss: 1.204345, accurancy: 0.600000.\n",
      "Step 1862, loss: 1.245566, accurancy: 0.586667.\n",
      "Step 1863, loss: 1.204785, accurancy: 0.626667.\n",
      "Step 1864, loss: 1.275752, accurancy: 0.620000.\n",
      "Step 1865, loss: 1.256558, accurancy: 0.623333.\n",
      "Step 1866, loss: 1.212795, accurancy: 0.603333.\n",
      "Step 1867, loss: 1.105737, accurancy: 0.653333.\n",
      "Step 1868, loss: 1.273668, accurancy: 0.600000.\n",
      "Step 1869, loss: 1.234821, accurancy: 0.643333.\n",
      "Step 1870, loss: 1.242135, accurancy: 0.603333.\n",
      "Step 1871, loss: 1.210377, accurancy: 0.606667.\n",
      "Step 1872, loss: 1.289760, accurancy: 0.583333.\n",
      "Step 1873, loss: 1.155572, accurancy: 0.583333.\n",
      "Step 1874, loss: 1.122568, accurancy: 0.643333.\n",
      "Step 1875, loss: 1.247675, accurancy: 0.590000.\n",
      "Step 1876, loss: 1.181683, accurancy: 0.613333.\n",
      "Step 1877, loss: 1.151883, accurancy: 0.626667.\n",
      "Step 1878, loss: 1.198023, accurancy: 0.660000.\n",
      "Step 1879, loss: 1.129584, accurancy: 0.613333.\n",
      "Step 1880, loss: 1.270980, accurancy: 0.616667.\n",
      "Step 1881, loss: 1.212715, accurancy: 0.650000.\n",
      "Step 1882, loss: 1.120628, accurancy: 0.620000.\n",
      "Step 1883, loss: 1.250221, accurancy: 0.626667.\n",
      "Step 1884, loss: 1.244172, accurancy: 0.596667.\n",
      "Step 1885, loss: 1.152184, accurancy: 0.656667.\n",
      "Step 1886, loss: 1.050212, accurancy: 0.673333.\n",
      "Step 1887, loss: 1.251270, accurancy: 0.583333.\n",
      "Step 1888, loss: 1.131107, accurancy: 0.670000.\n",
      "Step 1889, loss: 1.219406, accurancy: 0.620000.\n",
      "Step 1890, loss: 1.111552, accurancy: 0.646667.\n",
      "Step 1891, loss: 1.179275, accurancy: 0.646667.\n",
      "Step 1892, loss: 1.042401, accurancy: 0.683333.\n",
      "Step 1893, loss: 1.055383, accurancy: 0.650000.\n",
      "Step 1894, loss: 1.255216, accurancy: 0.640000.\n",
      "Step 1895, loss: 1.140432, accurancy: 0.660000.\n",
      "Step 1896, loss: 1.103640, accurancy: 0.660000.\n",
      "Step 1897, loss: 1.251396, accurancy: 0.583333.\n",
      "Step 1898, loss: 1.174518, accurancy: 0.650000.\n",
      "Step 1899, loss: 1.260330, accurancy: 0.596667.\n",
      "Step 1900, loss: 1.103364, accurancy: 0.660000.\n",
      "Validation accurancy is 0.586667\n",
      "Step 1901, loss: 1.325097, accurancy: 0.593333.\n",
      "Step 1902, loss: 1.098543, accurancy: 0.670000.\n",
      "Step 1903, loss: 1.242279, accurancy: 0.583333.\n",
      "Step 1904, loss: 1.138203, accurancy: 0.636667.\n",
      "Step 1905, loss: 1.140651, accurancy: 0.640000.\n",
      "Step 1906, loss: 1.079885, accurancy: 0.670000.\n",
      "Step 1907, loss: 1.114028, accurancy: 0.643333.\n",
      "Step 1908, loss: 1.197377, accurancy: 0.610000.\n",
      "Step 1909, loss: 1.161235, accurancy: 0.673333.\n",
      "Step 1910, loss: 1.240931, accurancy: 0.583333.\n",
      "Step 1911, loss: 1.113080, accurancy: 0.653333.\n",
      "Step 1912, loss: 1.182386, accurancy: 0.620000.\n",
      "Step 1913, loss: 1.149775, accurancy: 0.640000.\n",
      "Step 1914, loss: 1.068786, accurancy: 0.646667.\n",
      "Step 1915, loss: 1.187262, accurancy: 0.593333.\n",
      "Step 1916, loss: 1.211210, accurancy: 0.630000.\n",
      "Step 1917, loss: 1.116402, accurancy: 0.690000.\n",
      "Step 1918, loss: 1.208693, accurancy: 0.606667.\n",
      "Step 1919, loss: 1.151394, accurancy: 0.626667.\n",
      "Step 1920, loss: 1.115010, accurancy: 0.646667.\n",
      "Step 1921, loss: 1.271183, accurancy: 0.606667.\n",
      "Step 1922, loss: 1.096923, accurancy: 0.670000.\n",
      "Step 1923, loss: 1.060218, accurancy: 0.666667.\n",
      "Step 1924, loss: 1.138801, accurancy: 0.630000.\n",
      "Step 1925, loss: 1.209929, accurancy: 0.640000.\n",
      "Step 1926, loss: 1.170794, accurancy: 0.616667.\n",
      "Step 1927, loss: 1.124534, accurancy: 0.643333.\n",
      "Step 1928, loss: 1.150800, accurancy: 0.616667.\n",
      "Step 1929, loss: 1.324855, accurancy: 0.566667.\n",
      "Step 1930, loss: 1.257615, accurancy: 0.596667.\n",
      "Step 1931, loss: 1.270661, accurancy: 0.593333.\n",
      "Step 1932, loss: 1.288643, accurancy: 0.613333.\n",
      "Step 1933, loss: 1.314291, accurancy: 0.596667.\n",
      "Step 1934, loss: 1.099747, accurancy: 0.666667.\n",
      "Step 1935, loss: 1.193675, accurancy: 0.630000.\n",
      "Step 1936, loss: 1.239279, accurancy: 0.603333.\n",
      "Step 1937, loss: 1.252147, accurancy: 0.616667.\n",
      "Step 1938, loss: 1.176930, accurancy: 0.600000.\n",
      "Step 1939, loss: 1.227329, accurancy: 0.593333.\n",
      "Step 1940, loss: 1.294778, accurancy: 0.583333.\n",
      "Step 1941, loss: 1.193879, accurancy: 0.610000.\n",
      "Step 1942, loss: 1.112123, accurancy: 0.656667.\n",
      "Step 1943, loss: 1.169537, accurancy: 0.606667.\n",
      "Step 1944, loss: 1.140307, accurancy: 0.653333.\n",
      "Step 1945, loss: 1.257064, accurancy: 0.603333.\n",
      "Step 1946, loss: 1.278247, accurancy: 0.620000.\n",
      "Step 1947, loss: 1.214093, accurancy: 0.623333.\n",
      "Step 1948, loss: 1.288979, accurancy: 0.540000.\n",
      "Step 1949, loss: 1.164774, accurancy: 0.653333.\n",
      "Step 1950, loss: 1.195190, accurancy: 0.606667.\n",
      "Step 1951, loss: 1.175602, accurancy: 0.630000.\n",
      "Step 1952, loss: 1.175900, accurancy: 0.636667.\n",
      "Step 1953, loss: 1.091774, accurancy: 0.673333.\n",
      "Step 1954, loss: 1.149199, accurancy: 0.633333.\n",
      "Step 1955, loss: 1.187635, accurancy: 0.620000.\n",
      "Step 1956, loss: 1.263545, accurancy: 0.566667.\n",
      "Step 1957, loss: 1.207370, accurancy: 0.646667.\n",
      "Step 1958, loss: 1.141473, accurancy: 0.626667.\n",
      "Step 1959, loss: 1.124650, accurancy: 0.652632.\n",
      "Step 1960, loss: 1.161751, accurancy: 0.663333.\n",
      "Step 1961, loss: 1.101986, accurancy: 0.646667.\n",
      "Step 1962, loss: 1.227868, accurancy: 0.626667.\n",
      "Step 1963, loss: 1.148605, accurancy: 0.613333.\n",
      "Step 1964, loss: 1.174511, accurancy: 0.623333.\n",
      "Step 1965, loss: 1.127759, accurancy: 0.650000.\n",
      "Step 1966, loss: 1.248381, accurancy: 0.623333.\n",
      "Step 1967, loss: 1.207619, accurancy: 0.643333.\n",
      "Step 1968, loss: 1.201968, accurancy: 0.616667.\n",
      "Step 1969, loss: 1.273014, accurancy: 0.603333.\n",
      "Step 1970, loss: 1.126038, accurancy: 0.636667.\n",
      "Step 1971, loss: 1.301650, accurancy: 0.583333.\n",
      "Step 1972, loss: 1.143931, accurancy: 0.613333.\n",
      "Step 1973, loss: 1.145026, accurancy: 0.663333.\n",
      "Step 1974, loss: 1.095040, accurancy: 0.636667.\n",
      "Step 1975, loss: 1.076522, accurancy: 0.680000.\n",
      "Step 1976, loss: 1.182445, accurancy: 0.636667.\n",
      "Step 1977, loss: 1.260373, accurancy: 0.603333.\n",
      "Step 1978, loss: 1.153496, accurancy: 0.656667.\n",
      "Step 1979, loss: 1.231927, accurancy: 0.606667.\n",
      "Step 1980, loss: 1.247446, accurancy: 0.620000.\n",
      "Step 1981, loss: 1.048392, accurancy: 0.686667.\n",
      "Step 1982, loss: 1.105311, accurancy: 0.643333.\n",
      "Step 1983, loss: 1.137042, accurancy: 0.626667.\n",
      "Step 1984, loss: 1.214987, accurancy: 0.636667.\n",
      "Step 1985, loss: 1.124033, accurancy: 0.660000.\n",
      "Step 1986, loss: 1.252997, accurancy: 0.580000.\n",
      "Step 1987, loss: 1.137131, accurancy: 0.653333.\n",
      "Step 1988, loss: 1.162183, accurancy: 0.620000.\n",
      "Step 1989, loss: 1.239390, accurancy: 0.626667.\n",
      "Step 1990, loss: 1.156567, accurancy: 0.660000.\n",
      "Step 1991, loss: 1.023950, accurancy: 0.680000.\n",
      "Step 1992, loss: 1.143137, accurancy: 0.660000.\n",
      "Step 1993, loss: 1.205417, accurancy: 0.596667.\n",
      "Step 1994, loss: 1.170164, accurancy: 0.646667.\n",
      "Step 1995, loss: 1.155129, accurancy: 0.636667.\n",
      "Step 1996, loss: 1.198200, accurancy: 0.580000.\n",
      "Step 1997, loss: 1.265343, accurancy: 0.593333.\n",
      "Step 1998, loss: 1.220926, accurancy: 0.623333.\n",
      "Step 1999, loss: 1.300693, accurancy: 0.613333.\n",
      "Step 2000, loss: 1.254165, accurancy: 0.613333.\n",
      "Validation accurancy is 0.570000\n",
      "Step 2001, loss: 1.206875, accurancy: 0.616667.\n",
      "Step 2002, loss: 1.136494, accurancy: 0.636667.\n",
      "Step 2003, loss: 1.305011, accurancy: 0.600000.\n",
      "Step 2004, loss: 1.197407, accurancy: 0.666667.\n",
      "Step 2005, loss: 1.184473, accurancy: 0.636667.\n",
      "Step 2006, loss: 1.251152, accurancy: 0.580000.\n",
      "Step 2007, loss: 1.266192, accurancy: 0.590000.\n",
      "Step 2008, loss: 1.186950, accurancy: 0.616667.\n",
      "Step 2009, loss: 1.099957, accurancy: 0.640000.\n",
      "Step 2010, loss: 1.232649, accurancy: 0.580000.\n",
      "Step 2011, loss: 1.164205, accurancy: 0.606667.\n",
      "Step 2012, loss: 1.202583, accurancy: 0.600000.\n",
      "Step 2013, loss: 1.284004, accurancy: 0.586667.\n",
      "Step 2014, loss: 1.157786, accurancy: 0.616667.\n",
      "Step 2015, loss: 1.288629, accurancy: 0.590000.\n",
      "Step 2016, loss: 1.162720, accurancy: 0.650000.\n",
      "Step 2017, loss: 1.120559, accurancy: 0.610000.\n",
      "Step 2018, loss: 1.192587, accurancy: 0.646667.\n",
      "Step 2019, loss: 1.174875, accurancy: 0.640000.\n",
      "Step 2020, loss: 1.135057, accurancy: 0.680000.\n",
      "Step 2021, loss: 1.098950, accurancy: 0.660000.\n",
      "Step 2022, loss: 1.217223, accurancy: 0.616667.\n",
      "Step 2023, loss: 1.144345, accurancy: 0.630000.\n",
      "Step 2024, loss: 1.206914, accurancy: 0.610000.\n",
      "Step 2025, loss: 1.147719, accurancy: 0.623333.\n",
      "Step 2026, loss: 1.249579, accurancy: 0.583333.\n",
      "Step 2027, loss: 0.990098, accurancy: 0.686667.\n",
      "Step 2028, loss: 1.127438, accurancy: 0.656667.\n",
      "Step 2029, loss: 1.147019, accurancy: 0.663333.\n",
      "Step 2030, loss: 1.204097, accurancy: 0.596667.\n",
      "Step 2031, loss: 1.141237, accurancy: 0.636667.\n",
      "Step 2032, loss: 1.281945, accurancy: 0.593333.\n",
      "Step 2033, loss: 1.157479, accurancy: 0.636667.\n",
      "Step 2034, loss: 1.270594, accurancy: 0.600000.\n",
      "Step 2035, loss: 1.089416, accurancy: 0.660000.\n",
      "Step 2036, loss: 1.232720, accurancy: 0.606667.\n",
      "Step 2037, loss: 1.109884, accurancy: 0.640000.\n",
      "Step 2038, loss: 1.264367, accurancy: 0.596667.\n",
      "Step 2039, loss: 1.126759, accurancy: 0.640000.\n",
      "Step 2040, loss: 1.132678, accurancy: 0.653333.\n",
      "Step 2041, loss: 1.094827, accurancy: 0.636667.\n",
      "Step 2042, loss: 1.101492, accurancy: 0.610000.\n",
      "Step 2043, loss: 1.247899, accurancy: 0.586667.\n",
      "Step 2044, loss: 1.140697, accurancy: 0.643333.\n",
      "Step 2045, loss: 1.198127, accurancy: 0.646667.\n",
      "Step 2046, loss: 1.191041, accurancy: 0.653333.\n",
      "Step 2047, loss: 1.189911, accurancy: 0.610000.\n",
      "Step 2048, loss: 1.136428, accurancy: 0.670000.\n",
      "Step 2049, loss: 1.070336, accurancy: 0.676667.\n",
      "Step 2050, loss: 1.135828, accurancy: 0.656667.\n",
      "Step 2051, loss: 1.189868, accurancy: 0.630000.\n",
      "Step 2052, loss: 1.144546, accurancy: 0.626667.\n",
      "Step 2053, loss: 1.217562, accurancy: 0.620000.\n",
      "Step 2054, loss: 1.214178, accurancy: 0.603333.\n",
      "Step 2055, loss: 1.088129, accurancy: 0.666667.\n",
      "Step 2056, loss: 1.236047, accurancy: 0.623333.\n",
      "Step 2057, loss: 1.201151, accurancy: 0.633333.\n",
      "Step 2058, loss: 1.087264, accurancy: 0.660000.\n",
      "Step 2059, loss: 1.139173, accurancy: 0.636667.\n",
      "Step 2060, loss: 1.250696, accurancy: 0.613333.\n",
      "Step 2061, loss: 1.075405, accurancy: 0.650000.\n",
      "Step 2062, loss: 1.139971, accurancy: 0.626667.\n",
      "Step 2063, loss: 1.117237, accurancy: 0.623333.\n",
      "Step 2064, loss: 1.272774, accurancy: 0.590000.\n",
      "Step 2065, loss: 1.226026, accurancy: 0.590000.\n",
      "Step 2066, loss: 1.230341, accurancy: 0.610000.\n",
      "Step 2067, loss: 1.230559, accurancy: 0.626667.\n",
      "Step 2068, loss: 1.298438, accurancy: 0.593333.\n",
      "Step 2069, loss: 1.151203, accurancy: 0.656667.\n",
      "Step 2070, loss: 1.208450, accurancy: 0.630000.\n",
      "Step 2071, loss: 1.246006, accurancy: 0.603333.\n",
      "Step 2072, loss: 1.223021, accurancy: 0.663333.\n",
      "Step 2073, loss: 1.212576, accurancy: 0.593333.\n",
      "Step 2074, loss: 1.166821, accurancy: 0.616667.\n",
      "Step 2075, loss: 1.253036, accurancy: 0.616667.\n",
      "Step 2076, loss: 1.188039, accurancy: 0.620000.\n",
      "Step 2077, loss: 1.136881, accurancy: 0.646667.\n",
      "Step 2078, loss: 1.220905, accurancy: 0.596667.\n",
      "Step 2079, loss: 1.118966, accurancy: 0.663333.\n",
      "Step 2080, loss: 1.212330, accurancy: 0.620000.\n",
      "Step 2081, loss: 1.306505, accurancy: 0.583333.\n",
      "Step 2082, loss: 1.184240, accurancy: 0.633333.\n",
      "Step 2083, loss: 1.268583, accurancy: 0.576667.\n",
      "Step 2084, loss: 1.186261, accurancy: 0.596667.\n",
      "Step 2085, loss: 1.215055, accurancy: 0.633333.\n",
      "Step 2086, loss: 1.091740, accurancy: 0.663333.\n",
      "Step 2087, loss: 1.238805, accurancy: 0.606667.\n",
      "Step 2088, loss: 1.067378, accurancy: 0.686667.\n",
      "Step 2089, loss: 1.104340, accurancy: 0.643333.\n",
      "Step 2090, loss: 1.185513, accurancy: 0.623333.\n",
      "Step 2091, loss: 1.233528, accurancy: 0.593333.\n",
      "Step 2092, loss: 1.179704, accurancy: 0.640000.\n",
      "Step 2093, loss: 1.229129, accurancy: 0.603333.\n",
      "Step 2094, loss: 1.123583, accurancy: 0.643333.\n",
      "Step 2095, loss: 0.701290, accurancy: 0.800000.\n",
      "Step 2096, loss: 0.986452, accurancy: 0.690000.\n",
      "Step 2097, loss: 1.280732, accurancy: 0.636667.\n",
      "Step 2098, loss: 1.145399, accurancy: 0.630000.\n",
      "Step 2099, loss: 1.115964, accurancy: 0.663333.\n",
      "Step 2100, loss: 1.145805, accurancy: 0.666667.\n",
      "Validation accurancy is 0.616667\n",
      "Step 2101, loss: 1.274223, accurancy: 0.600000.\n",
      "Step 2102, loss: 1.151732, accurancy: 0.646667.\n",
      "Step 2103, loss: 1.197527, accurancy: 0.663333.\n",
      "Step 2104, loss: 1.277968, accurancy: 0.623333.\n",
      "Step 2105, loss: 1.103206, accurancy: 0.636667.\n",
      "Step 2106, loss: 1.312618, accurancy: 0.590000.\n",
      "Step 2107, loss: 1.045030, accurancy: 0.643333.\n",
      "Step 2108, loss: 1.227503, accurancy: 0.630000.\n",
      "Step 2109, loss: 1.019253, accurancy: 0.686667.\n",
      "Step 2110, loss: 1.045304, accurancy: 0.696667.\n",
      "Step 2111, loss: 1.187978, accurancy: 0.620000.\n",
      "Step 2112, loss: 1.285965, accurancy: 0.600000.\n",
      "Step 2113, loss: 1.193857, accurancy: 0.636667.\n",
      "Step 2114, loss: 1.100784, accurancy: 0.623333.\n",
      "Step 2115, loss: 1.196746, accurancy: 0.643333.\n",
      "Step 2116, loss: 1.092772, accurancy: 0.673333.\n",
      "Step 2117, loss: 1.131166, accurancy: 0.643333.\n",
      "Step 2118, loss: 1.197429, accurancy: 0.610000.\n",
      "Step 2119, loss: 1.235998, accurancy: 0.603333.\n",
      "Step 2120, loss: 1.083595, accurancy: 0.636667.\n",
      "Step 2121, loss: 1.280483, accurancy: 0.593333.\n",
      "Step 2122, loss: 1.125542, accurancy: 0.616667.\n",
      "Step 2123, loss: 1.168091, accurancy: 0.626667.\n",
      "Step 2124, loss: 1.279689, accurancy: 0.603333.\n",
      "Step 2125, loss: 1.128848, accurancy: 0.640000.\n",
      "Step 2126, loss: 1.019695, accurancy: 0.676667.\n",
      "Step 2127, loss: 1.153169, accurancy: 0.643333.\n",
      "Step 2128, loss: 1.181726, accurancy: 0.616667.\n",
      "Step 2129, loss: 1.144471, accurancy: 0.630000.\n",
      "Step 2130, loss: 1.094728, accurancy: 0.636667.\n",
      "Step 2131, loss: 1.088056, accurancy: 0.630000.\n",
      "Step 2132, loss: 1.298742, accurancy: 0.606667.\n",
      "Step 2133, loss: 1.213745, accurancy: 0.586667.\n",
      "Step 2134, loss: 1.293521, accurancy: 0.603333.\n",
      "Step 2135, loss: 1.226192, accurancy: 0.593333.\n",
      "Step 2136, loss: 1.323271, accurancy: 0.570000.\n",
      "Step 2137, loss: 1.050229, accurancy: 0.666667.\n",
      "Step 2138, loss: 1.264256, accurancy: 0.586667.\n",
      "Step 2139, loss: 1.235290, accurancy: 0.653333.\n",
      "Step 2140, loss: 1.176545, accurancy: 0.620000.\n",
      "Step 2141, loss: 1.144407, accurancy: 0.640000.\n",
      "Step 2142, loss: 1.250485, accurancy: 0.583333.\n",
      "Step 2143, loss: 1.149502, accurancy: 0.656667.\n",
      "Step 2144, loss: 1.167342, accurancy: 0.623333.\n",
      "Step 2145, loss: 1.159124, accurancy: 0.606667.\n",
      "Step 2146, loss: 1.144861, accurancy: 0.630000.\n",
      "Step 2147, loss: 1.179367, accurancy: 0.633333.\n",
      "Step 2148, loss: 1.222000, accurancy: 0.636667.\n",
      "Step 2149, loss: 1.214978, accurancy: 0.603333.\n",
      "Step 2150, loss: 1.253339, accurancy: 0.626667.\n",
      "Step 2151, loss: 1.131868, accurancy: 0.666667.\n",
      "Step 2152, loss: 1.069626, accurancy: 0.666667.\n",
      "Step 2153, loss: 1.289390, accurancy: 0.573333.\n",
      "Step 2154, loss: 1.145338, accurancy: 0.613333.\n",
      "Step 2155, loss: 1.201302, accurancy: 0.626667.\n",
      "Step 2156, loss: 1.138252, accurancy: 0.636667.\n",
      "Step 2157, loss: 1.154517, accurancy: 0.633333.\n",
      "Step 2158, loss: 1.220970, accurancy: 0.653333.\n",
      "Step 2159, loss: 1.169835, accurancy: 0.653333.\n",
      "Step 2160, loss: 1.131065, accurancy: 0.613333.\n",
      "Step 2161, loss: 1.209235, accurancy: 0.610000.\n",
      "Step 2162, loss: 1.014985, accurancy: 0.683333.\n",
      "Step 2163, loss: 1.128397, accurancy: 0.666667.\n",
      "Step 2164, loss: 1.109374, accurancy: 0.656667.\n",
      "Step 2165, loss: 1.203284, accurancy: 0.643333.\n",
      "Step 2166, loss: 1.148428, accurancy: 0.650000.\n",
      "Step 2167, loss: 1.181456, accurancy: 0.620000.\n",
      "Step 2168, loss: 1.179410, accurancy: 0.630000.\n",
      "Step 2169, loss: 1.231340, accurancy: 0.600000.\n",
      "Step 2170, loss: 1.080371, accurancy: 0.653333.\n",
      "Step 2171, loss: 1.228817, accurancy: 0.596667.\n",
      "Step 2172, loss: 1.159106, accurancy: 0.660000.\n",
      "Step 2173, loss: 1.220955, accurancy: 0.633333.\n",
      "Step 2174, loss: 1.228572, accurancy: 0.600000.\n",
      "Step 2175, loss: 1.155550, accurancy: 0.633333.\n",
      "Step 2176, loss: 1.064367, accurancy: 0.653333.\n",
      "Step 2177, loss: 1.036382, accurancy: 0.683333.\n",
      "Step 2178, loss: 1.132667, accurancy: 0.656667.\n",
      "Step 2179, loss: 1.179410, accurancy: 0.650000.\n",
      "Step 2180, loss: 1.225166, accurancy: 0.603333.\n",
      "Step 2181, loss: 1.183382, accurancy: 0.666667.\n",
      "Step 2182, loss: 1.125373, accurancy: 0.630000.\n",
      "Step 2183, loss: 1.170558, accurancy: 0.643333.\n",
      "Step 2184, loss: 1.043663, accurancy: 0.683333.\n",
      "Step 2185, loss: 1.126934, accurancy: 0.610000.\n",
      "Step 2186, loss: 1.119182, accurancy: 0.656667.\n",
      "Step 2187, loss: 1.166077, accurancy: 0.656667.\n",
      "Step 2188, loss: 1.206459, accurancy: 0.620000.\n",
      "Step 2189, loss: 1.126399, accurancy: 0.636667.\n",
      "Step 2190, loss: 1.134798, accurancy: 0.673333.\n",
      "Step 2191, loss: 1.203883, accurancy: 0.613333.\n",
      "Step 2192, loss: 1.172831, accurancy: 0.616667.\n",
      "Step 2193, loss: 1.045918, accurancy: 0.700000.\n",
      "Step 2194, loss: 1.213858, accurancy: 0.603333.\n",
      "Step 2195, loss: 1.224007, accurancy: 0.626667.\n",
      "Step 2196, loss: 1.148606, accurancy: 0.610000.\n",
      "Step 2197, loss: 1.168666, accurancy: 0.653333.\n",
      "Step 2198, loss: 1.098326, accurancy: 0.646667.\n",
      "Step 2199, loss: 1.248499, accurancy: 0.596667.\n",
      "Step 2200, loss: 1.173781, accurancy: 0.643333.\n",
      "Validation accurancy is 0.633333\n",
      "Step 2201, loss: 1.174688, accurancy: 0.620000.\n",
      "Step 2202, loss: 1.296933, accurancy: 0.623333.\n",
      "Step 2203, loss: 1.300727, accurancy: 0.583333.\n",
      "Step 2204, loss: 1.159713, accurancy: 0.626667.\n",
      "Step 2205, loss: 1.151856, accurancy: 0.656667.\n",
      "Step 2206, loss: 1.259847, accurancy: 0.600000.\n",
      "Step 2207, loss: 1.212102, accurancy: 0.643333.\n",
      "Step 2208, loss: 1.137990, accurancy: 0.603333.\n",
      "Step 2209, loss: 1.199688, accurancy: 0.600000.\n",
      "Step 2210, loss: 1.268332, accurancy: 0.596667.\n",
      "Step 2211, loss: 1.205043, accurancy: 0.606667.\n",
      "Step 2212, loss: 1.063291, accurancy: 0.666667.\n",
      "Step 2213, loss: 1.189241, accurancy: 0.620000.\n",
      "Step 2214, loss: 1.152194, accurancy: 0.636667.\n",
      "Step 2215, loss: 1.182990, accurancy: 0.616667.\n",
      "Step 2216, loss: 1.270245, accurancy: 0.583333.\n",
      "Step 2217, loss: 1.177265, accurancy: 0.610000.\n",
      "Step 2218, loss: 1.298903, accurancy: 0.570000.\n",
      "Step 2219, loss: 1.152241, accurancy: 0.653333.\n",
      "Step 2220, loss: 1.175076, accurancy: 0.600000.\n",
      "Step 2221, loss: 1.158461, accurancy: 0.670000.\n",
      "Step 2222, loss: 1.231368, accurancy: 0.593333.\n",
      "Step 2223, loss: 1.043168, accurancy: 0.653333.\n",
      "Step 2224, loss: 1.138563, accurancy: 0.630000.\n",
      "Step 2225, loss: 1.210747, accurancy: 0.620000.\n",
      "Step 2226, loss: 1.222852, accurancy: 0.606667.\n",
      "Step 2227, loss: 1.163696, accurancy: 0.620000.\n",
      "Step 2228, loss: 1.158293, accurancy: 0.636667.\n",
      "Step 2229, loss: 1.138425, accurancy: 0.653333.\n",
      "Step 2230, loss: 0.802949, accurancy: 0.822222.\n",
      "Step 2231, loss: 1.027595, accurancy: 0.690000.\n",
      "Step 2232, loss: 1.308012, accurancy: 0.596667.\n",
      "Step 2233, loss: 1.094722, accurancy: 0.663333.\n",
      "Step 2234, loss: 1.174720, accurancy: 0.620000.\n",
      "Step 2235, loss: 1.089184, accurancy: 0.683333.\n",
      "Step 2236, loss: 1.279645, accurancy: 0.603333.\n",
      "Step 2237, loss: 1.096099, accurancy: 0.660000.\n",
      "Step 2238, loss: 1.126660, accurancy: 0.643333.\n",
      "Step 2239, loss: 1.290770, accurancy: 0.620000.\n",
      "Step 2240, loss: 1.101056, accurancy: 0.643333.\n",
      "Step 2241, loss: 1.285899, accurancy: 0.603333.\n",
      "Step 2242, loss: 1.104909, accurancy: 0.650000.\n",
      "Step 2243, loss: 1.175997, accurancy: 0.603333.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7de22a5ec62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m                     \u001b[0mground_truth_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_ground_truth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                     \u001b[0mlearning_rate_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlearning_decay\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mlearning_decay_period\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                     \u001b[0mdropout_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 })\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/maikfangogoair/assignment1/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/maikfangogoair/assignment1/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/maikfangogoair/assignment1/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/maikfangogoair/assignment1/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/maikfangogoair/assignment1/.env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1222)\n",
    "size=len([x for x in all_data if x[\"group\"] == \"training\" ])\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 300\n",
    "learning_rate = 1e-4\n",
    "learning_decay = 0.9\n",
    "learning_decay_period = 350\n",
    "eval_every_steps = 100\n",
    "training_steps = 10000\n",
    "print_every = 1\n",
    "eval_size = 300\n",
    "validation_set = [x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"validation\"]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, data_dir + 'spectrogram_ConvNet.ckpt-500')\n",
    "    for i in range(1, training_steps+1):\n",
    "        start_pos = (i-1) * batch_size % size\n",
    "        end_pos = (i-1) * batch_size % size + batch_size \n",
    "        train_fingerprints = np.asarray([x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos])\n",
    "        train_ground_truth = np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos]) \n",
    "        loss, _ ,accurancy,  = sess.run([cross_entropy_mean, train_step, evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: train_fingerprints,\n",
    "                    ground_truth_input: train_ground_truth,\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 0.5\n",
    "                })\n",
    "        if i % print_every == 0:\n",
    "            print(\"Step %d, loss: %f, accurancy: %f.\" % (i, loss, accurancy))\n",
    "        if  i % eval_every_steps == 0:\n",
    "            saver.save(sess, data_dir + model_name +\".ckpt\", global_step=i)\n",
    "            rand_choice = [random.randint(0,len(validation_set)-1) for x in range(eval_size)]\n",
    "            _, accurancy = sess.run([train_step,evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: np.asarray(validation_set)[rand_choice],\n",
    "                    ground_truth_input: np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"validation\"])[rand_choice], \\\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 1.0\n",
    "                })\n",
    "            print(\"Validation accurancy is %f\" % accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-1600\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, data_dir + 'spectrogram_ConvNet.ckpt-1600')\n",
    "    predicted_list = sess.run(predicted_indices, \\\n",
    "        feed_dict={\n",
    "            input: np.asarray([x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"testing\"]) ,\n",
    "            dropout_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_part = np.hstack((predicted_list_part1,predicted_list_part2,predicted_list_part3))\n",
    "\n",
    "predicted_list_part\n",
    "\n",
    "correct_idx = np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"testing\"])\n",
    "\n",
    "wrong_cnt = 0\n",
    "for i in range(correct_idx.shape[0]):\n",
    "    if predicted_list_part[i] != correct_idx[i]:\n",
    "        wrong_cnt += 1\n",
    "wrong_cnt\n",
    "\n",
    "(2979 - 1255)/2979\n",
    "\n",
    "test_data = [x for x in all_data if x[\"group\"] == \"testing\"]\n",
    "test_data[1]\n",
    "\n",
    "wrong_data = []\n",
    "word_list = ['silence','unknown', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "for i in range(correct_idx.shape[0]):\n",
    "    if predicted_list_part[i] != correct_idx[i]:\n",
    "        test_data[i][\"predicted_label\"] = word_list[predicted_list_part[i]]\n",
    "        wrong_data.append(test_data[i])\n",
    "\n",
    "#which pair most confusing?\n",
    "confuse_map = {}\n",
    "for x in wrong_data:\n",
    "    key = x[\"label\"] + \" is confused by \" + x[\"predicted_label\"]\n",
    "    cnt = confuse_map.get(key, 0)\n",
    "    confuse_map[key] = cnt + 1 \n",
    "#https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "import operator\n",
    "sorted_confuse_map = sorted(confuse_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_confuse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
