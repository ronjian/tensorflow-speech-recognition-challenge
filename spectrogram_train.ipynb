{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dir = \"/home/maikfangogoair/tmp/save/\"\n",
    "with open (data_dir + \"spectrogram_v1.pkl\", 'rb') as fp:\n",
    "    all_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "input=tf.placeholder(tf.float32, (None, 99, 161), 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_ConvNet(input):\n",
    "    \"\"\"\n",
    "    architecture as below:\n",
    "     [Conv2D]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "        [Relu]\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "      \n",
    "    output is the logits in shape (batch_size, 12)\n",
    "    \"\"\"\n",
    "    \n",
    "    reshape_input_1 = tf.reshape(input, [-1, 99, 161, 1], 'reshape_input_1')\n",
    "    conv_2 = tf.layers.conv2d(reshape_input_1, 64, (3,3), (1,1), 'same', name='conv_2')\n",
    "    relu_3 = tf.nn.relu(conv_2,'relu_3')\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    print(dropout_prob)\n",
    "    relu_3 = tf.nn.dropout(relu_3, dropout_prob, name = 'dropout')\n",
    "    print(relu_3)\n",
    "    size=int(relu_3.get_shape()[1]) * int(relu_3.get_shape()[2]) * int(relu_3.get_shape()[3])\n",
    "    relu_3 = tf.reshape(relu_3, (-1, size), 'reshape_relu_3')\n",
    "    print(relu_3)\n",
    "    fc_4 = tf.layers.dense(relu_3, 128,name = 'fc_4')\n",
    "    fc_5 = tf.layers.dense(fc_4, 128,name = 'fc_5')\n",
    "    logits = tf.layers.dense(fc_5, 12,name = 'logits')\n",
    "    \n",
    "    return logits,dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_prob:0\", dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 99, 161, 64), dtype=float32)\n",
      "Tensor(\"reshape_relu_3:0\", shape=(?, 1020096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"spectrogram_ConvNet\"\n",
    "if model_name == \"spectrogram_ConvNet\":\n",
    "    logits,dropout_prob = spectrogram_ConvNet(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training steps\n",
    "ground_truth_input = tf.placeholder(tf.int64, [None], name='groundtruth_input')\n",
    "learning_rate_input = tf.placeholder(tf.float32, [], name='learning_rate_input')\n",
    "cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(ground_truth_input, logits)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate_input).minimize(cross_entropy_mean)\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "confusion_matrix = tf.confusion_matrix(ground_truth_input, predicted_indices, num_classes=12)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.000000, loss: 3.920009, accurancy: 0.120000.\n",
      "Step 2.000000, loss: 111.481384, accurancy: 0.080000.\n",
      "Step 3.000000, loss: 37.752464, accurancy: 0.080000.\n",
      "Step 4.000000, loss: 26.697796, accurancy: 0.050000.\n",
      "Step 5.000000, loss: 19.076529, accurancy: 0.110000.\n",
      "Step 6.000000, loss: 13.675368, accurancy: 0.130000.\n",
      "Step 7.000000, loss: 13.753356, accurancy: 0.110000.\n",
      "Step 8.000000, loss: 12.363038, accurancy: 0.100000.\n",
      "Step 9.000000, loss: 7.840403, accurancy: 0.120000.\n",
      "Step 10.000000, loss: 5.706493, accurancy: 0.120000.\n",
      "Step 11.000000, loss: 4.510104, accurancy: 0.080000.\n",
      "Step 12.000000, loss: 5.326552, accurancy: 0.140000.\n",
      "Step 13.000000, loss: 3.475159, accurancy: 0.230000.\n",
      "Step 14.000000, loss: 3.746217, accurancy: 0.050000.\n",
      "Step 15.000000, loss: 4.638170, accurancy: 0.170000.\n",
      "Step 16.000000, loss: 4.132074, accurancy: 0.110000.\n",
      "Step 17.000000, loss: 3.419880, accurancy: 0.170000.\n",
      "Step 18.000000, loss: 2.630610, accurancy: 0.260000.\n",
      "Step 19.000000, loss: 3.020654, accurancy: 0.120000.\n",
      "Step 20.000000, loss: 4.292509, accurancy: 0.070000.\n",
      "Step 21.000000, loss: 2.920331, accurancy: 0.170000.\n",
      "Step 22.000000, loss: 2.575098, accurancy: 0.210000.\n",
      "Step 23.000000, loss: 2.898606, accurancy: 0.190000.\n",
      "Step 24.000000, loss: 2.914571, accurancy: 0.160000.\n",
      "Step 25.000000, loss: 3.734573, accurancy: 0.110000.\n",
      "Step 26.000000, loss: 3.159998, accurancy: 0.180000.\n",
      "Step 27.000000, loss: 2.904793, accurancy: 0.180000.\n",
      "Step 28.000000, loss: 2.606368, accurancy: 0.190000.\n",
      "Step 29.000000, loss: 2.366141, accurancy: 0.140000.\n",
      "Step 30.000000, loss: 2.457302, accurancy: 0.270000.\n",
      "Step 31.000000, loss: 2.392247, accurancy: 0.250000.\n",
      "Step 32.000000, loss: 2.312858, accurancy: 0.280000.\n",
      "Step 33.000000, loss: 2.243365, accurancy: 0.150000.\n",
      "Step 34.000000, loss: 2.490610, accurancy: 0.170000.\n",
      "Step 35.000000, loss: 2.729000, accurancy: 0.240000.\n",
      "Step 36.000000, loss: 2.418895, accurancy: 0.230000.\n",
      "Step 37.000000, loss: 2.522442, accurancy: 0.190000.\n",
      "Step 38.000000, loss: 2.985182, accurancy: 0.190000.\n",
      "Step 39.000000, loss: 3.115611, accurancy: 0.180000.\n",
      "Step 40.000000, loss: 2.567773, accurancy: 0.200000.\n",
      "Step 41.000000, loss: 2.658394, accurancy: 0.190000.\n",
      "Step 42.000000, loss: 2.370017, accurancy: 0.230000.\n",
      "Step 43.000000, loss: 2.410383, accurancy: 0.270000.\n",
      "Step 44.000000, loss: 2.128845, accurancy: 0.240000.\n",
      "Step 45.000000, loss: 2.117943, accurancy: 0.210000.\n",
      "Step 46.000000, loss: 2.277303, accurancy: 0.210000.\n",
      "Step 47.000000, loss: 2.803760, accurancy: 0.230000.\n",
      "Step 48.000000, loss: 2.183359, accurancy: 0.180000.\n",
      "Step 49.000000, loss: 2.228391, accurancy: 0.280000.\n",
      "Step 50.000000, loss: 2.257164, accurancy: 0.290000.\n",
      "Step 51.000000, loss: 2.329352, accurancy: 0.210000.\n",
      "Step 52.000000, loss: 2.472488, accurancy: 0.180000.\n",
      "Step 53.000000, loss: 2.248204, accurancy: 0.330000.\n",
      "Step 54.000000, loss: 2.301449, accurancy: 0.230000.\n",
      "Step 55.000000, loss: 2.172659, accurancy: 0.220000.\n",
      "Step 56.000000, loss: 2.829369, accurancy: 0.280000.\n",
      "Step 57.000000, loss: 2.145695, accurancy: 0.250000.\n",
      "Step 58.000000, loss: 2.580579, accurancy: 0.230000.\n",
      "Step 59.000000, loss: 2.151642, accurancy: 0.300000.\n",
      "Step 60.000000, loss: 2.302998, accurancy: 0.310000.\n",
      "Step 61.000000, loss: 2.729300, accurancy: 0.260000.\n",
      "Step 62.000000, loss: 3.143101, accurancy: 0.160000.\n",
      "Step 63.000000, loss: 3.591687, accurancy: 0.200000.\n",
      "Step 64.000000, loss: 2.525627, accurancy: 0.300000.\n",
      "Step 65.000000, loss: 2.208193, accurancy: 0.270000.\n",
      "Step 66.000000, loss: 2.110849, accurancy: 0.250000.\n",
      "Step 67.000000, loss: 2.205112, accurancy: 0.250000.\n",
      "Step 68.000000, loss: 1.970204, accurancy: 0.300000.\n",
      "Step 69.000000, loss: 2.055522, accurancy: 0.360000.\n",
      "Step 70.000000, loss: 2.609180, accurancy: 0.160000.\n",
      "Step 71.000000, loss: 2.935563, accurancy: 0.190000.\n",
      "Step 72.000000, loss: 2.746747, accurancy: 0.240000.\n",
      "Step 73.000000, loss: 2.450090, accurancy: 0.270000.\n",
      "Step 74.000000, loss: 2.296224, accurancy: 0.260000.\n",
      "Step 75.000000, loss: 1.906204, accurancy: 0.370000.\n",
      "Step 76.000000, loss: 2.364065, accurancy: 0.260000.\n",
      "Step 77.000000, loss: 2.129334, accurancy: 0.250000.\n",
      "Step 78.000000, loss: 1.969767, accurancy: 0.360000.\n",
      "Step 79.000000, loss: 1.902670, accurancy: 0.300000.\n",
      "Step 80.000000, loss: 1.927649, accurancy: 0.370000."
     ]
    }
   ],
   "source": [
    "size=len([x for x in all_data if x[\"group\"] == \"training\" ])\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.9\n",
    "learning_decay_period = 1000\n",
    "eval_every_steps = 1000\n",
    "training_steps = 10000\n",
    "print_every = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1, training_steps+1):\n",
    "        start_pos = (i-1) * batch_size % size\n",
    "        end_pos = (i-1) * batch_size % size + batch_size \n",
    "        train_fingerprints = np.asarray([x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos])\n",
    "        train_ground_truth = np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos]) \n",
    "        loss, _ ,accurancy,  = sess.run([cross_entropy_mean, train_step, evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: train_fingerprints,\n",
    "                    ground_truth_input: train_ground_truth,\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 0.5\n",
    "                })\n",
    "        if i % print_every == 0:\n",
    "            print(\"Step %f, loss: %f, accurancy: %f.\" % (i, loss, accurancy))\n",
    "        if  i % eval_every_steps == 0:\n",
    "            saver.save(sess, data_dir + model_name +\".ckpt\", global_step=i)\n",
    "            accurancy = sess.run(evaluation_step, \\\n",
    "                feed_dict={\n",
    "                    input: np.asarray([x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"validation\"]),\n",
    "                    ground_truth_input: np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"validation\"]),\n",
    "                    dropout_prob: 1.0\n",
    "                })\n",
    "            print(\"Validation accurancy is %f\" % accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
