{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_dir = \"/home/maikfangogoair/tmp/save/\"\n",
    "pkl_file = \"spect_v1.pkl\"\n",
    "with open (data_dir + pkl_file, 'rb') as fp:\n",
    "    all_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "input=tf.placeholder(tf.float32, (None, 99, 161), 'input')\n",
    "def spectrogram_ConvNet(input):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    \n",
    "    \n",
    "    reshape_input = tf.reshape(input, [-1, 99, 161, 1], 'reshape_input')\n",
    "    batch_input = tf.layers.batch_normalization(reshape_input)\n",
    "    \n",
    "    conv_1 = tf.layers.conv2d(batch_input, 64, (5,5), (1,1), 'same', name='conv_1')\n",
    "    relu_1 = tf.nn.relu(conv_1,'relu_1')\n",
    "    dropout_1 = tf.nn.dropout(relu_1, dropout_prob, name = 'dropout_1')\n",
    "    \n",
    "    max_pool = tf.nn.max_pool(dropout_1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "    conv_2 = tf.layers.conv2d(max_pool, 64, (3,3), (1,1), 'same')\n",
    "    relu_2 = tf.nn.relu(conv_2,'relu_2')\n",
    "    dropout_2 = tf.nn.dropout(relu_2, dropout_prob, name = 'dropout_2')\n",
    "    \n",
    "    size=int(dropout_2.get_shape()[1]) * int(dropout_2.get_shape()[2]) * int(dropout_2.get_shape()[3])\n",
    "    flatten = tf.reshape(dropout_2, (-1, size), 'flatten')\n",
    "\n",
    "    logits = tf.layers.dense(flatten, 12,name = 'logits')\n",
    "    \n",
    "    return logits,dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph();\n",
    "input=tf.placeholder(tf.float32, (None, 1, 161, 101))\n",
    "              \n",
    "def VGG11(input, final_class, verbose):\n",
    "    steps = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    features = tf.transpose(input,[0, 2, 3, 1])\n",
    "    if verbose: print(features)\n",
    "    for i in range(len(steps)):\n",
    "        step = steps[i]\n",
    "        with tf.variable_scope(\"Layer\"+str(i+1), reuse=False):\n",
    "            if step == 'M':\n",
    "                features = tf.layers.max_pooling2d(features , (2,2) , (2,2) ,\"same\")\n",
    "                if verbose: print(features)\n",
    "            else:\n",
    "                features = tf.layers.conv2d(features, step, (3,3), (1,1), 'same', name='conv_1')\n",
    "                if verbose: print(features)\n",
    "                features = tf.layers.batch_normalization(features)\n",
    "                if verbose: print(features)\n",
    "                features = tf.nn.relu(features)\n",
    "                if verbose: print(features)\n",
    "    features = tf.layers.average_pooling2d(features , (1,1), (1,1), 'same')\n",
    "    if verbose: print(features)\n",
    "    features = tf.layers.flatten(features)\n",
    "    if verbose: print(features)\n",
    "    features = tf.layers.dense(features, 512)\n",
    "    if verbose: print(features)\n",
    "    logits = tf.layers.dense(features, final_class )\n",
    "    if verbose: print(features)\n",
    "    \n",
    "    return logits,dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"VGG11\"\n",
    "model_cfg = {\"spectrogram_ConvNet\": spectrogram_ConvNet, \"VGG11\": VGG11}\n",
    "model = model_cfg[model_name]\n",
    "final_class = 30\n",
    "logits,dropout_prob = model(input, final_class, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training steps\n",
    "ground_truth_input = tf.placeholder(tf.int64, [None], name='groundtruth_input')\n",
    "learning_rate_input = tf.placeholder(tf.float32, [], name='learning_rate_input')\n",
    "cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(ground_truth_input, logits)\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate_input).minimize(cross_entropy_mean)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate_input).minimize(cross_entropy_mean)\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "confusion_matrix = tf.confusion_matrix(ground_truth_input, predicted_indices, num_classes=12)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set count:6798\n",
      "Step 10, loss: 3.396253, accurancy: 0.050000.\n",
      "Step 20, loss: 3.377050, accurancy: 0.060000.\n",
      "Step 30, loss: 3.237314, accurancy: 0.050000.\n",
      "Step 40, loss: 2.989235, accurancy: 0.170000.\n",
      "Step 50, loss: 2.843735, accurancy: 0.160000.\n",
      "Step 60, loss: 2.320439, accurancy: 0.360000.\n",
      "Step 70, loss: 2.692460, accurancy: 0.300000.\n",
      "Step 80, loss: 1.835875, accurancy: 0.440000.\n",
      "Step 90, loss: 1.621885, accurancy: 0.520000.\n",
      "Step 100, loss: 1.549716, accurancy: 0.510000.\n",
      "Step 110, loss: 1.153748, accurancy: 0.660000.\n",
      "Step 120, loss: 0.887473, accurancy: 0.730000.\n",
      "Step 130, loss: 0.998535, accurancy: 0.680000.\n",
      "Step 140, loss: 1.051070, accurancy: 0.660000.\n",
      "Step 150, loss: 1.405718, accurancy: 0.640000.\n",
      "Step 160, loss: 1.043543, accurancy: 0.700000.\n",
      "Step 170, loss: 0.920910, accurancy: 0.710000.\n",
      "Step 180, loss: 1.006861, accurancy: 0.730000.\n",
      "Step 190, loss: 0.805675, accurancy: 0.800000.\n",
      "Step 200, loss: 0.633963, accurancy: 0.760000.\n",
      "Step 210, loss: 0.901938, accurancy: 0.750000.\n",
      "Step 220, loss: 0.697654, accurancy: 0.790000.\n",
      "Step 230, loss: 0.839145, accurancy: 0.790000.\n",
      "Step 240, loss: 0.590620, accurancy: 0.840000.\n",
      "Step 250, loss: 0.514904, accurancy: 0.880000.\n",
      "Step 260, loss: 0.628482, accurancy: 0.790000.\n",
      "Step 270, loss: 0.646799, accurancy: 0.820000.\n",
      "Step 280, loss: 0.465066, accurancy: 0.850000.\n",
      "Step 290, loss: 0.601492, accurancy: 0.880000.\n",
      "Step 300, loss: 0.471244, accurancy: 0.850000.\n",
      "Step 310, loss: 0.486169, accurancy: 0.870000.\n",
      "Step 320, loss: 0.567926, accurancy: 0.790000.\n",
      "Step 330, loss: 0.447480, accurancy: 0.850000.\n",
      "Step 340, loss: 0.447180, accurancy: 0.850000.\n",
      "Step 350, loss: 0.482785, accurancy: 0.860000.\n",
      "Step 360, loss: 0.389031, accurancy: 0.850000.\n",
      "Step 370, loss: 0.237690, accurancy: 0.910000.\n",
      "Step 380, loss: 0.648365, accurancy: 0.840000.\n",
      "Step 390, loss: 0.295720, accurancy: 0.880000.\n",
      "Step 400, loss: 0.380470, accurancy: 0.910000.\n",
      "Step 410, loss: 0.407802, accurancy: 0.870000.\n",
      "Step 420, loss: 0.427038, accurancy: 0.870000.\n",
      "Step 430, loss: 0.301831, accurancy: 0.920000.\n",
      "Step 440, loss: 0.692157, accurancy: 0.820000.\n",
      "Step 450, loss: 0.343521, accurancy: 0.870000.\n",
      "Step 460, loss: 0.329955, accurancy: 0.900000.\n",
      "Step 470, loss: 0.379840, accurancy: 0.920000.\n",
      "Step 480, loss: 0.288628, accurancy: 0.890000.\n",
      "Step 490, loss: 0.416243, accurancy: 0.870000.\n",
      "Step 500, loss: 0.314936, accurancy: 0.930000.\n",
      "Step 510, loss: 0.201463, accurancy: 0.930000.\n",
      "Step 520, loss: 0.296599, accurancy: 0.900000.\n",
      "Step 530, loss: 0.275945, accurancy: 0.930000.\n",
      "Step 540, loss: 0.261634, accurancy: 0.940000.\n",
      "Step 550, loss: 0.564349, accurancy: 0.860000.\n",
      "Step 560, loss: 0.451949, accurancy: 0.880000.\n",
      "Step 570, loss: 0.234592, accurancy: 0.940000.\n",
      "Step 580, loss: 0.200631, accurancy: 0.960000.\n",
      "Step 590, loss: 0.447095, accurancy: 0.880000.\n",
      "Step 600, loss: 0.236234, accurancy: 0.970000.\n",
      "Step 610, loss: 0.272113, accurancy: 0.900000.\n",
      "Step 620, loss: 0.418027, accurancy: 0.860000.\n",
      "Step 630, loss: 0.338739, accurancy: 0.910000.\n",
      "Step 640, loss: 0.255762, accurancy: 0.930000.\n",
      "Step 650, loss: 0.439732, accurancy: 0.910000.\n",
      "Step 660, loss: 0.199989, accurancy: 0.940000.\n",
      "Step 670, loss: 0.397653, accurancy: 0.870000.\n",
      "Step 680, loss: 0.240502, accurancy: 0.920000.\n",
      "Step 690, loss: 0.115266, accurancy: 0.970000.\n",
      "Step 700, loss: 0.253586, accurancy: 0.930000.\n",
      "Step 710, loss: 0.155024, accurancy: 0.950000.\n",
      "Step 720, loss: 0.315438, accurancy: 0.930000.\n",
      "Step 730, loss: 0.585991, accurancy: 0.860000.\n",
      "Step 740, loss: 0.365057, accurancy: 0.900000.\n",
      "Step 750, loss: 0.236702, accurancy: 0.940000.\n",
      "Step 760, loss: 0.427797, accurancy: 0.870000.\n",
      "Step 770, loss: 0.377216, accurancy: 0.880000.\n",
      "Step 780, loss: 0.112949, accurancy: 0.970000.\n",
      "Step 790, loss: 0.237110, accurancy: 0.930000.\n",
      "Step 800, loss: 0.277435, accurancy: 0.940000.\n",
      "Step 810, loss: 0.425051, accurancy: 0.870000.\n",
      "Step 820, loss: 0.224713, accurancy: 0.930000.\n",
      "Step 830, loss: 0.239225, accurancy: 0.920000.\n",
      "Step 840, loss: 0.243325, accurancy: 0.910000.\n",
      "Step 850, loss: 0.308820, accurancy: 0.910000.\n",
      "Step 860, loss: 0.189202, accurancy: 0.920000.\n",
      "Step 870, loss: 0.400873, accurancy: 0.880000.\n",
      "Step 880, loss: 0.218252, accurancy: 0.920000.\n",
      "Step 890, loss: 0.350778, accurancy: 0.890000.\n",
      "Step 900, loss: 0.137828, accurancy: 0.950000.\n",
      "Step 910, loss: 0.259916, accurancy: 0.900000.\n",
      "Step 920, loss: 0.316450, accurancy: 0.900000.\n",
      "Step 930, loss: 0.227799, accurancy: 0.960000.\n",
      "Step 940, loss: 0.245886, accurancy: 0.940000."
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1222)\n",
    "size=len([x for x in all_data if x[\"group\"] == \"training\" ])\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.9\n",
    "learning_decay_period = 1000\n",
    "eval_every_steps = 1000\n",
    "training_steps = 20000\n",
    "print_every = 10\n",
    "eval_size = 100\n",
    "validation_size = len([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"validation\"])\n",
    "print(\"validation set count:\" + str(validation_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, data_dir + 'spectrogram_ConvNet.ckpt-700')\n",
    "    for i in range(1, training_steps+1):\n",
    "        start_pos = (i-1) * batch_size % size\n",
    "        end_pos = (i-1) * batch_size % size + batch_size \n",
    "        train_fingerprints = np.asarray([x[\"feature\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos])\n",
    "        train_ground_truth = np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"training\"][start_pos:end_pos]) \n",
    "        loss, _ ,accurancy,  = sess.run([cross_entropy_mean, train_step, evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: train_fingerprints,\n",
    "                    ground_truth_input: train_ground_truth,\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 0.5\n",
    "                })\n",
    "        if i % print_every == 0:\n",
    "            print(\"Step %d, loss: %f, accurancy: %f.\" % (i, loss, accurancy))\n",
    "        if  i % eval_every_steps == 0:\n",
    "            saver.save(sess, data_dir + model_name +\".ckpt\", global_step=i)\n",
    "            rand_choice = [random.randint(0,validation_size-1) for x in range(eval_size)]\n",
    "            _, accurancy = sess.run([train_step,evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: np.asarray([x[\"feature\"] for x in all_data if x[\"group\"] == \"validation\"])[rand_choice],\n",
    "                    ground_truth_input: np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"validation\"])[rand_choice], \\\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 1.0\n",
    "                })\n",
    "            print(\"Validation accurancy is %f\" % accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/tmp/save/spectrogram_ConvNet.ckpt-1600\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, data_dir + 'spectrogram_ConvNet.ckpt-1600')\n",
    "    predicted_list = sess.run(predicted_indices, \\\n",
    "        feed_dict={\n",
    "            input: np.asarray([x[\"spectrogram\"] for x in all_data if x[\"group\"] == \"testing\"]) ,\n",
    "            dropout_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list_part = np.hstack((predicted_list_part1,predicted_list_part2,predicted_list_part3))\n",
    "\n",
    "predicted_list_part\n",
    "\n",
    "correct_idx = np.asarray([x[\"label_idx\"] for x in all_data if x[\"group\"] == \"testing\"])\n",
    "\n",
    "wrong_cnt = 0\n",
    "for i in range(correct_idx.shape[0]):\n",
    "    if predicted_list_part[i] != correct_idx[i]:\n",
    "        wrong_cnt += 1\n",
    "wrong_cnt\n",
    "\n",
    "(2979 - 1255)/2979\n",
    "\n",
    "test_data = [x for x in all_data if x[\"group\"] == \"testing\"]\n",
    "test_data[1]\n",
    "\n",
    "wrong_data = []\n",
    "word_list = ['silence','unknown', 'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "for i in range(correct_idx.shape[0]):\n",
    "    if predicted_list_part[i] != correct_idx[i]:\n",
    "        test_data[i][\"predicted_label\"] = word_list[predicted_list_part[i]]\n",
    "        wrong_data.append(test_data[i])\n",
    "\n",
    "#which pair most confusing?\n",
    "confuse_map = {}\n",
    "for x in wrong_data:\n",
    "    key = x[\"label\"] + \" is confused by \" + x[\"predicted_label\"]\n",
    "    cnt = confuse_map.get(key, 0)\n",
    "    confuse_map[key] = cnt + 1 \n",
    "#https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "import operator\n",
    "sorted_confuse_map = sorted(confuse_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_confuse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
