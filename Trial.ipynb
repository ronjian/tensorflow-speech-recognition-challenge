{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda:\n",
    "[AlexNet](#AlexNet)  \n",
    "[simple ConvNet](#simple-ConvNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph();\n",
    "input=tf.placeholder(tf.float32, (None, 3920), 'input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "The famous AlexNet [paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n",
    "<img src=\"assets/AlexNet_architecture.png\" width=\"800\" />\n",
    "<img src=\"assets/cs231n_alexNet.png\" width=\"800\" />\n",
    "\n",
    "[tensorflow impletation](https://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward_newtf.py)  \n",
    "in the following architecture:  \n",
    ".conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')  \n",
    ".lrn(2, 2e-05, 0.75, name='norm1')  \n",
    ".max_pool(3, 3, 2, 2, padding='VALID', name='pool1')  \n",
    ".conv(5, 5, 256, 1, 1, group=2, name='conv2')  \n",
    ".lrn(2, 2e-05, 0.75, name='norm2')  \n",
    ".max_pool(3, 3, 2, 2, padding='VALID', name='pool2')  \n",
    ".conv(3, 3, 384, 1, 1, name='conv3')  \n",
    ".conv(3, 3, 384, 1, 1, group=2, name='conv4')  \n",
    ".conv(3, 3, 256, 1, 1, group=2, name='conv5')  \n",
    ".max_pool(3, 3, 2, 2, padding='VALID', name='pool3')  \n",
    ".fc(4096, name='fc6')  \n",
    ".fc(4096, name='fc7')  \n",
    ".fc(1000, relu=False, name='fc8')  \n",
    ".softmax(name='prob')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet(input, is_training):\n",
    "    #reshape to 2d [batch_size, 98, 40, 1]  name='reshape_input_1'\n",
    "    reshape_input_1 = tf.reshape(input, [-1, 98, 40, 1], 'reshape_input_1')\n",
    "    #conv(6, 3, 96, 2, 2, padding='VALID', name='conv_2')  \n",
    "    conv_2 = tf.layers.conv2d(reshape_input_1, 96, (6,3), (2,2), 'same', name='conv_2')\n",
    "\n",
    "    #lrn(2, 2e-05, 0.75, name='norm_3')  \n",
    "    norm_3 = tf.nn.local_response_normalization(conv_2, 2, None, 2e-05, 0.75, 'norm_3')\n",
    "\n",
    "    #max_pool(3, 2, 1, 1, padding='VALID', name='pool_4')  \n",
    "    pool_4 = tf.layers.max_pooling2d(norm_3, (3,2), (1,1), 'same', 'channels_last', 'pool_4')\n",
    "\n",
    "    #conv(3, 2, 256, 1, 1, group=2, name='conv_5')  \n",
    "    conv_5 = tf.layers.conv2d(pool_4, 256, (3,2), (1,1), 'same', name='conv_5')\n",
    "\n",
    "    #lrn(2, 2e-05, 0.75, name='norm_6')  \n",
    "    norm_6 = tf.nn.local_response_normalization(conv_5, 2, None, 2e-05, 0.75, 'norm_6')\n",
    "\n",
    "    #max_pool(3, 2, 1, 1, padding='VALID', name='pool_7')  \n",
    "    pool_7 = tf.layers.max_pooling2d(norm_6, (3,2), (1,1), 'same', 'channels_last', 'pool_7')\n",
    "\n",
    "    #conv(3, 2, 384, 1, 1, name='conv_8')  \n",
    "    conv_8 = tf.layers.conv2d(pool_7, 384, (3,2), (1,1), 'valid', name='conv_8')\n",
    "\n",
    "    #conv(3, 2, 384, 1, 1, group=2, name='conv_9')  \n",
    "    conv_9 = tf.layers.conv2d(conv_8, 384, (3,2), (1,1), 'valid', name='conv_9')\n",
    "\n",
    "    #conv(3, 2, 256, 1, 1, group=2, name='conv_10')  \n",
    "    conv_10 = tf.layers.conv2d(conv_9, 384, (3,2), (1,1), 'valid', name='conv_10')\n",
    "\n",
    "    #max_pool(3, 2, 2, 2, padding='VALID', name='pool_11')  \n",
    "    pool_11 = tf.layers.max_pooling2d(conv_10, (3,2), (2,2), 'valid', 'channels_last', 'pool_11')\n",
    "\n",
    "    #reshape to 1d [-1, ?] name='reshape_input_12'\n",
    "    size=int(pool_11.get_shape()[1]) * int(pool_11.get_shape()[2]) * int(pool_11.get_shape()[3])\n",
    "    reshape_input_12 = tf.reshape(pool_11, (-1, size), 'reshape_input_12')\n",
    "\n",
    "    #fc(1000, name='fc_13')  \n",
    "    fc_13 = tf.layers.dense(reshape_input_12, 1000, activation=tf.nn.relu, name='fc_13')\n",
    "\n",
    "    #fc(1000, name='fc_14')  \n",
    "    fc_14 = tf.layers.dense(fc_13, 1000, activation=tf.nn.relu, name='fc_14')\n",
    "\n",
    "    #fc(12, relu=False, name='fc_15')  \n",
    "    fc_15 = tf.layers.dense(fc_14, 12, activation=None, name='fc_15')\n",
    "\n",
    "    #softmax(name='prob_16'))  \n",
    "    prob_16 = tf.nn.softmax(fc_15, name='prob_16')\n",
    "    \n",
    "    return prob_16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training result for AlexNet:\n",
    "```\n",
    "Step 0.000000, loss: 2.484012, accurancy: 0.120000.\n",
    "Step 1.000000, loss: 2.485092, accurancy: 0.120000.\n",
    "Step 2.000000, loss: 2.484010, accurancy: 0.060000.\n",
    "Step 3.000000, loss: 2.485292, accurancy: 0.080000.\n",
    "Step 4.000000, loss: 2.484657, accurancy: 0.140000.\n",
    "Step 5.000000, loss: 2.485170, accurancy: 0.050000.\n",
    "Step 6.000000, loss: 2.484865, accurancy: 0.080000.\n",
    "Step 7.000000, loss: 2.484811, accurancy: 0.090000.\n",
    "Step 8.000000, loss: 2.485102, accurancy: 0.110000.\n",
    "Step 9.000000, loss: 2.486204, accurancy: 0.070000.\n",
    "Step 10.000000, loss: 2.484865, accurancy: 0.120000.\n",
    "Step 11.000000, loss: 2.484856, accurancy: 0.070000.\n",
    "Step 12.000000, loss: 2.485339, accurancy: 0.060000.\n",
    "Step 13.000000, loss: 2.483910, accurancy: 0.120000.\n",
    "Step 14.000000, loss: 2.485012, accurancy: 0.130000.\n",
    "Step 15.000000, loss: 2.484593, accurancy: 0.110000.\n",
    "Step 16.000000, loss: 2.484843, accurancy: 0.070000.\n",
    "Step 17.000000, loss: 2.485991, accurancy: 0.090000.\n",
    "Step 18.000000, loss: 2.483060, accurancy: 0.070000.\n",
    "Step 19.000000, loss: 2.485357, accurancy: 0.040000.\n",
    "```\n",
    "AlexNet is too deep for the feature in shape (98,40), \n",
    "the training can't learn anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"simple-ConvNet\">Simple ConvNet<h2/>\n",
    "Inspired by [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/models.py#L273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleConvNet(input):\n",
    "    \"\"\"\n",
    "    architecture as below:\n",
    "     [Conv2D]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "        [Relu]\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "          v\n",
    "      [MatMul]<-(weights)\n",
    "          v\n",
    "      [BiasAdd]<-(bias)\n",
    "      \n",
    "    output is the logits in shape (batch_size, 12)\n",
    "    \"\"\"\n",
    "    \n",
    "    reshape_input_1 = tf.reshape(input, [-1, 98, 40, 1], 'reshape_input_1')\n",
    "    conv_2 = tf.layers.conv2d(reshape_input_1, 186, (98,8), (1,1), 'valid', name='conv_2')\n",
    "    relu_3 = tf.nn.relu(conv_2,'relu_3')\n",
    "    dropout_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "    relu_3 = tf.nn.dropout(relu_3, dropout_prob, name = 'dropout')\n",
    "    size=int(relu_3.get_shape()[1]) * int(relu_3.get_shape()[2]) * int(relu_3.get_shape()[3])\n",
    "    relu_3 = tf.reshape(relu_3, (-1, size), 'reshape_relu_3')\n",
    "    fc_4 = tf.layers.dense(relu_3, 128,name = 'fc_4')\n",
    "    fc_5 = tf.layers.dense(fc_4, 128,name = 'fc_5')\n",
    "    logits = tf.layers.dense(fc_5, 12,name = 'logits')\n",
    "    \n",
    "    return logits,dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"simpleConvNet\"\n",
    "if model_name == \"AlexNet\":\n",
    "    logits = AlexNet(input)\n",
    "elif model_name == \"simpleConvNet\":\n",
    "    logits,dropout_prob = simpleConvNet(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training steps\n",
    "ground_truth_input = tf.placeholder(tf.int64, [None], name='groundtruth_input')\n",
    "learning_rate_input = tf.placeholder(tf.float32, [], name='learning_rate_input')\n",
    "cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(ground_truth_input, logits)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate_input).minimize(cross_entropy_mean)\n",
    "\n",
    "predicted_indices = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(predicted_indices, ground_truth_input)\n",
    "confusion_matrix = tf.confusion_matrix(ground_truth_input, predicted_indices, num_classes=12)\n",
    "evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "data_dir = \"/home/maikfangogoair/tmp/\"\n",
    "train_fingerprints=np.load(data_dir+\"train_fingerprints.npy\")\n",
    "train_ground_truth=np.load(data_dir+\"train_ground_truth.npy\")\n",
    "validate_fingerprints=np.load(data_dir+\"validation_fingerprints.npy\")\n",
    "validate_ground_truth=np.load(data_dir+\"validation_ground_truth.npy\")\n",
    "test_fingerprints=np.load(data_dir+\"test_fingerprints.npy\")\n",
    "test_ground_truth=np.load(data_dir+\"test_ground_truth.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22246, 3920)\n",
      "(22246,)\n",
      "(3093, 3920)\n",
      "(3093,)\n",
      "(3081, 3920)\n",
      "(3081,)\n"
     ]
    }
   ],
   "source": [
    "print(train_fingerprints.shape)\n",
    "print(train_ground_truth.shape)\n",
    "print(validate_fingerprints.shape)\n",
    "print(validate_ground_truth.shape)\n",
    "print(test_fingerprints.shape)\n",
    "print(test_ground_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=train_ground_truth.shape[0]\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100.000000, loss: 2.320090, accurancy: 0.200000.\n",
      "Step 200.000000, loss: 2.200897, accurancy: 0.320000.\n",
      "Step 300.000000, loss: 2.102604, accurancy: 0.300000.\n",
      "Step 400.000000, loss: 2.140839, accurancy: 0.340000.\n",
      "Step 500.000000, loss: 2.037806, accurancy: 0.360000.\n",
      "Step 600.000000, loss: 1.923688, accurancy: 0.340000.\n",
      "Step 700.000000, loss: 1.843368, accurancy: 0.390000.\n",
      "Step 800.000000, loss: 1.964853, accurancy: 0.390000.\n",
      "Step 900.000000, loss: 1.836546, accurancy: 0.460000.\n",
      "Step 1000.000000, loss: 1.804350, accurancy: 0.450000.\n",
      "Validation accurancy is 0.485936\n",
      "Step 1100.000000, loss: 1.941732, accurancy: 0.380000.\n",
      "Step 1200.000000, loss: 1.738186, accurancy: 0.460000.\n",
      "Step 1300.000000, loss: 1.634242, accurancy: 0.480000.\n",
      "Step 1400.000000, loss: 1.744711, accurancy: 0.470000.\n",
      "Step 1500.000000, loss: 1.852699, accurancy: 0.440000.\n",
      "Step 1600.000000, loss: 1.694729, accurancy: 0.480000.\n",
      "Step 1700.000000, loss: 1.656821, accurancy: 0.500000.\n",
      "Step 1800.000000, loss: 1.828674, accurancy: 0.450000.\n",
      "Step 1900.000000, loss: 1.605244, accurancy: 0.470000.\n",
      "Step 2000.000000, loss: 1.487569, accurancy: 0.520000.\n",
      "Validation accurancy is 0.554478\n",
      "Step 2100.000000, loss: 1.496809, accurancy: 0.560000.\n",
      "Step 2200.000000, loss: 1.744783, accurancy: 0.430000.\n",
      "Step 2300.000000, loss: 1.610920, accurancy: 0.470000.\n",
      "Step 2400.000000, loss: 1.729406, accurancy: 0.450000.\n",
      "Step 2500.000000, loss: 1.562867, accurancy: 0.540000.\n",
      "Step 2600.000000, loss: 1.481460, accurancy: 0.530000.\n",
      "Step 2700.000000, loss: 1.480649, accurancy: 0.540000.\n",
      "Step 2800.000000, loss: 1.515212, accurancy: 0.510000.\n",
      "Step 2900.000000, loss: 1.512863, accurancy: 0.470000.\n",
      "Step 3000.000000, loss: 1.458468, accurancy: 0.530000.\n",
      "Validation accurancy is 0.576463\n",
      "Step 3100.000000, loss: 1.474823, accurancy: 0.500000.\n",
      "Step 3200.000000, loss: 1.430315, accurancy: 0.530000.\n",
      "Step 3300.000000, loss: 1.451852, accurancy: 0.530000.\n",
      "Step 3400.000000, loss: 1.681519, accurancy: 0.450000.\n",
      "Step 3500.000000, loss: 1.559735, accurancy: 0.520000.\n",
      "Step 3600.000000, loss: 1.416698, accurancy: 0.590000.\n",
      "Step 3700.000000, loss: 1.579245, accurancy: 0.540000.\n",
      "Step 3800.000000, loss: 1.469637, accurancy: 0.520000.\n",
      "Step 3900.000000, loss: 1.659009, accurancy: 0.490000.\n",
      "Step 4000.000000, loss: 1.337192, accurancy: 0.610000.\n",
      "Validation accurancy is 0.605561\n",
      "Step 4100.000000, loss: 1.362395, accurancy: 0.540000.\n",
      "Step 4200.000000, loss: 1.442412, accurancy: 0.560000.\n",
      "Step 4300.000000, loss: 1.284605, accurancy: 0.620000.\n",
      "Step 4400.000000, loss: 1.443167, accurancy: 0.480000.\n",
      "Step 4500.000000, loss: 1.434494, accurancy: 0.520000.\n",
      "Step 4600.000000, loss: 1.457552, accurancy: 0.500000.\n",
      "Step 4700.000000, loss: 1.371181, accurancy: 0.600000.\n",
      "Step 4800.000000, loss: 1.305545, accurancy: 0.700000.\n",
      "Step 4900.000000, loss: 1.244207, accurancy: 0.610000.\n",
      "Step 5000.000000, loss: 1.506555, accurancy: 0.460000.\n",
      "Validation accurancy is 0.621726\n",
      "Step 5100.000000, loss: 1.542846, accurancy: 0.550000.\n",
      "Step 5200.000000, loss: 1.248780, accurancy: 0.550000.\n",
      "Step 5300.000000, loss: 1.428072, accurancy: 0.540000.\n",
      "Step 5400.000000, loss: 1.180204, accurancy: 0.690000.\n",
      "Step 5500.000000, loss: 1.359764, accurancy: 0.630000.\n",
      "Step 5600.000000, loss: 1.444574, accurancy: 0.520000.\n",
      "Step 5700.000000, loss: 1.438279, accurancy: 0.550000.\n",
      "Step 5800.000000, loss: 1.443109, accurancy: 0.530000.\n",
      "Step 5900.000000, loss: 1.131180, accurancy: 0.670000.\n",
      "Step 6000.000000, loss: 1.317086, accurancy: 0.550000.\n",
      "Validation accurancy is 0.641125\n",
      "Step 6100.000000, loss: 1.479871, accurancy: 0.530000.\n",
      "Step 6200.000000, loss: 1.339314, accurancy: 0.590000.\n",
      "Step 6300.000000, loss: 1.328057, accurancy: 0.590000.\n",
      "Step 6400.000000, loss: 1.265533, accurancy: 0.680000.\n",
      "Step 6500.000000, loss: 1.284906, accurancy: 0.580000.\n",
      "Step 6600.000000, loss: 1.346902, accurancy: 0.560000.\n",
      "Step 6700.000000, loss: 1.239395, accurancy: 0.640000.\n",
      "Step 6800.000000, loss: 1.377986, accurancy: 0.570000.\n",
      "Step 6900.000000, loss: 1.241653, accurancy: 0.590000.\n",
      "Step 7000.000000, loss: 1.221570, accurancy: 0.610000.\n",
      "Validation accurancy is 0.652764\n",
      "Step 7100.000000, loss: 1.559222, accurancy: 0.520000.\n",
      "Step 7200.000000, loss: 1.343474, accurancy: 0.500000.\n",
      "Step 7300.000000, loss: 1.460761, accurancy: 0.550000.\n",
      "Step 7400.000000, loss: 1.385806, accurancy: 0.570000.\n",
      "Step 7500.000000, loss: 1.293157, accurancy: 0.610000.\n",
      "Step 7600.000000, loss: 1.279550, accurancy: 0.650000.\n",
      "Step 7700.000000, loss: 1.278853, accurancy: 0.640000.\n",
      "Step 7800.000000, loss: 1.293419, accurancy: 0.550000.\n",
      "Step 7900.000000, loss: 1.378477, accurancy: 0.620000.\n",
      "Step 8000.000000, loss: 1.317793, accurancy: 0.660000.\n",
      "Validation accurancy is 0.659554\n",
      "Step 8100.000000, loss: 1.129668, accurancy: 0.610000.\n",
      "Step 8200.000000, loss: 1.079112, accurancy: 0.690000.\n",
      "Step 8300.000000, loss: 1.435768, accurancy: 0.520000.\n",
      "Step 8400.000000, loss: 1.221525, accurancy: 0.630000.\n",
      "Step 8500.000000, loss: 1.198090, accurancy: 0.630000.\n",
      "Step 8600.000000, loss: 1.264883, accurancy: 0.600000.\n",
      "Step 8700.000000, loss: 1.196211, accurancy: 0.640000.\n",
      "Step 8800.000000, loss: 1.395991, accurancy: 0.600000.\n",
      "Step 8900.000000, loss: 1.376688, accurancy: 0.560000.\n",
      "Step 9000.000000, loss: 1.307727, accurancy: 0.580000.\n",
      "Validation accurancy is 0.673456\n",
      "Step 9100.000000, loss: 1.189085, accurancy: 0.600000.\n",
      "Step 9200.000000, loss: 1.241573, accurancy: 0.540000.\n",
      "Step 9300.000000, loss: 1.237898, accurancy: 0.580000.\n",
      "Step 9400.000000, loss: 1.347350, accurancy: 0.570000.\n",
      "Step 9500.000000, loss: 1.206180, accurancy: 0.690000.\n",
      "Step 9600.000000, loss: 1.034848, accurancy: 0.630000.\n",
      "Step 9700.000000, loss: 1.175975, accurancy: 0.600000.\n",
      "Step 9800.000000, loss: 1.418653, accurancy: 0.540000.\n",
      "Step 9900.000000, loss: 1.168163, accurancy: 0.680000.\n",
      "Step 10000.000000, loss: 1.127314, accurancy: 0.680000.\n",
      "Validation accurancy is 0.679276\n",
      "Step 10100.000000, loss: 1.245737, accurancy: 0.600000.\n",
      "Step 10200.000000, loss: 1.215043, accurancy: 0.620000.\n",
      "Step 10300.000000, loss: 0.975053, accurancy: 0.710000.\n",
      "Step 10400.000000, loss: 1.228502, accurancy: 0.680000.\n",
      "Step 10500.000000, loss: 1.302508, accurancy: 0.590000.\n",
      "Step 10600.000000, loss: 1.206560, accurancy: 0.660000.\n",
      "Step 10700.000000, loss: 1.238217, accurancy: 0.580000.\n",
      "Step 10800.000000, loss: 1.052007, accurancy: 0.690000.\n",
      "Step 10900.000000, loss: 1.071692, accurancy: 0.640000.\n",
      "Step 11000.000000, loss: 1.052629, accurancy: 0.640000.\n",
      "Validation accurancy is 0.686712\n",
      "Step 11100.000000, loss: 1.043867, accurancy: 0.720000.\n",
      "Step 11200.000000, loss: 1.191602, accurancy: 0.630000.\n",
      "Step 11300.000000, loss: 1.134020, accurancy: 0.670000.\n",
      "Step 11400.000000, loss: 1.206995, accurancy: 0.640000.\n",
      "Step 11500.000000, loss: 1.048394, accurancy: 0.670000.\n",
      "Step 11600.000000, loss: 1.146306, accurancy: 0.660000.\n",
      "Step 11700.000000, loss: 1.140967, accurancy: 0.610000.\n",
      "Step 11800.000000, loss: 1.403185, accurancy: 0.520000.\n",
      "Step 11900.000000, loss: 1.097282, accurancy: 0.680000.\n",
      "Step 12000.000000, loss: 1.366511, accurancy: 0.650000.\n",
      "Validation accurancy is 0.688329\n",
      "Step 12100.000000, loss: 1.432220, accurancy: 0.630000.\n",
      "Step 12200.000000, loss: 1.049935, accurancy: 0.650000.\n",
      "Step 12300.000000, loss: 1.135105, accurancy: 0.700000.\n",
      "Step 12400.000000, loss: 0.964036, accurancy: 0.690000.\n",
      "Step 12500.000000, loss: 1.057827, accurancy: 0.690000.\n",
      "Step 12600.000000, loss: 1.034184, accurancy: 0.670000.\n",
      "Step 12700.000000, loss: 1.224488, accurancy: 0.600000.\n",
      "Step 12800.000000, loss: 1.018772, accurancy: 0.710000.\n",
      "Step 12900.000000, loss: 1.088361, accurancy: 0.670000.\n",
      "Step 13000.000000, loss: 1.104456, accurancy: 0.690000.\n",
      "Validation accurancy is 0.693178\n",
      "Step 13100.000000, loss: 1.147147, accurancy: 0.640000.\n",
      "Step 13200.000000, loss: 1.095221, accurancy: 0.620000.\n",
      "Step 13300.000000, loss: 1.044346, accurancy: 0.610000.\n",
      "Step 13400.000000, loss: 1.003922, accurancy: 0.650000.\n",
      "Step 13500.000000, loss: 1.049883, accurancy: 0.610000.\n",
      "Step 13600.000000, loss: 0.997003, accurancy: 0.680000.\n",
      "Step 13700.000000, loss: 1.046852, accurancy: 0.690000.\n",
      "Step 13800.000000, loss: 1.152066, accurancy: 0.660000.\n",
      "Step 13900.000000, loss: 1.173251, accurancy: 0.610000.\n",
      "Step 14000.000000, loss: 1.021608, accurancy: 0.650000.\n",
      "Validation accurancy is 0.698998\n",
      "Step 14100.000000, loss: 0.912266, accurancy: 0.680000.\n",
      "Step 14200.000000, loss: 0.995761, accurancy: 0.660000.\n",
      "Step 14300.000000, loss: 1.221613, accurancy: 0.650000.\n",
      "Step 14400.000000, loss: 1.104143, accurancy: 0.620000.\n",
      "Step 14500.000000, loss: 1.083373, accurancy: 0.650000.\n",
      "Step 14600.000000, loss: 1.146365, accurancy: 0.590000.\n",
      "Step 14700.000000, loss: 1.011650, accurancy: 0.680000.\n",
      "Step 14800.000000, loss: 1.067446, accurancy: 0.670000.\n",
      "Step 14900.000000, loss: 0.987817, accurancy: 0.680000.\n",
      "Step 15000.000000, loss: 0.994575, accurancy: 0.670000.\n",
      "Validation accurancy is 0.704171\n",
      "Step 15100.000000, loss: 1.137606, accurancy: 0.680000.\n",
      "Step 15200.000000, loss: 1.033852, accurancy: 0.640000.\n",
      "Step 15300.000000, loss: 1.073340, accurancy: 0.700000.\n",
      "Step 15400.000000, loss: 1.245811, accurancy: 0.620000.\n",
      "Step 15500.000000, loss: 1.168477, accurancy: 0.620000.\n",
      "Step 15600.000000, loss: 1.112466, accurancy: 0.640000.\n",
      "Step 15700.000000, loss: 0.918169, accurancy: 0.720000.\n",
      "Step 15800.000000, loss: 0.955779, accurancy: 0.710000.\n",
      "Step 15900.000000, loss: 1.013074, accurancy: 0.710000.\n",
      "Step 16000.000000, loss: 1.077801, accurancy: 0.660000.\n",
      "Validation accurancy is 0.705464\n",
      "Step 16100.000000, loss: 0.921549, accurancy: 0.750000.\n",
      "Step 16200.000000, loss: 1.089926, accurancy: 0.680000.\n",
      "Step 16300.000000, loss: 1.130131, accurancy: 0.620000.\n",
      "Step 16400.000000, loss: 0.956133, accurancy: 0.710000.\n",
      "Step 16500.000000, loss: 1.079690, accurancy: 0.660000.\n",
      "Step 16600.000000, loss: 0.960569, accurancy: 0.740000.\n",
      "Step 16700.000000, loss: 1.210483, accurancy: 0.580000.\n",
      "Step 16800.000000, loss: 0.976442, accurancy: 0.640000.\n",
      "Step 16900.000000, loss: 1.278625, accurancy: 0.590000.\n",
      "Step 17000.000000, loss: 0.991613, accurancy: 0.720000.\n",
      "Validation accurancy is 0.706757\n",
      "Step 17100.000000, loss: 1.223423, accurancy: 0.550000.\n",
      "Step 17200.000000, loss: 0.902763, accurancy: 0.760000.\n",
      "Step 17300.000000, loss: 1.184972, accurancy: 0.630000.\n",
      "Step 17400.000000, loss: 1.209910, accurancy: 0.550000.\n",
      "Step 17500.000000, loss: 1.480962, accurancy: 0.560000.\n",
      "Step 17600.000000, loss: 1.260865, accurancy: 0.580000.\n",
      "Step 17700.000000, loss: 1.269080, accurancy: 0.610000.\n",
      "Step 17800.000000, loss: 1.038924, accurancy: 0.650000.\n",
      "Step 17900.000000, loss: 0.930135, accurancy: 0.770000.\n",
      "Step 18000.000000, loss: 1.075318, accurancy: 0.700000.\n",
      "Validation accurancy is 0.709344\n",
      "Step 18100.000000, loss: 1.234023, accurancy: 0.600000.\n",
      "Step 18200.000000, loss: 1.235537, accurancy: 0.620000.\n",
      "Step 18300.000000, loss: 1.102783, accurancy: 0.660000.\n",
      "Step 18400.000000, loss: 1.179423, accurancy: 0.650000.\n",
      "Step 18500.000000, loss: 1.017285, accurancy: 0.660000.\n",
      "Step 18600.000000, loss: 1.047497, accurancy: 0.690000.\n",
      "Step 18700.000000, loss: 1.100907, accurancy: 0.610000.\n",
      "Step 18800.000000, loss: 0.970622, accurancy: 0.680000.\n",
      "Step 18900.000000, loss: 0.951426, accurancy: 0.670000.\n",
      "Step 19000.000000, loss: 0.917954, accurancy: 0.700000.\n",
      "Validation accurancy is 0.711284\n",
      "Step 19100.000000, loss: 0.862930, accurancy: 0.740000.\n",
      "Step 19200.000000, loss: 1.008973, accurancy: 0.640000."
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "learning_decay = 0.9\n",
    "learning_decay_period = 1000\n",
    "eval_every_steps = 1000\n",
    "training_steps = 50000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1, training_steps+1):\n",
    "        start_pos = (i-1) * batch_size % size\n",
    "        end_pos = (i-1) * batch_size % size + batch_size \n",
    "        loss, _ ,accurancy,  = sess.run([cross_entropy_mean, train_step, evaluation_step], \\\n",
    "                feed_dict={\n",
    "                    input: train_fingerprints[start_pos:end_pos],\n",
    "                    ground_truth_input: train_ground_truth[start_pos:end_pos],\n",
    "                    learning_rate_input: learning_rate * (learning_decay**(i // learning_decay_period)),\n",
    "                    dropout_prob: 0.5\n",
    "                })\n",
    "        if i % 100 == 0:\n",
    "            print(\"Step %f, loss: %f, accurancy: %f.\" % (i, loss, accurancy))\n",
    "        if  i % eval_every_steps == 0:\n",
    "            saver.save(sess, data_dir + model_name +\".ckpt\", global_step=i)\n",
    "            accurancy = sess.run(evaluation_step, \\\n",
    "                feed_dict={\n",
    "                    input: validate_fingerprints,\n",
    "                    ground_truth_input: validate_ground_truth,\n",
    "                    dropout_prob: 1.0\n",
    "                })\n",
    "            print(\"Validation accurancy is %f\" % accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/simpleConvNet.ckpt-40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/maikfangogoair/simpleConvNet.ckpt-40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accurancy is 0.697501\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, '/home/maikfangogoair/simpleConvNet.ckpt-40000')\n",
    "    accurancy = sess.run(evaluation_step, \\\n",
    "        feed_dict={\n",
    "            input: test_fingerprints,\n",
    "            ground_truth_input: test_ground_truth,\n",
    "            dropout_prob: 1.0\n",
    "        })\n",
    "    print(\"Test accurancy is %f\" % accurancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification:  \n",
    "```\n",
    "'_silence_',\n",
    "'_unknown_',\n",
    "'yes',\n",
    "'no',\n",
    "'up',\n",
    "'down',\n",
    "'left',\n",
    "'right',\n",
    "'on',\n",
    "'off',\n",
    "'stop',\n",
    "'go'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #audio data exploration\n",
    "# from scipy.io import wavfile\n",
    "\n",
    "# sample_rate, samples = wavfile.read(\"../data/test/audio/clip_ff72c4530.wav\")\n",
    "\n",
    "# from scipy import signal\n",
    "# import numpy as np\n",
    "# def log_specgram(audio, sample_rate, window_size=20,\n",
    "#                  step_size=10, eps=1e-10):\n",
    "#     nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "#     noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "#     freqs, times, spec = signal.spectrogram(audio,\n",
    "#                                     fs=sample_rate,\n",
    "#                                     window='hann',\n",
    "#                                     nperseg=nperseg,\n",
    "#                                     noverlap=noverlap,\n",
    "#                                     detrend=False)\n",
    "#     return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "# freqs, times, spectrogram = log_specgram(samples, sample_rate)\n",
    "\n",
    "# spectrogram.shape\n",
    "\n",
    "# import librosa\n",
    "\n",
    "# S = librosa.feature.melspectrogram(samples, sr=sample_rate, n_mels=128)\n",
    "\n",
    "# S.shape\n",
    "\n",
    "# log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "# log_S.shape\n",
    "\n",
    "# mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n",
    "\n",
    "# mfcc.shape\n",
    "\n",
    "# delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "# delta2_mfcc.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
